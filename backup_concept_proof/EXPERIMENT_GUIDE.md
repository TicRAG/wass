# WASS å®éªŒæŒ‡å—

æœ¬æ–‡æ¡£å°†æŒ‡å¯¼ä½ å¦‚ä½•ä½¿ç”¨WASSæ¡†æ¶è¿›è¡Œå¼±ç›‘ç£å­¦ä¹ +å›¾ç¥ç»ç½‘ç»œ+å¼ºåŒ–å­¦ä¹ +RAGçš„å®éªŒç ”ç©¶ã€‚

## ğŸ¯ å®éªŒæ¦‚è¿°

WASSæ¡†æ¶æ”¯æŒä»¥ä¸‹ç±»å‹çš„å®éªŒï¼š
- **å¼±ç›‘ç£å­¦ä¹ å®éªŒ**ï¼šæ¯”è¾ƒä¸åŒæ ‡ç­¾æ¨¡å‹çš„æ•ˆæœ
- **å›¾å­¦ä¹ å®éªŒ**ï¼šè¯„ä¼°ä¸åŒå›¾æ„å»ºç­–ç•¥å’ŒGNNæ¨¡å‹
- **ä¸»åŠ¨å­¦ä¹ å®éªŒ**ï¼šç ”ç©¶DRLç­–ç•¥çš„é‡‡æ ·æ•ˆæœ
- **æ£€ç´¢å¢å¼ºå®éªŒ**ï¼šæµ‹è¯•RAGå¯¹é¢„æµ‹æ€§èƒ½çš„æå‡
- **ç«¯åˆ°ç«¯å®éªŒ**ï¼šå®Œæ•´pipelineçš„ç»¼åˆè¯„ä¼°

## ğŸš€ å¿«é€Ÿå¼€å§‹å®éªŒ

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd wass

# åˆ›å»ºå¹¶æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ (æ¨è)
python -m venv wass_env

# Windows
.\wass_env\Scripts\activate
# æˆ–è€…ç›´æ¥è¿è¡Œ
activate_env.bat

# Linux/macOS
source wass_env/bin/activate
# æˆ–è€…ç›´æ¥è¿è¡Œ
./activate_env.sh

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å¯é€‰ï¼šå®‰è£…Wrench (å¦‚æœè¦ä½¿ç”¨çœŸå®çš„å¼±ç›‘ç£æ¨¡å‹)
pip install wrench-ml
```

**å¿«é€Ÿå¯åŠ¨**: åŒå‡» `activate_env.bat` (Windows) æˆ–è¿è¡Œ `./activate_env.sh` (Linux/macOS) å³å¯å¿«é€Ÿæ¿€æ´»ç¯å¢ƒå¹¶æŸ¥çœ‹å¯ç”¨å‘½ä»¤ã€‚

### 2. ç”Ÿæˆå®éªŒæ•°æ®

**æ³¨æ„**: ç¡®ä¿å·²æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ (è¿è¡Œ `activate_env.bat` æˆ– `./activate_env.sh`)

```bash
# ç”Ÿæˆå°è§„æ¨¡æµ‹è¯•æ•°æ®
python scripts/gen_fake_data.py --out_dir data --train 100 --valid 30 --test 30

# ç”Ÿæˆä¸­ç­‰è§„æ¨¡æ•°æ®
python scripts/gen_fake_data.py --out_dir data --train 1000 --valid 200 --test 200

# ç”Ÿæˆå¤§è§„æ¨¡æ•°æ®
python scripts/gen_fake_data.py --out_dir data --train 5000 --valid 1000 --test 1000
```

### 3. è¿è¡ŒåŸºç¡€å®éªŒ

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®è¿è¡Œ
python -m src.pipeline_enhanced configs_example.yaml

# ä½¿ç”¨åˆ†æ¨¡å—é…ç½®è¿è¡Œ
python -m src.pipeline_enhanced configs/experiment.yaml

# è¿è¡Œæ¼”ç¤ºï¼ˆåŒ…å«å¤šä¸ªé…ç½®æµ‹è¯•ï¼‰
python demo.py
```

## ğŸ“‹ å®éªŒç±»å‹è¯¦è§£

### å®éªŒç±»å‹1ï¼šå¼±ç›‘ç£å­¦ä¹ å¯¹æ¯”å®éªŒ

**ç›®æ ‡**ï¼šæ¯”è¾ƒä¸åŒæ ‡ç­¾æ¨¡å‹çš„æ€§èƒ½

#### 1.1 åˆ›å»ºå®éªŒé…ç½®

åˆ›å»º `configs/exp_label_models.yaml`ï¼š
```yaml
experiment_name: label_model_comparison
paths:
  data_dir: data/
  results_dir: results/label_model_exp/

data:
  adapter: simple_jsonl
  train_file: train.jsonl
  valid_file: valid.jsonl
  test_file: test.jsonl

labeling:
  abstain: -1
  lfs:
    - name: keyword_positive
      type: keyword
      keywords: ["good", "excellent", "amazing", "great", "wonderful"]
      label: 1
    - name: keyword_negative
      type: keyword
      keywords: ["bad", "terrible", "awful", "poor", "horrible"]
      label: 0
    - name: length_filter
      type: length
      min_length: 5
      max_length: 50
      label: 1
    - name: regex_excitement
      type: regex
      pattern: "!{2,}|wow|amazing"
      label: 1

# å®éªŒå˜é‡ï¼šä¸åŒçš„æ ‡ç­¾æ¨¡å‹
label_model:
  type: majority_vote  # æˆ–è€… wrench
  params: {}

graph:
  builder: cooccurrence
  params:
    window_size: 5
  gnn_model: gcn
  gnn_params:
    hidden_dim: 64

rag:
  retriever: simple_bm25
  fusion: concat
  top_k: 5

drl:
  env: active_learning
  policy: random
  episodes: 3

eval:
  metrics: ["accuracy", "f1", "precision", "recall"]
```

#### 1.2 è¿è¡Œå¯¹æ¯”å®éªŒ

```bash
# 1. è¿è¡ŒMajorityVote
cp configs/exp_label_models.yaml configs/exp_majority_vote.yaml
python -m src.pipeline_enhanced configs/exp_majority_vote.yaml

# 2. åˆ›å»ºWrenché…ç½®
sed 's/type: majority_vote/type: wrench\n  model_name: MajorityVoting/' configs/exp_label_models.yaml > configs/exp_wrench_mv.yaml
python -m src.pipeline_enhanced configs/exp_wrench_mv.yaml

# 3. åˆ›å»ºSnorkelé…ç½®
sed 's/model_name: MajorityVoting/model_name: Snorkel/' configs/exp_wrench_mv.yaml > configs/exp_wrench_snorkel.yaml
python -m src.pipeline_enhanced configs/exp_wrench_snorkel.yaml
```

#### 1.3 ç»“æœåˆ†æ

```bash
# æ¯”è¾ƒç»“æœ
python scripts/compare_results.py results/label_model_exp/ --metric accuracy
```

### å®éªŒç±»å‹2ï¼šLabel Function è®¾è®¡å®éªŒ

**ç›®æ ‡**ï¼šç ”ç©¶ä¸åŒLabel Functionç»„åˆçš„æ•ˆæœ

#### 2.1 åˆ›å»ºLFå˜ä½“é…ç½®

```bash
# åˆ›å»ºå¤šä¸ªLFé…ç½®å˜ä½“
mkdir -p configs/lf_experiments
```

åˆ›å»º `configs/lf_experiments/lf_keyword_only.yaml`ï¼š
```yaml
labeling:
  lfs:
    - name: keyword_positive
      type: keyword
      keywords: ["good", "excellent"]
      label: 1
    - name: keyword_negative
      type: keyword
      keywords: ["bad", "terrible"]
      label: 0
```

åˆ›å»º `configs/lf_experiments/lf_keyword_regex.yaml`ï¼š
```yaml
labeling:
  lfs:
    - name: keyword_positive
      type: keyword
      keywords: ["good", "excellent", "amazing"]
      label: 1
    - name: keyword_negative
      type: keyword
      keywords: ["bad", "terrible", "awful"]
      label: 0
    - name: regex_positive
      type: regex
      pattern: "\\b(great|awesome|fantastic)\\b"
      label: 1
    - name: regex_negative
      type: regex
      pattern: "\\b(hate|disgusting|worst)\\b"
      label: 0
```

#### 2.2 æ‰¹é‡è¿è¡Œå®éªŒ

åˆ›å»º `scripts/run_lf_experiments.py`ï¼š
```python
#!/usr/bin/env python3
"""æ‰¹é‡è¿è¡ŒLabel Functionå®éªŒ."""

import os
import yaml
from pathlib import Path
from src.pipeline_enhanced import run_enhanced_pipeline

def run_lf_experiments():
    """è¿è¡Œæ‰€æœ‰LFé…ç½®å®éªŒ."""
    base_config = yaml.safe_load(Path('configs_example.yaml').read_text())
    lf_configs = Path('configs/lf_experiments').glob('*.yaml')
    
    results = {}
    for lf_config in lf_configs:
        print(f"è¿è¡Œå®éªŒ: {lf_config.name}")
        
        # åˆå¹¶é…ç½®
        lf_data = yaml.safe_load(lf_config.read_text())
        config = base_config.copy()
        config['labeling'] = lf_data['labeling']
        config['experiment_name'] = f"lf_exp_{lf_config.stem}"
        config['paths']['results_dir'] = f"results/lf_experiments/{lf_config.stem}/"
        
        # ä¿å­˜ä¸´æ—¶é…ç½®
        temp_config = f"temp_{lf_config.stem}.yaml"
        Path(temp_config).write_text(yaml.dump(config))
        
        try:
            # è¿è¡Œå®éªŒ
            result = run_enhanced_pipeline(temp_config)
            results[lf_config.stem] = result
            print(f"âœ“ {lf_config.name} å®Œæˆ")
        except Exception as e:
            print(f"âœ— {lf_config.name} å¤±è´¥: {e}")
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if Path(temp_config).exists():
                Path(temp_config).unlink()
    
    return results

if __name__ == '__main__':
    run_lf_experiments()
```

è¿è¡Œæ‰¹é‡å®éªŒï¼š
```bash
python scripts/run_lf_experiments.py
```

### å®éªŒç±»å‹3ï¼šå›¾æ„å»ºç­–ç•¥å®éªŒ

**ç›®æ ‡**ï¼šæ¯”è¾ƒä¸åŒå›¾æ„å»ºæ–¹æ³•çš„æ•ˆæœ

#### 3.1 æ‰©å±•å›¾æ„å»ºå™¨

é¦–å…ˆæ‰©å±• `src/graph/graph_builder.py`ï¼š
```python
class SimilarityGraphBuilder:
    """åŸºäºç›¸ä¼¼åº¦çš„å›¾æ„å»ºå™¨."""
    def __init__(self, similarity_threshold: float = 0.5, field: str = 'text'):
        self.threshold = similarity_threshold
        self.field = field
    
    def build(self, data: List[Dict[str, Any]], labels):
        # è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦å¹¶æ„å»ºå›¾
        # è¿™é‡Œå¯ä»¥ç”¨TF-IDF + ä½™å¼¦ç›¸ä¼¼åº¦
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity
        
        texts = [sample.get(self.field, '') for sample in data]
        vectorizer = TfidfVectorizer(max_features=1000)
        tfidf_matrix = vectorizer.fit_transform(texts)
        similarity_matrix = cosine_similarity(tfidf_matrix)
        
        graph = defaultdict(lambda: defaultdict(float))
        for i in range(len(data)):
            for j in range(i+1, len(data)):
                sim = similarity_matrix[i, j]
                if sim > self.threshold:
                    graph[f"node_{i}"][f"node_{j}"] = sim
                    graph[f"node_{j}"][f"node_{i}"] = sim
        
        return graph
```

#### 3.2 åˆ›å»ºå›¾å®éªŒé…ç½®

åˆ›å»º `configs/graph_experiments/`ç›®å½•ï¼ŒåŒ…å«ä¸åŒå›¾é…ç½®ï¼š

`cooccurrence_graph.yaml`:
```yaml
graph:
  builder: cooccurrence
  params:
    window_size: 5
```

`similarity_graph.yaml`:
```yaml
graph:
  builder: similarity
  params:
    similarity_threshold: 0.3
```

#### 3.3 è¿è¡Œå›¾å®éªŒ

```bash
python scripts/run_graph_experiments.py
```

### å®éªŒç±»å‹4ï¼šç«¯åˆ°ç«¯ç³»ç»Ÿå®éªŒ

**ç›®æ ‡**ï¼šè¯„ä¼°å®Œæ•´ç³»ç»Ÿåœ¨çœŸå®åœºæ™¯ä¸‹çš„è¡¨ç°

#### 4.1 å‡†å¤‡çœŸå®æ•°æ®

```python
# scripts/prepare_real_data.py
"""å‡†å¤‡çœŸå®æ•°æ®é›†çš„è„šæœ¬."""

def convert_imdb_to_jsonl():
    """å°†IMDBæ•°æ®è½¬æ¢ä¸ºJSONLæ ¼å¼."""
    # å‡è®¾ä½ æœ‰IMDBæ•°æ®
    import pandas as pd
    
    # è¯»å–æ•°æ®
    df = pd.read_csv('path/to/imdb.csv')
    
    # è½¬æ¢æ ¼å¼
    for split in ['train', 'valid', 'test']:
        split_df = df[df['split'] == split]
        with open(f'data/{split}.jsonl', 'w') as f:
            for _, row in split_df.iterrows():
                item = {
                    'text': row['review'],
                    'label': row['sentiment'],  # çœŸå®æ ‡ç­¾ï¼Œç”¨äºè¯„ä¼°
                    'id': row['id']
                }
                f.write(json.dumps(item) + '\n')

def convert_amazon_to_jsonl():
    """è½¬æ¢Amazonè¯„è®ºæ•°æ®."""
    # ç±»ä¼¼å®ç°
    pass
```

#### 4.2 åˆ›å»ºçœŸå®æ•°æ®å®éªŒé…ç½®

`configs/real_data_exp.yaml`:
```yaml
experiment_name: real_data_evaluation
paths:
  data_dir: data/real/
  results_dir: results/real_data_exp/

data:
  adapter: simple_jsonl
  train_file: train.jsonl
  valid_file: valid.jsonl
  test_file: test.jsonl

labeling:
  lfs:
    # åŸºäºé¢†åŸŸçŸ¥è¯†è®¾è®¡çš„LF
    - name: positive_words
      type: keyword
      keywords: ["excellent", "outstanding", "wonderful", "fantastic", "amazing", "great", "love", "perfect", "brilliant"]
      label: 1
    - name: negative_words
      type: keyword
      keywords: ["terrible", "awful", "horrible", "disgusting", "hate", "worst", "bad", "poor", "disappointing"]
      label: 0
    - name: rating_patterns
      type: regex
      pattern: "5\\s*(stars?|/5|out of 5)"
      label: 1
    - name: short_negative
      type: length
      max_length: 10
      label: 0

label_model:
  type: wrench
  model_name: Snorkel
  params:
    lr: 0.01
    l2: 0.01
    n_epochs: 100

# å…¶ä»–é…ç½®...
```

#### 4.3 è¿è¡Œå®Œæ•´è¯„ä¼°

```bash
python -m src.pipeline_enhanced configs/real_data_exp.yaml
```

## ğŸ“Š å®éªŒåˆ†æä¸æŠ¥å‘Š

### 1. ç»“æœæ¯”è¾ƒè„šæœ¬

åˆ›å»º `scripts/analyze_results.py`ï¼š
```python
"""å®éªŒç»“æœåˆ†æè„šæœ¬."""

import json
import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

def load_experiment_results(results_dir: str):
    """åŠ è½½æ‰€æœ‰å®éªŒç»“æœ."""
    results = {}
    results_path = Path(results_dir)
    
    for exp_dir in results_path.iterdir():
        if exp_dir.is_dir():
            summary_file = exp_dir / 'summary.json'
            if summary_file.exists():
                with open(summary_file) as f:
                    results[exp_dir.name] = json.load(f)
    
    return results

def create_comparison_table(results):
    """åˆ›å»ºå¯¹æ¯”è¡¨æ ¼."""
    data = []
    for exp_name, result in results.items():
        row = {
            'experiment': exp_name,
            'train_size': result.get('data_stats', {}).get('train_size', 0),
            'coverage': result.get('labeling_stats', {}).get('coverage', 0),
            'conflict_rate': result.get('labeling_stats', {}).get('conflict_rate', 0),
            'accuracy': result.get('eval_stats', {}).get('accuracy', 0),
            'f1': result.get('eval_stats', {}).get('f1', 0),
        }
        data.append(row)
    
    df = pd.DataFrame(data)
    return df

def plot_results(df):
    """ç»˜åˆ¶ç»“æœå›¾è¡¨."""
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # å‡†ç¡®ç‡å¯¹æ¯”
    axes[0, 0].bar(df['experiment'], df['accuracy'])
    axes[0, 0].set_title('Accuracy Comparison')
    axes[0, 0].tick_params(axis='x', rotation=45)
    
    # F1å¯¹æ¯”
    axes[0, 1].bar(df['experiment'], df['f1'])
    axes[0, 1].set_title('F1 Score Comparison')
    axes[0, 1].tick_params(axis='x', rotation=45)
    
    # è¦†ç›–ç‡ vs å‡†ç¡®ç‡
    axes[1, 0].scatter(df['coverage'], df['accuracy'])
    axes[1, 0].set_xlabel('Coverage')
    axes[1, 0].set_ylabel('Accuracy')
    axes[1, 0].set_title('Coverage vs Accuracy')
    
    # å†²çªç‡ vs F1
    axes[1, 1].scatter(df['conflict_rate'], df['f1'])
    axes[1, 1].set_xlabel('Conflict Rate')
    axes[1, 1].set_ylabel('F1 Score')
    axes[1, 1].set_title('Conflict Rate vs F1')
    
    plt.tight_layout()
    plt.savefig('experiment_results.png', dpi=300, bbox_inches='tight')
    plt.show()

if __name__ == '__main__':
    # åˆ†æç»“æœ
    results = load_experiment_results('results/')
    df = create_comparison_table(results)
    
    print("å®éªŒç»“æœå¯¹æ¯”:")
    print(df.to_string(index=False))
    
    # ä¿å­˜åˆ°CSV
    df.to_csv('experiment_comparison.csv', index=False)
    
    # ç»˜åˆ¶å›¾è¡¨
    plot_results(df)
```

### 2. è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆ

åˆ›å»º `scripts/generate_report.py`ï¼š
```python
"""ç”Ÿæˆå®éªŒæŠ¥å‘Š."""

def generate_markdown_report(results_df):
    """ç”ŸæˆMarkdownæ ¼å¼çš„å®éªŒæŠ¥å‘Š."""
    report = f"""# WASS å®éªŒæŠ¥å‘Š
    
## å®éªŒæ¦‚è¿°
æœ¬æŠ¥å‘ŠåŒ…å«äº† {len(results_df)} ä¸ªå®éªŒçš„ç»“æœå¯¹æ¯”ã€‚

## å®éªŒç»“æœ

### æ•´ä½“æ€§èƒ½å¯¹æ¯”
{results_df.to_markdown(index=False)}

### æœ€ä½³æ€§èƒ½å®éªŒ
- **æœ€é«˜å‡†ç¡®ç‡**: {results_df.loc[results_df['accuracy'].idxmax(), 'experiment']} ({results_df['accuracy'].max():.3f})
- **æœ€é«˜F1**: {results_df.loc[results_df['f1'].idxmax(), 'experiment']} ({results_df['f1'].max():.3f})
- **æœ€é«˜è¦†ç›–ç‡**: {results_df.loc[results_df['coverage'].idxmax(), 'experiment']} ({results_df['coverage'].max():.3f})
- **æœ€ä½å†²çªç‡**: {results_df.loc[results_df['conflict_rate'].idxmin(), 'experiment']} ({results_df['conflict_rate'].min():.3f})

### æ€§èƒ½åˆ†æ
1. **è¦†ç›–ç‡ä¸å‡†ç¡®ç‡çš„å…³ç³»**: 
   - ç›¸å…³ç³»æ•°: {results_df['coverage'].corr(results_df['accuracy']):.3f}
   
2. **å†²çªç‡ä¸æ€§èƒ½çš„å…³ç³»**:
   - å†²çªç‡ä¸F1ç›¸å…³ç³»æ•°: {results_df['conflict_rate'].corr(results_df['f1']):.3f}

## ç»“è®ºä¸å»ºè®®
[æ ¹æ®å®éªŒç»“æœå¡«å†™ç»“è®º]

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
    return report
```

## ğŸ”§ é«˜çº§å®éªŒæŠ€å·§

### 1. è¶…å‚æ•°æœç´¢

åˆ›å»º `scripts/hyperparameter_search.py`ï¼š
```python
"""è¶…å‚æ•°æœç´¢è„šæœ¬."""

from itertools import product
import yaml

def grid_search_label_model():
    """æ ‡ç­¾æ¨¡å‹è¶…å‚æ•°ç½‘æ ¼æœç´¢."""
    if model_type == 'wrench':
        param_grid = {
            'lr': [0.001, 0.01, 0.1],
            'l2': [0.001, 0.01, 0.1],
            'n_epochs': [50, 100, 200]
        }
        
        for lr, l2, epochs in product(*param_grid.values()):
            config = base_config.copy()
            config['label_model']['params'] = {
                'lr': lr, 'l2': l2, 'n_epochs': epochs
            }
            config['experiment_name'] = f"grid_search_lr{lr}_l2{l2}_ep{epochs}"
            
            # è¿è¡Œå®éªŒ
            run_experiment(config)
```

### 2. äº¤å‰éªŒè¯

```python
def k_fold_validation(k=5):
    """KæŠ˜äº¤å‰éªŒè¯."""
    from sklearn.model_selection import KFold
    
    # åŠ è½½æ•°æ®
    data = load_data()
    kf = KFold(n_splits=k, shuffle=True, random_state=42)
    
    results = []
    for fold, (train_idx, val_idx) in enumerate(kf.split(data)):
        # åˆ›å»ºfoldç‰¹å®šçš„æ•°æ®æ–‡ä»¶
        create_fold_data(data, train_idx, val_idx, fold)
        
        # è¿è¡Œå®éªŒ
        config = create_fold_config(fold)
        result = run_enhanced_pipeline(config)
        results.append(result)
    
    # æ±‡æ€»ç»“æœ
    return aggregate_cv_results(results)
```

### 3. ç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯•

```python
def statistical_significance_test(results1, results2):
    """ç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯•."""
    from scipy import stats
    
    # æå–æ€§èƒ½æŒ‡æ ‡
    acc1 = [r['eval_stats']['accuracy'] for r in results1]
    acc2 = [r['eval_stats']['accuracy'] for r in results2]
    
    # tæ£€éªŒ
    t_stat, p_value = stats.ttest_ind(acc1, acc2)
    
    print(f"T-statistic: {t_stat:.4f}")
    print(f"P-value: {p_value:.4f}")
    print(f"Significant: {'Yes' if p_value < 0.05 else 'No'}")
```

## ğŸ“ å®éªŒæœ€ä½³å®è·µ

### 1. å®éªŒè®¾è®¡åŸåˆ™
- **æ§åˆ¶å˜é‡**: æ¯æ¬¡å®éªŒåªæ”¹å˜ä¸€ä¸ªå˜é‡
- **å¤šæ¬¡è¿è¡Œ**: ä½¿ç”¨ä¸åŒéšæœºç§å­è¿è¡Œå¤šæ¬¡
- **åŸºçº¿å¯¹æ¯”**: å§‹ç»ˆåŒ…å«ç®€å•åŸºçº¿æ–¹æ³•
- **ç»Ÿè®¡æ£€éªŒ**: è¿›è¡Œæ˜¾è‘—æ€§æµ‹è¯•éªŒè¯ç»“æœ

### 2. ç»“æœè®°å½•
- è¯¦ç»†è®°å½•å®éªŒè®¾ç½®å’Œè¶…å‚æ•°
- ä¿å­˜ä¸­é—´ç»“æœå’Œæ¨¡å‹æƒé‡
- è®°å½•å®éªŒç¯å¢ƒä¿¡æ¯
- å¤‡ä»½åŸå§‹æ•°æ®å’Œä»£ç ç‰ˆæœ¬

### 3. å¯å¤ç°æ€§
```bash
# è®¾ç½®éšæœºç§å­
export PYTHONHASHSEED=0
python -c "import random; random.seed(42)"

# è®°å½•ç¯å¢ƒä¿¡æ¯
pip freeze > requirements.txt
python --version > python_version.txt
git rev-parse HEAD > git_commit.txt
```

## ğŸ“ ç¤ºä¾‹ï¼šå®Œæ•´å®éªŒå·¥ä½œæµ

```bash
# 1. å‡†å¤‡ç¯å¢ƒ
python scripts/setup_experiment_env.py

# 2. ç”Ÿæˆæ•°æ®
python scripts/gen_fake_data.py --out_dir data --train 1000 --valid 200 --test 200

# 3. è¿è¡ŒåŸºçº¿å®éªŒ
python -m src.pipeline_enhanced configs/baseline.yaml

# 4. è¿è¡Œå¯¹æ¯”å®éªŒ
python scripts/run_comparison_experiments.py

# 5. åˆ†æç»“æœ
python scripts/analyze_results.py

# 6. ç”ŸæˆæŠ¥å‘Š
python scripts/generate_report.py

# 7. æäº¤ç»“æœ
git add results/ reports/
git commit -m "Add experiment results for [å®éªŒåç§°]"
```

## ğŸ› å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### 1. Wrenchç¯å¢ƒé—®é¢˜
```bash
# å¦‚æœWrenchå¯¼å…¥å¤±è´¥
pip install wrench-ml==1.2.0

# å¦‚æœç‰ˆæœ¬å†²çª
conda create -n wrench python=3.8
conda activate wrench
pip install wrench-ml
```

### 2. å†…å­˜ä¸è¶³
```python
# å‡å°‘æ•°æ®è§„æ¨¡
python scripts/gen_fake_data.py --train 100 --valid 20 --test 20

# æˆ–è°ƒæ•´æ‰¹å¤„ç†å¤§å°
config['training']['batch_size'] = 16
```

### 3. å®éªŒé‡ç°é—®é¢˜
```python
# ç¡®ä¿è®¾ç½®éšæœºç§å­
import random
import numpy as np
random.seed(42)
np.random.seed(42)
```

---

è¿™ä¸ªå®éªŒæŒ‡å—æä¾›äº†ä»åŸºç¡€åˆ°é«˜çº§çš„å®Œæ•´å®éªŒæµç¨‹ã€‚æ ¹æ®ä½ çš„ç ”ç©¶éœ€æ±‚ï¼Œå¯ä»¥é€‰æ‹©ç›¸åº”çš„å®éªŒç±»å‹å’Œåˆ†ææ–¹æ³•ã€‚

éœ€è¦é’ˆå¯¹ç‰¹å®šå®éªŒåœºæ™¯çš„æ›´è¯¦ç»†æŒ‡å¯¼å—ï¼Ÿ
