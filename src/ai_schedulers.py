#!/usr/bin/env python3
"""
WASS-RAG AIè°ƒåº¦å™¨æ ¸å¿ƒå®ç°
åŒ…å«æ‰€æœ‰AIè°ƒåº¦æ–¹æ³•ï¼šWASS (Heuristic), WASS-DRL (w/o RAG), WASS-RAG
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Any, Tuple, Optional
from dataclasses import dataclass
import faiss
import pickle
from pathlib import Path
import json

# PyTorch Geometric imports for GNN
try:
    import torch_geometric
    from torch_geometric.nn import GCNConv, GATConv, global_mean_pool
    from torch_geometric.data import Data, Batch
    HAS_TORCH_GEOMETRIC = True
except ImportError:
    HAS_TORCH_GEOMETRIC = False
    print("Warning: torch_geometric not installed. GNN functionality will be limited.")

@dataclass
class SchedulingState:
    """è°ƒåº¦çŠ¶æ€è¡¨ç¤º"""
    workflow_graph: Dict[str, Any]
    cluster_state: Dict[str, Any] 
    pending_tasks: List[str]
    current_task: str
    available_nodes: List[str]
    timestamp: float

@dataclass
class SchedulingAction:
    """è°ƒåº¦åŠ¨ä½œ"""
    task_id: str
    target_node: str
    confidence: float
    reasoning: Optional[str] = None

class BaseScheduler:
    """åŸºç¡€è°ƒåº¦å™¨æŠ½è±¡ç±»"""
    
    def __init__(self, name: str):
        self.name = name
        
    def make_decision(self, state: SchedulingState) -> SchedulingAction:
        """åšå‡ºè°ƒåº¦å†³ç­–"""
        raise NotImplementedError
        
    def reset(self):
        """é‡ç½®è°ƒåº¦å™¨çŠ¶æ€"""
        pass

class WASSHeuristicScheduler(BaseScheduler):
    """WASSå¯å‘å¼è°ƒåº¦å™¨ - åŸºäºå¤šæ•°ç¥¨å’Œæ•°æ®å±€éƒ¨æ€§çš„è§„åˆ™"""
    
    def __init__(self):
        super().__init__("WASS (Heuristic)")
        
    def make_decision(self, state: SchedulingState) -> SchedulingAction:
        """åŸºäºå¯å‘å¼è§„åˆ™çš„è°ƒåº¦å†³ç­–"""
        
        task_id = state.current_task
        available_nodes = state.available_nodes
        
        if not available_nodes:
            raise ValueError("No available nodes for scheduling")
            
        # è·å–ä»»åŠ¡ä¿¡æ¯
        task_info = self._get_task_info(state.workflow_graph, task_id)
        
        # è§„åˆ™1: æ•°æ®å±€éƒ¨æ€§ - ä¼˜å…ˆé€‰æ‹©æœ‰è¾“å…¥æ•°æ®çš„èŠ‚ç‚¹
        data_locality_scores = self._calculate_data_locality_scores(
            task_info, available_nodes, state.cluster_state
        )
        
        # è§„åˆ™2: èµ„æºåŒ¹é… - ä¼˜å…ˆé€‰æ‹©èµ„æºæœ€åŒ¹é…çš„èŠ‚ç‚¹
        resource_match_scores = self._calculate_resource_match_scores(
            task_info, available_nodes, state.cluster_state
        )
        
        # è§„åˆ™3: è´Ÿè½½å‡è¡¡ - ä¼˜å…ˆé€‰æ‹©è´Ÿè½½è¾ƒä½çš„èŠ‚ç‚¹
        load_balance_scores = self._calculate_load_balance_scores(
            available_nodes, state.cluster_state
        )
        
        # å¤šæ•°ç¥¨å†³ç­–ï¼šç»¼åˆæ‰€æœ‰è§„åˆ™
        final_scores = {}
        for node in available_nodes:
            # åŠ æƒç»„åˆå„é¡¹å¾—åˆ†
            final_scores[node] = (
                0.4 * data_locality_scores.get(node, 0) +
                0.3 * resource_match_scores.get(node, 0) +
                0.3 * load_balance_scores.get(node, 0)
            )
        
        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„èŠ‚ç‚¹
        best_node = max(final_scores.keys(), key=lambda k: final_scores[k])
        confidence = final_scores[best_node]
        
        # ç”Ÿæˆå†³ç­–è§£é‡Š
        reasoning = f"Heuristic decision: data_locality={data_locality_scores.get(best_node, 0):.2f}, " \
                   f"resource_match={resource_match_scores.get(best_node, 0):.2f}, " \
                   f"load_balance={load_balance_scores.get(best_node, 0):.2f}"
        
        return SchedulingAction(
            task_id=task_id,
            target_node=best_node,
            confidence=confidence,
            reasoning=reasoning
        )
    
    def _get_task_info(self, workflow_graph: Dict[str, Any], task_id: str) -> Dict[str, Any]:
        """è·å–ä»»åŠ¡ä¿¡æ¯"""
        # å¤„ç†ä¸¤ç§æ ¼å¼ï¼šå­—ç¬¦ä¸²åˆ—è¡¨æˆ–å­—å…¸åˆ—è¡¨
        tasks = workflow_graph.get("tasks", [])
        task_requirements = workflow_graph.get("task_requirements", {})
        
        for task in tasks:
            if isinstance(task, str):
                # ä»»åŠ¡æ˜¯å­—ç¬¦ä¸²æ ¼å¼
                if task == task_id:
                    # ä»task_requirementsè·å–ä»»åŠ¡ä¿¡æ¯
                    return task_requirements.get(task_id, {
                        "cpu": 2.0, "memory": 4.0, "duration": 5.0,
                        "dependencies": workflow_graph.get("dependencies", {}).get(task_id, [])
                    })
            elif isinstance(task, dict):
                # ä»»åŠ¡æ˜¯å­—å…¸æ ¼å¼
                if task.get("id") == task_id:
                    return task
                    
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›é»˜è®¤ä¿¡æ¯
        return {
            "cpu": 2.0, "memory": 4.0, "duration": 5.0,
            "dependencies": workflow_graph.get("dependencies", {}).get(task_id, [])
        }
    
    def _calculate_data_locality_scores(self, task_info: Dict[str, Any], 
                                      available_nodes: List[str], 
                                      cluster_state: Dict[str, Any]) -> Dict[str, float]:
        """è®¡ç®—æ•°æ®å±€éƒ¨æ€§å¾—åˆ†"""
        scores = {}
        
        # ç®€åŒ–çš„æ•°æ®å±€éƒ¨æ€§è®¡ç®—
        for node in available_nodes:
            score = 0.5  # åŸºç¡€å¾—åˆ†
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¾èµ–ä»»åŠ¡çš„è¾“å‡ºæ•°æ®åœ¨æ­¤èŠ‚ç‚¹
            dependencies = task_info.get("dependencies", [])
            if dependencies:
                # å‡è®¾æœ‰30%çš„æ¦‚ç‡ä¾èµ–æ•°æ®åœ¨æ­¤èŠ‚ç‚¹
                score += 0.3 * len(dependencies) / max(len(dependencies), 1)
            
            scores[node] = min(score, 1.0)
            
        return scores
    
    def _calculate_resource_match_scores(self, task_info: Dict[str, Any],
                                       available_nodes: List[str],
                                       cluster_state: Dict[str, Any]) -> Dict[str, float]:
        """è®¡ç®—èµ„æºåŒ¹é…å¾—åˆ†"""
        scores = {}
        
        # å…¼å®¹ä¸åŒçš„å­—æ®µå
        task_cpu_req = task_info.get("cpu", task_info.get("flops", 2.0))
        task_memory_req = task_info.get("memory", 4.0)
        
        for node in available_nodes:
            node_info = cluster_state.get("nodes", {}).get(node, {})
            node_cpu_capacity = node_info.get("cpu_capacity", 2.0)  # ä¸è®­ç»ƒæ•°æ®ä¸€è‡´
            node_memory_capacity = node_info.get("memory_capacity", 16.0)
            
            # è®¡ç®—èµ„æºåˆ©ç”¨ç‡åŒ¹é…åº¦
            cpu_utilization = float(task_cpu_req) / float(node_cpu_capacity)
            memory_utilization = float(task_memory_req) / float(node_memory_capacity)
            
            # ç†æƒ³åˆ©ç”¨ç‡åœ¨60-80%ä¹‹é—´
            cpu_score = 1.0 - abs(cpu_utilization - 0.7)
            memory_score = 1.0 - abs(memory_utilization - 0.7)
            
            scores[node] = max(0.0, (cpu_score + memory_score) / 2.0)
            
        return scores
    
    def _calculate_load_balance_scores(self, available_nodes: List[str],
                                     cluster_state: Dict[str, Any]) -> Dict[str, float]:
        """è®¡ç®—è´Ÿè½½å‡è¡¡å¾—åˆ†"""
        scores = {}
        
        for node in available_nodes:
            node_info = cluster_state.get("nodes", {}).get(node, {})
            current_load = node_info.get("current_load", 0.5)  # 0-1ä¹‹é—´
            
            # è´Ÿè½½è¶Šä½ï¼Œå¾—åˆ†è¶Šé«˜
            scores[node] = 1.0 - current_load
            
        return scores

class WASSSmartScheduler(BaseScheduler):
    """WASS-DRLæ™ºèƒ½è°ƒåº¦å™¨ (w/o RAG) - æ ‡å‡†DRLæ–¹æ³•"""
    
    def __init__(self, model_path: Optional[str] = None):
        super().__init__("WASS-DRL (w/o RAG)")
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆå§‹åŒ–GNNæ¨¡å‹
        if HAS_TORCH_GEOMETRIC:
            self.gnn_encoder = GraphEncoder(
                node_feature_dim=8,
                edge_feature_dim=4,
                hidden_dim=64,
                output_dim=32
            ).to(self.device)
        else:
            self.gnn_encoder = None
            
        # åˆå§‹åŒ–ç­–ç•¥ç½‘ç»œ
        self.policy_network = PolicyNetwork(
            state_dim=32,
            action_dim=1,
            hidden_dim=128
        ).to(self.device)
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        if model_path and Path(model_path).exists():
            self.load_model(model_path)
        else:
            print(f"Warning: No pretrained model found at {model_path}, using random initialization")
    
    def make_decision(self, state: SchedulingState) -> SchedulingAction:
        """åŸºäºDRLç­–ç•¥çš„è°ƒåº¦å†³ç­–"""
        
        try:
            # ç¼–ç çŠ¶æ€å›¾
            if self.gnn_encoder is not None:
                state_embedding = self._encode_state_graph(state)
            else:
                # å¦‚æœæ²¡æœ‰torch_geometricï¼Œä½¿ç”¨ç®€åŒ–çš„ç‰¹å¾æå–
                state_embedding = self._extract_simple_features(state)
            
            # è®¡ç®—æ¯ä¸ªå¯ç”¨èŠ‚ç‚¹çš„Qå€¼
            node_scores = {}
            for node in state.available_nodes:
                # å°†èŠ‚ç‚¹ä¿¡æ¯ç¼–ç åˆ°çŠ¶æ€ä¸­
                node_state = self._encode_node_context(state_embedding, node, state)
                
                # è·å–Qå€¼
                with torch.no_grad():
                    q_value = self.policy_network(node_state).item()
                
                node_scores[node] = q_value
            
            # é€‰æ‹©Qå€¼æœ€é«˜çš„èŠ‚ç‚¹
            best_node = max(node_scores.keys(), key=lambda k: node_scores[k])
            confidence = torch.sigmoid(torch.tensor(node_scores[best_node])).item()
            
            reasoning = f"DRL decision: Q-values = {dict(sorted(node_scores.items(), key=lambda x: x[1], reverse=True))}"
            
            return SchedulingAction(
                task_id=state.current_task,
                target_node=best_node,
                confidence=confidence,
                reasoning=reasoning
            )
            
        except Exception as e:
            print(f"âš ï¸  [DEGRADATION] DRL decision making failed: {e}")
            print(f"âš ï¸  [DEGRADATION] WASSSmartScheduler falling back to RANDOM selection")
            print(f"âš ï¸  [DEGRADATION] Task: {state.current_task}, Available nodes: {state.available_nodes}")
            
            # é™çº§åˆ°éšæœºé€‰æ‹©
            fallback_node = np.random.choice(state.available_nodes)
            print(f"âš ï¸  [DEGRADATION] Random fallback selected: {fallback_node}")
            
            return SchedulingAction(
                task_id=state.current_task,
                target_node=fallback_node,
                confidence=0.1,
                reasoning=f"ğŸ”´ DEGRADED: Random fallback due to DRL error: {e}"
            )
    
    def _encode_state_graph(self, state: SchedulingState) -> torch.Tensor:
        """ä½¿ç”¨GNNç¼–ç çŠ¶æ€å›¾"""
        if self.gnn_encoder is None:
            print(f"âš ï¸  [DEGRADATION] GNN encoder not available, using simple features for {state.current_task}")
            return self._extract_simple_features(state)
            
        # æ„å»ºPyTorch Geometricå›¾æ•°æ®
        graph_data = self._build_graph_data(state)
        
        # æ£€æŸ¥å›¾æ•°æ®æ˜¯å¦å¯ç”¨
        if graph_data is None:
            print(f"âš ï¸  [DEGRADATION] Graph data is None, falling back to simple features for {state.current_task}")
            return self._extract_simple_features(state)
        
        # GNNå‰å‘ä¼ æ’­
        with torch.no_grad():
            embedding = self.gnn_encoder(graph_data)
            
        return embedding
    
    def _extract_simple_features(self, state: SchedulingState) -> torch.Tensor:
        """æå–ç®€åŒ–çš„çŠ¶æ€ç‰¹å¾ï¼ˆå½“æ²¡æœ‰torch_geometricæ—¶ä½¿ç”¨ï¼‰"""
        features = []
        
        # å·¥ä½œæµç‰¹å¾
        total_tasks = len(state.workflow_graph.get("tasks", []))
        pending_tasks = len(state.pending_tasks)
        features.extend([total_tasks, pending_tasks, pending_tasks/max(total_tasks, 1)])
        
        # é›†ç¾¤ç‰¹å¾
        total_nodes = len(state.available_nodes)
        avg_load = 0.5  # ç®€åŒ–å‡è®¾
        features.extend([total_nodes, avg_load])
        
        # å½“å‰ä»»åŠ¡ç‰¹å¾
        task_info = None
        tasks = state.workflow_graph.get("tasks", [])
        
        # å¤„ç†taskså¯èƒ½æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨æˆ–å­—å…¸åˆ—è¡¨çš„æƒ…å†µ
        for task in tasks:
            if isinstance(task, str):
                # ä»»åŠ¡æ˜¯å­—ç¬¦ä¸²æ ¼å¼
                if task == state.current_task:
                    # ä»task_requirementsè·å–ä»»åŠ¡ä¿¡æ¯
                    task_reqs = state.workflow_graph.get("task_requirements", {})
                    task_info = task_reqs.get(task, {})
                    break
            elif isinstance(task, dict):
                # ä»»åŠ¡æ˜¯å­—å…¸æ ¼å¼
                if task.get("id") == state.current_task:
                    task_info = task
                    break
                
        if task_info:
            # ä½¿ç”¨ä¸åŒå­—æ®µåå°è¯•è·å–ä»»åŠ¡å±æ€§
            task_cpu = task_info.get("cpu", task_info.get("flops", 2.0))
            task_memory = task_info.get("memory", 4.0)
            task_duration = task_info.get("duration", task_info.get("runtime", 5.0))
            
            # å½’ä¸€åŒ–è¿™äº›å€¼ï¼ˆä¸è®­ç»ƒæ•°æ®ä¸€è‡´ï¼‰
            cpu_norm = min(1.0, float(task_cpu) / 15e9)  # è®­ç»ƒæ—¶æœ€å¤§æ˜¯15e9 GFlops
            mem_norm = min(1.0, float(task_memory) / 8.0)  # è®­ç»ƒæ—¶æœ€å¤§æ˜¯8.0 GB
            dur_norm = min(1.0, float(task_duration) / 180.0)  # è®­ç»ƒæ—¶æœ€å¤§æ˜¯180ç§’
            
            features.extend([cpu_norm, mem_norm, dur_norm])
        else:
            features.extend([0.2, 0.25, 0.25])  # é»˜è®¤ä¸­ç­‰éœ€æ±‚
        
        # å¡«å……åˆ°å›ºå®šé•¿åº¦
        while len(features) < 32:
            features.append(0.0)
            
        return torch.tensor(features[:32], dtype=torch.float32, device=self.device)
    
    def _encode_node_context(self, state_embedding: torch.Tensor, node: str, state: SchedulingState) -> torch.Tensor:
        """ç¼–ç èŠ‚ç‚¹ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        # ç®€åŒ–çš„èŠ‚ç‚¹ç‰¹å¾
        node_features = [
            hash(node) % 100 / 100.0,  # èŠ‚ç‚¹IDçš„ç®€å•ç¼–ç 
            0.5,  # å‡è®¾çš„è´Ÿè½½
            1.0,  # å‡è®¾çš„å¯ç”¨æ€§
        ]
        
        # å°†èŠ‚ç‚¹ç‰¹å¾ä¸çŠ¶æ€åµŒå…¥è¿æ¥
        node_tensor = torch.tensor(node_features, dtype=torch.float32, device=self.device)
        
        # ç¡®ä¿state_embeddingæ˜¯1Då¼ é‡
        if state_embedding.dim() > 1:
            state_embedding = state_embedding.flatten()
        
        # æˆªæ–­æˆ–å¡«å……çŠ¶æ€åµŒå…¥åˆ°åˆé€‚çš„å¤§å°
        if len(state_embedding) > 29:
            state_embedding = state_embedding[:29]
        else:
            padding = torch.zeros(29 - len(state_embedding), device=self.device)
            state_embedding = torch.cat([state_embedding, padding])
            
        combined = torch.cat([state_embedding, node_tensor])
        return combined
    
    def _build_graph_data(self, state: SchedulingState):
        """æ„å»ºPyTorch Geometricå›¾æ•°æ®"""
        if not HAS_TORCH_GEOMETRIC:
            # å¦‚æœæ²¡æœ‰torch_geometricï¼Œè¿”å›Noneï¼Œä¼šé™çº§åˆ°ç®€å•ç‰¹å¾æå–
            print(f"âš ï¸  [DEGRADATION] torch_geometric not available, graph data will be None for {state.current_task}")
            return None
            
        try:
            from torch_geometric.data import Data
            
            # ä»å·¥ä½œæµå›¾ä¸­æå–ä»»åŠ¡å’Œä¾èµ–ä¿¡æ¯
            tasks = state.workflow_graph.get("tasks", [])
            task_requirements = state.workflow_graph.get("task_requirements", {})
            dependencies = state.workflow_graph.get("dependencies", {})
            
            # æ„å»ºèŠ‚ç‚¹ç‰¹å¾
            node_features = []
            for task in tasks:
                # å¤„ç†taskå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–å­—å…¸çš„æƒ…å†µ
                if isinstance(task, str):
                    task_id = task
                    task_info = task_requirements.get(task_id, {})
                    task_deps = dependencies.get(task_id, [])
                else:
                    task_id = task.get("id", str(task))
                    task_info = task
                    task_deps = task.get("dependencies", [])
                
                # æå–ä»»åŠ¡ç‰¹å¾
                cpu_req = task_info.get("cpu", task_info.get("flops", 2.0))
                mem_req = task_info.get("memory", 4.0)
                duration = task_info.get("duration", task_info.get("runtime", 5.0))
                
                task_features = [
                    min(1.0, float(cpu_req) / 10.0),  # å½’ä¸€åŒ–CPUéœ€æ±‚
                    min(1.0, float(mem_req) / 16.0),  # å½’ä¸€åŒ–å†…å­˜éœ€æ±‚
                    min(1.0, float(duration) / 20.0), # å½’ä¸€åŒ–æ‰§è¡Œæ—¶é—´
                    1.0 if task_id == state.current_task else 0.0,  # å½“å‰ä»»åŠ¡æ ‡è®°
                    1.0 if task_id in state.pending_tasks else 0.0,  # å¾…è°ƒåº¦æ ‡è®°
                    min(1.0, len(task_deps) / 5.0),  # å½’ä¸€åŒ–ä¾èµ–æ•°é‡
                    0.0,  # ä¿ç•™å­—æ®µ
                    0.0   # ä¿ç•™å­—æ®µ
                ]
                node_features.append(task_features)
            
            # æ„å»ºè¾¹ç´¢å¼•ï¼ˆä¾èµ–å…³ç³»ï¼‰
            edge_index = []
            task_to_index = {}
            
            # å»ºç«‹ä»»åŠ¡IDåˆ°ç´¢å¼•çš„æ˜ å°„
            for i, task in enumerate(tasks):
                task_id = task if isinstance(task, str) else task.get("id", str(task))
                task_to_index[task_id] = i
            
            # æ„å»ºä¾èµ–è¾¹
            for i, task in enumerate(tasks):
                task_id = task if isinstance(task, str) else task.get("id", str(task))
                
                # è·å–ä¾èµ–å…³ç³»
                if isinstance(task, str):
                    task_deps = dependencies.get(task_id, [])
                else:
                    task_deps = task.get("dependencies", [])
                
                for dep in task_deps:
                    if dep in task_to_index:
                        dep_index = task_to_index[dep]
                        edge_index.append([dep_index, i])  # ä»ä¾èµ–ä»»åŠ¡åˆ°å½“å‰ä»»åŠ¡
            
            # è½¬æ¢ä¸ºå¼ é‡
            x = torch.tensor(node_features, dtype=torch.float32, device=self.device)
            
            if edge_index:
                edge_index = torch.tensor(edge_index, dtype=torch.long, device=self.device).t().contiguous()
            else:
                # å¦‚æœæ²¡æœ‰è¾¹ï¼Œåˆ›å»ºç©ºçš„è¾¹ç´¢å¼•
                edge_index = torch.empty((2, 0), dtype=torch.long, device=self.device)
            
            # åˆ›å»ºå›¾æ•°æ®
            graph_data = Data(x=x, edge_index=edge_index)
            
            return graph_data
            
        except Exception as e:
            print(f"âš ï¸  [DEGRADATION] Graph data construction failed: {e}")
            print(f"âš ï¸  [DEGRADATION] Falling back to None (will use simple features)")
            return None
    
    def load_model(self, model_path: str):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        try:
            checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
            if self.gnn_encoder is not None:
                self.gnn_encoder.load_state_dict(checkpoint.get("gnn_encoder", {}))
            self.policy_network.load_state_dict(checkpoint.get("policy_network", {}))
            print(f"Successfully loaded model from {model_path}")
        except Exception as e:
            print(f"Failed to load model from {model_path}: {e}")

class WASSRAGScheduler(BaseScheduler):
    """WASS-RAGè°ƒåº¦å™¨ - RAGå¢å¼ºçš„DRLæ–¹æ³•"""
    
    def __init__(self, model_path: Optional[str] = None, knowledge_base_path: Optional[str] = None):
        super().__init__("WASS-RAG")
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # ç»§æ‰¿æ™ºèƒ½è°ƒåº¦å™¨çš„èƒ½åŠ›
        self.base_scheduler = WASSSmartScheduler(model_path)
        # å†…ç½®ä¸€ä¸ªå¯å‘å¼è°ƒåº¦å™¨ç”¨äºé™çº§ä¸æ‰“ç ´å¹³å±€
        self.heuristic_scheduler = WASSHeuristicScheduler()
        self._last_state_embedding: Optional[torch.Tensor] = None  # è®°å½•æœ€è¿‘ä¸€æ¬¡çŠ¶æ€åµŒå…¥ç”¨äºçŸ¥è¯†åº“å¢é‡
        
        # RAGç»„ä»¶
        self.knowledge_base = RAGKnowledgeBase(knowledge_base_path)
        self.performance_predictor = PerformancePredictor(
            input_dim=96,  # state + action + context
            hidden_dim=128
        ).to(self.device)

        # åŠ è½½æ€§èƒ½é¢„æµ‹å™¨
        if model_path:
            self._load_performance_predictor(model_path)
    
    def make_decision(self, state: SchedulingState) -> SchedulingAction:
        """åŸºäºRAGå¢å¼ºçš„DRLå†³ç­–"""
        
        try:
            # 1. ç¼–ç å½“å‰çŠ¶æ€
            if self.base_scheduler.gnn_encoder is not None:
                state_embedding = self.base_scheduler._encode_state_graph(state)
            else:
                state_embedding = self.base_scheduler._extract_simple_features(state)
            # è®°å½•æœ€è¿‘çš„çŠ¶æ€åµŒå…¥
            self._last_state_embedding = state_embedding.detach().clone() if torch.is_tensor(state_embedding) else None
            
            # 2. ä»çŸ¥è¯†åº“æ£€ç´¢ç›¸ä¼¼å†å²æ¡ˆä¾‹
            retrieved_context = self.knowledge_base.retrieve_similar_cases(
                state_embedding.cpu().numpy(), top_k=5
            )
            
            # 3. ä¸ºæ¯ä¸ªå¯ç”¨èŠ‚ç‚¹è®¡ç®—RAGå¢å¼ºçš„å¾—åˆ†
            node_makespans = {}  # å­˜å‚¨é¢„æµ‹çš„makespan
            node_scores = {}     # å­˜å‚¨è¯„åˆ†ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
            historical_optimal = None
            
            for node in state.available_nodes:
                # ç¼–ç åŠ¨ä½œï¼ˆèŠ‚ç‚¹é€‰æ‹©ï¼‰
                action_embedding = self._encode_action(node, state)
                
                # é¢„æµ‹æ€§èƒ½
                predicted_makespan = self._predict_performance(
                    state_embedding, action_embedding, retrieved_context
                )
                
                # å­˜å‚¨makespanå’Œè®¡ç®—è¯„åˆ†
                node_makespans[node] = predicted_makespan
                # è¯„åˆ† = 1/makespanï¼Œmakespanè¶Šå°è¯„åˆ†è¶Šé«˜
                node_scores[node] = 1.0 / max(predicted_makespan, 0.01)  # é¿å…é™¤é›¶
                
                # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºæ¯ä¸ªèŠ‚ç‚¹çš„é¢„æµ‹ï¼ˆç”Ÿäº§ç¯å¢ƒå¯æ³¨é‡Šæ‰ï¼‰
                # print(f"ğŸ” [DEBUG] Node {node}: makespan={predicted_makespan:.2f}s, score={node_scores[node]:.3f}")
                
                # è®°å½•å†å²æœ€ä¼˜
                if historical_optimal is None or predicted_makespan < historical_optimal:
                    historical_optimal = predicted_makespan
            
            # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºæ‰€æœ‰èŠ‚ç‚¹åˆ†æ•°ï¼ˆç”Ÿäº§ç¯å¢ƒå¯æ³¨é‡Šæ‰ï¼‰
            # print(f"ğŸ” [DEBUG] All node scores: {node_scores}")
            
            # 4. é€‰æ‹©é¢„æµ‹æ€§èƒ½æœ€å¥½çš„èŠ‚ç‚¹ï¼ˆè¯„åˆ†æœ€é«˜çš„ï¼‰
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰èŠ‚ç‚¹å¾—åˆ†ç›¸åŒï¼ˆæœªè®­ç»ƒæ¨¡å‹çš„æ ‡å¿—ï¼‰
            score_values = list(node_scores.values())
            unique_scores = set(score_values)
            
            if len(unique_scores) == 1:
                # æ‰€æœ‰å¾—åˆ†ç›¸åŒ -> æ¨¡å‹æœªåŒºåˆ†ï¼Œå°è¯•å¯å‘å¼å†æ¬¡æ‰“åˆ†
                print(f"âš ï¸ [DEGRADATION] All node scores identical ({score_values[0]:.3f}). Applying heuristic tie-break.")
                try:
                    heuristic_action = self.heuristic_scheduler.make_decision(state)
                    best_node = heuristic_action.target_node
                    base_confidence = min(0.55, 0.35 + heuristic_action.confidence * 0.3)
                    # å°†å¯å‘å¼å¾—åˆ†æ³¨å…¥ç”¨äºè§£é‡Šï¼ˆä¸æ”¹å˜åŸnode_scoresç»“æ„ï¼Œä»…é™„åŠ ï¼‰
                    node_scores = {k: v for k, v in node_scores.items()}  # æ‹·è´
                    node_scores[f"heuristic:{best_node}"] = node_scores.get(best_node, score_values[0]) + 1e-4
                except Exception as tie_e:
                    # å¯å‘å¼å¤±è´¥ï¼Œå†é‡‡ç”¨ç¡®å®šæ€§å¤šæ ·åŒ–ç­–ç•¥
                    print(f"âš ï¸ [DEGRADATION] Heuristic tie-break failed: {tie_e}. Fallback to deterministic diversification")
                    task_hash = hash(state.current_task) if state.current_task else 0
                    node_list = sorted(node_scores.keys())
                    selected_index = task_hash % len(node_list)
                    best_node = node_list[selected_index]
                    base_confidence = 0.3
            else:
                # æ­£å¸¸é€‰æ‹©æœ€ä½³èŠ‚ç‚¹
                best_node = max(node_scores.keys(), key=lambda k: node_scores[k])
                
                # åŸºäºå¾—åˆ†å·®å¼‚è®¡ç®—ç½®ä¿¡åº¦
                score_range = max(score_values) - min(score_values)
                base_confidence = 0.5 + min(0.4, score_range * 2)  # 0.5-0.9èŒƒå›´
            
            # 5. è®¡ç®—RAGå¥–åŠ±ä¿¡å·ï¼ˆå®é™…ç”¨äºè®­ç»ƒæ—¶ï¼‰
            rag_reward = self._calculate_rag_reward(node_scores, best_node, retrieved_context)
            
            # 6. ç”Ÿæˆå¯è§£é‡Šçš„å†³ç­–ç†ç”±
            reasoning = self._generate_explanation(best_node, retrieved_context, node_scores, node_makespans)
            
            # ä½¿ç”¨æ”¹è¿›çš„ç½®ä¿¡åº¦è®¡ç®—
            confidence = base_confidence
            
            return SchedulingAction(
                task_id=state.current_task,
                target_node=best_node,
                confidence=confidence,
                reasoning=reasoning
            )
            
        except Exception as e:
            print(f"âš ï¸  [DEGRADATION] RAG decision making failed: {e}")
            print(f"âš ï¸  [DEGRADATION] WASSRAGScheduler falling back to base DRL method")
            print(f"âš ï¸  [DEGRADATION] Task: {state.current_task}, Attempting base scheduler...")
            
            # é™çº§åˆ°åŸºç¡€DRLæ–¹æ³•
            fallback_action = self.base_scheduler.make_decision(state)
            print(f"âš ï¸  [DEGRADATION] Base DRL fallback result: {fallback_action.target_node}")
            
            # ä¿®æ”¹reasoningä»¥æ ‡æ˜è¿™æ˜¯é™çº§å†³ç­–
            fallback_action.reasoning = f"ğŸ”´ DEGRADED: RAG->DRL fallback due to error: {e}"
            
            return fallback_action
    
    def _encode_action(self, node: str, state: SchedulingState) -> torch.Tensor:
        """ç¼–ç è°ƒåº¦åŠ¨ä½œ"""
        # ç®€åŒ–çš„åŠ¨ä½œç¼–ç 
        node_info = state.cluster_state.get("nodes", {}).get(node, {}) if state and state.cluster_state else {}
        current_load = float(node_info.get("current_load", 0.5))
        cpu_cap = float(node_info.get("cpu_capacity", 2.0))  # ä¸è®­ç»ƒæ•°æ®ä¸€è‡´
        mem_cap = float(node_info.get("memory_capacity", 16.0))
        # å½’ä¸€åŒ–å®¹é‡ï¼ˆä½¿ç”¨ä¸è®­ç»ƒæ•°æ®ä¸€è‡´çš„èŒƒå›´ï¼‰
        cpu_norm = min(1.0, cpu_cap / 5.0)  # è®­ç»ƒæ—¶æœ€å¤§æ˜¯5.0 GFlops
        mem_norm = min(1.0, mem_cap / 64.0)
        action_features = [
            hash(node) % 100 / 100.0,            # èŠ‚ç‚¹IDå“ˆå¸Œ
            len(state.available_nodes),          # å¯ç”¨èŠ‚ç‚¹æ•°
            1.0 if node == state.available_nodes[0] else 0.0,  # æ˜¯å¦åˆ—è¡¨é¦–èŠ‚ç‚¹
            current_load,                        # å½“å‰è´Ÿè½½
            1.0 - current_load,                  # ç©ºé—²åº¦
            cpu_norm,                            # CPUå®¹é‡å½’ä¸€åŒ–
            mem_norm,                            # å†…å­˜å®¹é‡å½’ä¸€åŒ–
        ]
        
        # å¡«å……åˆ°å›ºå®šé•¿åº¦
        while len(action_features) < 32:
            action_features.append(0.0)
            
        return torch.tensor(action_features[:32], dtype=torch.float32, device=self.device)
    
    def _predict_performance(self, state_embedding: torch.Tensor, 
                           action_embedding: torch.Tensor,
                           context: Dict[str, Any]) -> float:
        """ä½¿ç”¨æ€§èƒ½é¢„æµ‹å™¨é¢„æµ‹makespan"""
        
        # ç¼–ç æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
        context_embedding = self._encode_context(context)
        
        # è¿æ¥æ‰€æœ‰ç‰¹å¾
        # ç¡®ä¿æ‰€æœ‰åµŒå…¥éƒ½æ˜¯1Då¼ é‡
        state_flat = state_embedding.flatten()[:32]
        action_flat = action_embedding.flatten()[:32]
        context_flat = context_embedding.flatten()[:32]
        
        # å¡«å……åˆ°32ç»´
        def pad_to_32(tensor):
            if len(tensor) < 32:
                padding = torch.zeros(32 - len(tensor), device=tensor.device)
                return torch.cat([tensor, padding])
            return tensor[:32]
        
        combined_features = torch.cat([
            pad_to_32(state_flat),
            pad_to_32(action_flat),
            pad_to_32(context_flat)
        ])
        
        # é¢„æµ‹æ€§èƒ½
        with torch.no_grad():
            predicted_makespan_normalized = self.performance_predictor(combined_features).item()
            
            # åå½’ä¸€åŒ–é¢„æµ‹ç»“æœï¼ˆå¦‚æœæœ‰è®­ç»ƒå…ƒæ•°æ®ï¼‰
            if hasattr(self, '_y_mean') and hasattr(self, '_y_std'):
                # ç›´æ¥åå½’ä¸€åŒ–ï¼Œä¸è¿›è¡Œè¿‡åº¦çº¦æŸ
                predicted_makespan = predicted_makespan_normalized * self._y_std + self._y_mean
                
                # è°ƒè¯•ä¿¡æ¯ï¼ˆç”Ÿäº§ç¯å¢ƒå¯æ³¨é‡Šæ‰ï¼‰
                # print(f"ğŸ” [DEBUG] PerformancePredictor: normalized={predicted_makespan_normalized:.3f}, denormalized={predicted_makespan:.2f}")
                
                # åªæœ‰åœ¨é¢„æµ‹å€¼æ˜æ˜¾ä¸åˆç†æ—¶æ‰è¿›è¡Œçº¦æŸ
                # å•ä»»åŠ¡æ‰§è¡Œæ—¶é—´åº”è¯¥åœ¨ 0.5-300 ç§’ä¹‹é—´
                if predicted_makespan < 0.5:
                    print(f"ğŸ”§ [CONSTRAINT] Too small prediction {predicted_makespan:.2f}, adjusting to 0.5")
                    predicted_makespan = 0.5
                elif predicted_makespan > 300.0:
                    print(f"ğŸ”§ [CONSTRAINT] Excessive prediction {predicted_makespan:.2f}, adjusting to 300.0")
                    predicted_makespan = 300.0
            else:
                # æ²¡æœ‰å½’ä¸€åŒ–å‚æ•°ï¼Œå¯èƒ½æ˜¯æœªè®­ç»ƒæ¨¡å‹
                predicted_makespan = abs(predicted_makespan_normalized) if predicted_makespan_normalized != 0 else 1.0
                print(f"ğŸ” [DEBUG] PerformancePredictor: raw={predicted_makespan:.2f}")
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœªè®­ç»ƒæ¨¡å‹ï¼ˆè¾“å‡ºå¼‚å¸¸å€¼ï¼‰
            if abs(predicted_makespan_normalized) < 0.01:  # åªæœ‰æ¥è¿‘é›¶çš„è¾“å‡ºæ‰è®¤ä¸ºæ˜¯æœªè®­ç»ƒ
                # ä½¿ç”¨å¯å‘å¼æ›¿ä»£ï¼Œå¢åŠ ä¸€äº›éšæœºæ€§
                node_index = int(action_embedding[0].item()) if len(action_embedding) > 0 else 0
                base_prediction = 10.0 + node_index * 2.0  # åŸºäºèŠ‚ç‚¹çš„ä¸åŒé¢„æµ‹
                # æ·»åŠ åŸºäºç‰¹å¾çš„å˜åŒ–
                feature_variance = torch.std(combined_features).item() * 5
                predicted_makespan = base_prediction + feature_variance
                print(f"âš ï¸ [DEGRADATION] Performance predictor appears untrained (output={predicted_makespan_normalized:.6f}), using heuristic fallback")
            
        return predicted_makespan
    
    def _encode_context(self, context: Dict[str, Any]) -> torch.Tensor:
        """ç¼–ç æ£€ç´¢åˆ°çš„å†å²ä¸Šä¸‹æ–‡"""
        if not context or "similar_cases" not in context:
            return torch.zeros(32, device=self.device)
        
        # æå–å†å²æ¡ˆä¾‹çš„ç‰¹å¾
        similar_cases = context["similar_cases"]
        if not similar_cases:
            return torch.zeros(32, device=self.device)
        
        # ç®€åŒ–çš„ä¸Šä¸‹æ–‡ç‰¹å¾
        features = []
        
        # å¹³å‡makespan
        makespans = [case.get("makespan", 100.0) for case in similar_cases]
        avg_makespan = np.mean(makespans) if makespans else 100.0
        min_makespan = np.min(makespans) if makespans else 100.0
        max_makespan = np.max(makespans) if makespans else 100.0
        
        features.extend([avg_makespan/100.0, min_makespan/100.0, max_makespan/100.0])
        
        # æ¡ˆä¾‹æ•°é‡
        features.append(len(similar_cases) / 10.0)
        
        # å¹³å‡ç›¸ä¼¼åº¦
        similarities = [case.get("similarity", 0.5) for case in similar_cases]
        avg_similarity = np.mean(similarities) if similarities else 0.5
        features.append(avg_similarity)
        
        # å¡«å……åˆ°32ç»´
        while len(features) < 32:
            features.append(0.0)
            
        return torch.tensor(features[:32], dtype=torch.float32, device=self.device)
    
    def _calculate_rag_reward(self, node_scores: Dict[str, float], 
                            chosen_node: str, context: Dict[str, Any]) -> float:
        """è®¡ç®—RAGå¥–åŠ±ä¿¡å·"""
        
        # æ‰¾åˆ°å†å²æœ€ä¼˜åŠ¨ä½œ
        if context and "similar_cases" in context:
            historical_best_makespan = float('inf')
            for case in context["similar_cases"]:
                case_makespan = case.get("makespan", float('inf'))
                if case_makespan < historical_best_makespan:
                    historical_best_makespan = case_makespan
        else:
            historical_best_makespan = 100.0
        
        # å½“å‰åŠ¨ä½œçš„é¢„æµ‹makespan
        current_predicted_makespan = -node_scores[chosen_node]
        
        # RAGå¥–åŠ± = å†å²æœ€ä¼˜ - å½“å‰é¢„æµ‹ (è¶Šå¤§è¶Šå¥½)
        rag_reward = historical_best_makespan - current_predicted_makespan
        
        return rag_reward
    
    def _generate_explanation(self, chosen_node: str, context: Dict[str, Any], 
                            node_scores: Dict[str, float], node_makespans: Dict[str, float]) -> str:
        """ç”Ÿæˆå¯è§£é‡Šçš„å†³ç­–è¯´æ˜"""
        
        explanation_parts = []
        
        # åŸºç¡€å†³ç­–ä¿¡æ¯
        explanation_parts.append(f"RAG-enhanced decision: chose node {chosen_node}")
        
        # æ€§èƒ½é¢„æµ‹ä¿¡æ¯
        predicted_makespan = node_makespans[chosen_node]
        explanation_parts.append(f"predicted makespan: {predicted_makespan:.2f}s")
        
        # å†å²æ¡ˆä¾‹ä¿¡æ¯
        if context and "similar_cases" in context:
            similar_cases = context["similar_cases"]
            if similar_cases:
                avg_historical_makespan = np.mean([case.get("makespan", 100.0) for case in similar_cases])
                explanation_parts.append(f"based on {len(similar_cases)} similar historical cases")
                explanation_parts.append(f"historical avg makespan: {avg_historical_makespan:.2f}s")
        
        # æ˜¾ç¤ºæ‰€æœ‰èŠ‚ç‚¹çš„makespanï¼ˆæ›´ç›´è§‚ï¼‰
        sorted_by_makespan = sorted(node_makespans.items(), key=lambda x: x[1])
        top_3 = sorted_by_makespan[:3]
        makespan_str = ", ".join([f"{node}:{makespan:.2f}s" for node, makespan in top_3])
        explanation_parts.append(f"top choices: {makespan_str}")
        
        return "; ".join(explanation_parts)
    
    def _load_performance_predictor(self, model_path: str):
        """åŠ è½½æ€§èƒ½é¢„æµ‹å™¨æ¨¡å‹"""
        try:
            # ä¿®å¤PyTorch 2.6å…¼å®¹æ€§é—®é¢˜
            checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
            if "performance_predictor" in checkpoint:
                self.performance_predictor.load_state_dict(checkpoint["performance_predictor"])
                print("Successfully loaded performance predictor")
                
                # åŠ è½½å½’ä¸€åŒ–å‚æ•°
                if "metadata" in checkpoint and "performance_predictor" in checkpoint["metadata"]:
                    metadata = checkpoint["metadata"]["performance_predictor"]
                    if isinstance(metadata, dict):
                        self._y_mean = metadata.get("y_mean", 0.0)
                        self._y_std = metadata.get("y_std", 1.0)
                        print(f"Loaded normalization params: mean={self._y_mean:.2f}, std={self._y_std:.2f}")
                    else:
                        self._y_mean = 0.0
                        self._y_std = 1.0
                        print("No normalization metadata found, using default values")
                else:
                    self._y_mean = 0.0
                    self._y_std = 1.0
                    print("No normalization metadata found, using default values")
        except Exception as e:
            print(f"Failed to load performance predictor: {e}")
            self._y_mean = 0.0
            self._y_std = 1.0

# ç¥ç»ç½‘ç»œç»„ä»¶
class GraphEncoder(nn.Module):
    """GNNçŠ¶æ€ç¼–ç å™¨"""
    
    def __init__(self, node_feature_dim: int, edge_feature_dim: int, 
                 hidden_dim: int, output_dim: int):
        super().__init__()
        
        if not HAS_TORCH_GEOMETRIC:
            raise ImportError("torch_geometric is required for GraphEncoder")
            
        self.node_embedding = nn.Linear(node_feature_dim, hidden_dim)
        self.edge_embedding = nn.Linear(edge_feature_dim, hidden_dim)
        
        self.conv1 = GCNConv(hidden_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, hidden_dim)
        self.conv3 = GATConv(hidden_dim, output_dim, heads=1)
        
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, graph_data: Data) -> torch.Tensor:
        x, edge_index = graph_data.x, graph_data.edge_index
        
        # èŠ‚ç‚¹ç‰¹å¾åµŒå…¥
        x = F.relu(self.node_embedding(x))
        
        # GNNå±‚
        x = F.relu(self.conv1(x, edge_index))
        x = self.dropout(x)
        x = F.relu(self.conv2(x, edge_index))
        x = self.dropout(x)
        x = self.conv3(x, edge_index)
        
        # å…¨å±€æ± åŒ–
        batch = getattr(graph_data, 'batch', None)
        if batch is not None:
            x = global_mean_pool(x, batch)
        else:
            x = x.mean(dim=0, keepdim=True)
            
        return x

class PolicyNetwork(nn.Module):
    """ç­–ç•¥ç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int):
        super().__init__()
        
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, action_dim)
        )
        
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        return self.network(state)

class PerformancePredictor(nn.Module):
    """æ€§èƒ½é¢„æµ‹å™¨ç½‘ç»œ"""
    
    def __init__(self, input_dim: int, hidden_dim: int):
        super().__init__()
        
        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)
            # ç§»é™¤æœ€åçš„ReLU - å½’ä¸€åŒ–æ•°æ®å¯èƒ½åŒ…å«è´Ÿå€¼
        )
        
    def forward(self, features: torch.Tensor) -> torch.Tensor:
        return self.network(features)

class RAGKnowledgeBase:
    """RAGçŸ¥è¯†åº“"""
    
    def __init__(self, knowledge_base_path: Optional[str] = None, embedding_dim: int = 32):
        self.knowledge_base_path = knowledge_base_path
        self.embedding_dim = embedding_dim  # ä¿å­˜embeddingç»´åº¦
        self.index = None
        self.cases = []
        
        if knowledge_base_path and Path(knowledge_base_path).exists():
            self.load_knowledge_base(knowledge_base_path)
        else:
            self._initialize_empty_kb()
    
    def _initialize_empty_kb(self):
        """åˆå§‹åŒ–ç©ºçš„çŸ¥è¯†åº“"""
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„FAISSç´¢å¼•
        self.index = faiss.IndexFlatIP(self.embedding_dim)  # å†…ç§¯ç›¸ä¼¼åº¦
        self.cases = []
        print("Initialized empty knowledge base")
    
    def retrieve_similar_cases(self, query_embedding: np.ndarray, top_k: int = 5) -> Dict[str, Any]:
        """æ£€ç´¢ç›¸ä¼¼çš„å†å²æ¡ˆä¾‹"""
        
        if self.index is None or self.index.ntotal == 0:
            # è¿”å›ç©ºç»“æœ
            return {
                "similar_cases": [],
                "query_embedding": query_embedding.tolist(),
                "top_k": top_k
            }
        
        try:
            # ç¡®ä¿æŸ¥è¯¢å‘é‡æ˜¯æ­£ç¡®çš„å½¢çŠ¶å’Œè¿ç»­å†…å­˜å¸ƒå±€
            query_vector = np.ascontiguousarray(
                query_embedding.reshape(1, -1), 
                dtype=np.float32
            )
            
            # æ£€ç´¢
            similarities, indices = self.index.search(query_vector, min(top_k, self.index.ntotal))
            
            # æ„å»ºç»“æœ
            similar_cases = []
            for i, (similarity, idx) in enumerate(zip(similarities[0], indices[0])):
                if idx < len(self.cases):
                    case = self.cases[idx].copy()
                    case["similarity"] = float(similarity)
                    case["rank"] = i + 1
                    similar_cases.append(case)
            
            return {
                "similar_cases": similar_cases,
                "query_embedding": query_embedding.tolist(),
                "top_k": top_k
            }
            
        except Exception as e:
            print(f"Error in knowledge base retrieval: {e}")
            return {
                "similar_cases": [],
                "query_embedding": query_embedding.tolist(),
                "top_k": top_k
            }

    def add_case(self, embedding: np.ndarray, workflow_info: Dict[str, Any],
                actions: List[str], makespan: float):
        """æ·»åŠ æ–°æ¡ˆä¾‹åˆ°çŸ¥è¯†åº“ï¼ˆæœ€ç»ˆç¨³å®šç‰ˆï¼‰"""

        case = {
            "workflow_info": workflow_info,
            "actions": actions,
            "makespan": makespan,
            "timestamp": str(np.datetime64('now'))
        }
        self.cases.append(case)

        if self.index is None:
            self._initialize_empty_kb()

        try:
            # ç¡®ä¿è¾“å…¥æ˜¯ float32 çš„ numpy æ•°ç»„
            embedding_np = np.asarray(embedding, dtype=np.float32)

            # ç¡®ä¿æ˜¯ 2D æ•°ç»„
            if embedding_np.ndim == 1:
                embedding_np = embedding_np.reshape(1, -1)

            # å…³é”®ï¼šåˆ›å»ºä¸€ä¸ªæ‹¥æœ‰è‡ªå·±æ•°æ®çš„ã€C-è¿ç»­çš„å‰¯æœ¬
            embedding_final = np.copy(embedding_np)

            # éªŒè¯ç»´åº¦
            if embedding_final.shape[1] != self.embedding_dim:
                raise ValueError(f"é”™è¯¯çš„ embedding ç»´åº¦: {embedding_final.shape[1]} vs {self.embedding_dim}")

            # è°ƒç”¨ FAISS
            self.index.add(embedding_final)

        except Exception as e:
            print(f"\nå‘çŸ¥è¯†åº“æ·»åŠ æ¡ˆä¾‹æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
            self.cases.pop() # å‡ºé”™æ—¶ç§»é™¤æ¡ˆä¾‹ï¼Œä¿æŒæ•°æ®ä¸€è‡´æ€§
            raise

    def load_knowledge_base(self, path: str):
        """åŠ è½½çŸ¥è¯†åº“"""
        try:
            with open(path, 'rb') as f:
                data = pickle.load(f)
                self.index = data.get("index")
                self.cases = data.get("cases", [])
            print(f"Loaded knowledge base with {len(self.cases)} cases")
        except Exception as e:
            print(f"Failed to load knowledge base: {e}")
            self._initialize_empty_kb()
    
    def save_knowledge_base(self, path: str):
        """ä¿å­˜çŸ¥è¯†åº“"""
        try:
            Path(path).parent.mkdir(parents=True, exist_ok=True)
            with open(path, 'wb') as f:
                pickle.dump({
                    "index": self.index,
                    "cases": self.cases
                }, f)
            print(f"Saved knowledge base with {len(self.cases)} cases to {path}")
        except Exception as e:
            print(f"Failed to save knowledge base: {e}")

def create_scheduler(method_name: str, **kwargs) -> BaseScheduler:
    """å·¥å‚å‡½æ•°ï¼šåˆ›å»ºè°ƒåº¦å™¨å®ä¾‹"""
    
    if method_name == "WASS (Heuristic)":
        return WASSHeuristicScheduler()
    elif method_name == "WASS-DRL (w/o RAG)":
        return WASSSmartScheduler(kwargs.get("model_path"))
    elif method_name == "WASS-RAG":
        return WASSRAGScheduler(
            kwargs.get("model_path"),
            kwargs.get("knowledge_base_path")
        )
    else:
        raise ValueError(f"Unknown scheduler method: {method_name}")
