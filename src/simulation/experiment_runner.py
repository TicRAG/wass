# src/utils.py
"""Â∑•ÂÖ∑ÂáΩÊï∞„ÄÅÊó•ÂøóËÆæÁΩÆÂíåÊúÄÁªàÁâàÁöÑÂÆûÈ™åËøêË°åÂô®„ÄÇ"""
from __future__ import annotations
import logging
import time
from contextlib import contextmanager
from typing import Dict, Any, Generator, List
from pathlib import Path
import sys
import random
import wrench
import pandas as pd
import numpy as np
import torch
import json
import importlib 

reference_flop_rate = '1Gf'  # or use '1000Mf' if preferred

def get_logger(name, level=logging.INFO):
    """Ëé∑Âèñ‰∏Ä‰∏™Â∑≤ÈÖçÁΩÆÁöÑÊó•ÂøóÂô®„ÄÇ"""
    logger = logging.getLogger(name)
    logger.setLevel(level)
    if not logger.handlers:
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler = logging.StreamHandler(sys.stdout)
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    return logger

class WrenchExperimentRunner:
    """Â§ÑÁêÜÂ§ö‰∏™WRENCHÊ®°ÊãüÁöÑÊâßË°åÔºå‰ª•ÊØîËæÉ‰∏çÂêåÁöÑË∞ÉÂ∫¶Âô®„ÄÇ"""
    def __init__(self, schedulers: Dict[str, Any], config: Dict[str, Any]):
        self.schedulers_map = schedulers
        self.config = config
        self.platform_file = config.get("platform_file")
        self.workflow_dir = Path(config.get("workflow_dir", "workflows"))
        self.workflow_sizes = config.get("workflow_sizes", [20, 50, 100])
        self.repetitions = config.get("repetitions", 3)
        self.output_dir = Path(config.get("output_dir", "results/final_experiments"))
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.random_seeds = config.get("random_seeds")
        self.include_aug = bool(config.get("include_aug", False))

    def _load_workflows(self, workflow_dir: Path) -> List[str]:
        base = sorted(str(p) for p in workflow_dir.glob("*.json"))
        if not base:
            return []
        if self.include_aug:
            aug_dir = workflow_dir.parent / "training_aug"
            if aug_dir.exists():
                aug_files = sorted(str(p) for p in aug_dir.glob("*.json"))
                if aug_files:
                    base = list(dict.fromkeys(base + aug_files))
        return base

    def run_single_seeding_simulation(self, scheduler_class: Any, workflow_file: str, scheduler_kwargs: Dict[str, Any] | None = None) -> tuple[float, Dict[str, Any]]:
        """ËøêË°å‰∏ÄÊ¨°Áî®‰∫éÁü•ËØÜÂ∫ì‚ÄúÊí≠Áßç‚ÄùÁöÑÊ®°Êãü„ÄÇ"""
        workflow_filename = Path(workflow_file).name
        scheduler_kwargs = scheduler_kwargs or {}
        try:
            with open(self.platform_file, "r") as f:
                platform_xml = f.read()
            simulation = wrench.Simulation()
            controller_host = "ControllerHost"
            simulation.start(platform_xml, controller_host)
            all_hostnames = simulation.get_all_hostnames()
            compute_hosts = [host for host in all_hostnames if host not in [controller_host, "StorageHost"]]
            storage_service = simulation.create_simple_storage_service("StorageHost", ["/storage"])
            min_host_speed = float(self.config.get("min_host_speed", 0.0) or 0.0)
            compute_services: Dict[str, Any] = {}
            hosts_properties: Dict[str, Dict[str, float]] = {}
            filtered_hosts: list[tuple[str, float]] = []
            for host in compute_hosts:
                service = simulation.create_bare_metal_compute_service(
                    host, {host: (-1, -1)}, "/scratch", {}, {})
                flop_rates = service.get_core_flop_rates()
                speed = list(flop_rates.values())[0] if flop_rates else 0.0
                speed_gf = speed / 1e9 if speed else 0.0
                if speed_gf < min_host_speed:
                    filtered_hosts.append((host, speed_gf))
                    continue
                compute_services[host] = service
                hosts_properties[host] = {"speed": speed}
            if not compute_services:
                raise RuntimeError(
                    "No compute hosts available after applying min_host_speed="
                    f"{min_host_speed}. Lower the threshold or adjust the platform configuration."
                )
            if filtered_hosts:
                skipped = ", ".join(f"{name}({speed:.2f} Gf/s)" for name, speed in filtered_hosts)
                print(f"‚öñÔ∏è  Filtered out slow hosts: {skipped}")
            
            with open(workflow_file, 'r') as f:
                workflow_data = json.load(f)
            
            workflow = self.create_workflow_from_json_data(simulation, workflow_data, workflow_file)
            
            scheduler_args = {
                "simulation": simulation,
                "compute_services": compute_services,
                "hosts": hosts_properties,
                "workflow_obj": workflow,
                "workflow_file": workflow_file
            }
            scheduler_args.update(scheduler_kwargs)
            scheduler_instance = scheduler_class(**scheduler_args)
            
            for file in workflow.get_input_files():
                storage_service.create_file_copy(file)
            if hasattr(scheduler_instance, 'schedule_ready_tasks'):
                scheduler_instance.schedule_ready_tasks(workflow, storage_service)
            while not workflow.is_done():
                event = simulation.wait_for_next_event()
                if event["event_type"] == "standard_job_completion":
                    job = event["standard_job"]
                    if hasattr(scheduler_instance, 'handle_completion'):
                        for task in job.get_tasks():
                            scheduler_instance.handle_completion(task)
                    if hasattr(scheduler_instance, 'schedule_ready_tasks'):
                        scheduler_instance.schedule_ready_tasks(workflow, storage_service)
                elif event["event_type"] == "simulation_termination":
                    break
            makespan = simulation.get_simulated_time()
            details: Dict[str, Any] = {}
            if hasattr(scheduler_instance, 'get_recorded_decisions'):
                details['decisions'] = scheduler_instance.get_recorded_decisions()
            if hasattr(scheduler_instance, 'get_knowledge_records'):
                try:
                    details['knowledge_records'] = scheduler_instance.get_knowledge_records(makespan)
                except TypeError:
                    # Fallback for implementations that do not accept makespan argument
                    details['knowledge_records'] = scheduler_instance.get_knowledge_records()
            if hasattr(scheduler_instance, 'get_potential_summary'):
                summary = scheduler_instance.get_potential_summary()
                if summary:
                    details['potential_summary'] = summary
            simulation.terminate()
            return makespan, details
        except Exception as e:
            import traceback
            print(f"ERROR running seeding simulation on {workflow_filename}: {e}")
            print(f"ËØ¶ÁªÜÈîôËØØ‰ø°ÊÅØ: {traceback.format_exc()}")
            return -1.0, {}

    def create_workflow_from_json_data(self, simulation, workflow_data, workflow_file_path):
        """‰ªé JSON Êï∞ÊçÆÂàõÂª∫ WRENCH Â∑•‰ΩúÊµÅÂØπË±°„ÄÇ

        ÊîØÊåÅ‰∏§ÁßçÊ†ºÂºè:
          1. È°πÁõÆÂÜÖÈÉ®ÁîüÊàêÊ†ºÂºè: workflow.tasks + metadata + workflow.files
          2. wfcommons ËΩ¨Êç¢Ê†ºÂºè: workflow.specification.tasks + workflow.specification.files + workflow.execution.tasks

        ÂØπ‰∫éÊ†ºÂºè2ÔºåÁõ¥Êé•‰ΩøÁî®ÂÖ∂‰ªªÂä°Âπ∂Êò†Â∞Ñ parents->children ÂÖ≥Á≥ªÔºõÂØπ‰∫éÊ†ºÂºè1ÔºåÊûÑÈÄ†‰∏Ä‰∏™ wfcommons ÂÖºÂÆπÂØπË±°„ÄÇ
        """
        wf_section = workflow_data.get('workflow', {})
        # Ensure top-level workflow_name exists for WRENCH parser (some augmented files lack it)
        if 'workflow_name' not in workflow_data:
            workflow_data['workflow_name'] = workflow_data.get('name', workflow_file_path and Path(workflow_file_path).stem or 'unknown')
        is_wfcommons_like = 'specification' in wf_section and 'tasks' in wf_section.get('specification', {})

        if is_wfcommons_like:
            # Â∑≤ÁªèÊòØËΩ¨Êç¢ÂêéÁöÑ wfcommons Ê†ºÂºè (scripts/0_convert_wfcommons.py ÁîüÊàê)
            spec = wf_section['specification']
            tasks = spec.get('tasks', [])
            # ÊûÑÂª∫ children ÂÖ≥Á≥ª: ‰æùÊçÆÊØè‰∏™‰ªªÂä°ÁöÑ parents ÂàóË°®
            task_children = {t.get('id'): [] for t in tasks if isinstance(t, dict)}
            for t in tasks:
                if not isinstance(t, dict):
                    continue
                for parent_id in t.get('parents', []) or []:
                    if parent_id in task_children:
                        task_children[parent_id].append(t.get('id'))
            # Â∞Ü children ÂÜôÂÖ•‰ªªÂä°Ôºà‰∏çÁ†¥ÂùèÂéüÊúâÁªìÊûÑÔºâ
            for t in tasks:
                if not isinstance(t, dict):
                    continue
                tid = t.get('id')
                if tid in task_children:
                    t.setdefault('children', task_children[tid])
            wfcommons_data = workflow_data  # Áõ¥Êé•‰ΩøÁî®
        else:
            # ÂéüÈ°πÁõÆÂÜÖÈÉ®Ê†ºÂºèÔºåÈúÄËΩ¨Êç¢Âà∞ wfcommons ÁªìÊûÑ
            internal_tasks = wf_section.get('tasks', [])
            task_children = {t.get('id'): [] for t in internal_tasks if isinstance(t, dict)}
            for t in internal_tasks:
                if not isinstance(t, dict):
                    continue
                for parent_id in t.get('dependencies', []) or []:
                    if parent_id in task_children:
                        task_children[parent_id].append(t.get('id'))
            metadata = workflow_data.get('metadata', {})
            wfcommons_data = {
                'name': metadata.get('name', 'unknown'),
                'workflow_name': metadata.get('name', 'unknown'),
                'description': metadata.get('description', ''),
                'schemaVersion': '1.5',
                'author': {'name': 'WRENCH Experiment', 'email': 'wrench@example.com'},
                'createdAt': metadata.get('generated_at', '2024-01-01T00:00:00Z'),
                'workflow': {'specification': {'tasks': [], 'files': []}, 'execution': {'tasks': []}}}
            for t in internal_tasks:
                if not isinstance(t, dict):
                    continue
                tid = t.get('id')
                wfcommons_data['workflow']['specification']['tasks'].append({
                    'name': t.get('name', tid), 'id': tid, 'children': task_children.get(tid, []),
                    'parents': t.get('dependencies', []), 'inputFiles': t.get('input_files', []),
                    'outputFiles': t.get('output_files', []), 'flops': t.get('flops', 0),
                    'memory': t.get('memory', 0)})
                wfcommons_data['workflow']['execution']['tasks'].append({
                    'id': tid, 'runtimeInSeconds': t.get('runtime', 1.0), 'cores': 1, 'avgCPU': 1.0})
            for f in wf_section.get('files', []):
                if not isinstance(f, dict):
                    continue
                wfcommons_data['workflow']['specification']['files'].append({
                    'id': f.get('id'), 'name': f.get('name'), 'sizeInBytes': f.get('size', 0)})
        if 'workflow_name' not in wfcommons_data:
            wfcommons_data['workflow_name'] = wfcommons_data.get('name', 'unknown')
        return simulation.create_workflow_from_json(
            wfcommons_data, reference_flop_rate=reference_flop_rate,
            ignore_machine_specs=False, redundant_dependencies=False,
            ignore_cycle_creating_dependencies=False, min_cores_per_task=1,
            max_cores_per_task=1, enforce_num_cores=True,
            ignore_avg_cpu=False, show_warnings=False)

    def _run_single_simulation(self, scheduler_name: str, scheduler_impl: Any, workflow_file: str, seed: int | None = None, extra_kwargs: Dict[str, Any] | None = None) -> Dict[str, Any]:
        """ËøêË°åÂçï‰∏™WRENCHÊ®°ÊãüÂπ∂ËøîÂõûÁªìÊûú„ÄÇ"""
        workflow_filename = Path(workflow_file).name
        try:
            if seed is not None:
                random.seed(seed)
                np.random.seed(seed)
                torch.manual_seed(seed)
            with open(self.platform_file, "r") as f:
                platform_xml = f.read()

            simulation = wrench.Simulation()
            controller_host = "ControllerHost"
            simulation.start(platform_xml, controller_host)

            all_hostnames = simulation.get_all_hostnames()
            compute_hosts_names = [h for h in all_hostnames if h not in [controller_host, "StorageHost"]]
            storage_service = simulation.create_simple_storage_service("StorageHost", ["/storage"])
            min_host_speed = float(self.config.get("min_host_speed", 0.0) or 0.0)
            compute_services: Dict[str, Any] = {}
            hosts_properties: Dict[str, Dict[str, float]] = {}
            filtered_hosts: list[tuple[str, float]] = []
            for host_name in compute_hosts_names:
                service = simulation.create_bare_metal_compute_service(
                    host_name, {host_name: (-1, -1)}, "/scratch", {}, {})
                flop_rates = service.get_core_flop_rates()
                speed = list(flop_rates.values())[0] if flop_rates else 0.0
                speed_gf = speed / 1e9 if speed else 0.0
                if speed_gf < min_host_speed:
                    filtered_hosts.append((host_name, speed_gf))
                    continue
                compute_services[host_name] = service
                hosts_properties[host_name] = {"speed": speed}
            if not compute_services:
                raise RuntimeError(
                    "No compute hosts available after applying min_host_speed="
                    f"{min_host_speed}. Lower the threshold or adjust the platform configuration."
                )
            if filtered_hosts:
                skipped = ", ".join(f"{name}({speed:.2f} Gf/s)" for name, speed in filtered_hosts)
                print(f"‚öñÔ∏è  Filtered out slow hosts: {skipped}")
            
            with open(workflow_file, 'r') as f:
                workflow_data = json.load(f)
            
            workflow = self.create_workflow_from_json_data(simulation, workflow_data, workflow_file)
            
            scheduler_args = {
                "simulation": simulation,
                "compute_services": compute_services,
                "hosts": hosts_properties,
                "workflow_obj": workflow,
                "workflow_file": workflow_file,
            }

            if extra_kwargs:
                scheduler_args.update(extra_kwargs)
            scheduler_instance = scheduler_impl(**scheduler_args)

            for file in workflow.get_input_files():
                storage_service.create_file_copy(file)
            
            if hasattr(scheduler_instance, 'schedule_ready_tasks'):
                scheduler_instance.schedule_ready_tasks(workflow, storage_service)
            
            while not workflow.is_done():
                event = simulation.wait_for_next_event()
                if event["event_type"] == "standard_job_completion":
                    job = event["standard_job"]
                    if hasattr(scheduler_instance, 'handle_completion'):
                        for task in job.get_tasks():
                            scheduler_instance.handle_completion(task)
                    if hasattr(scheduler_instance, 'schedule_ready_tasks'):
                        scheduler_instance.schedule_ready_tasks(workflow, storage_service)
                elif event["event_type"] == "simulation_termination":
                    break
            
            makespan = simulation.get_simulated_time()
            wf_section = workflow_data.get('workflow', {})
            if 'tasks' in wf_section and isinstance(wf_section.get('tasks'), list):
                task_count_source = wf_section.get('tasks')
            else:
                spec_tasks = wf_section.get('specification', {}).get('tasks', [])
                task_count_source = spec_tasks if isinstance(spec_tasks, list) else []
            task_count = len(task_count_source)
            simulation.terminate()
            result = {
                "scheduler": scheduler_name,
                "workflow": workflow_filename,
                "makespan": makespan,
                "status": "success",
                "task_count": task_count,
                "seed": seed,
            }
            if seed is not None:
                result["seed"] = seed
            return result
        except Exception as e:
            import traceback
            print(f"ERROR running {scheduler_name} on {workflow_filename}: {e}")
            print(f"ËØ¶ÁªÜÈîôËØØ‰ø°ÊÅØ: {traceback.format_exc()}")
            result = {
                "scheduler": scheduler_name,
                "workflow": workflow_filename,
                "makespan": float('inf'),
                "status": "failed",
                "task_count": 0,
                "seed": seed,
            }
            return result

    def analyze_results(self, results: List[Dict[str, Any]]):
        """ÂàÜÊûê„ÄÅÊâìÂç∞Âπ∂‰øùÂ≠òÂÆûÈ™åÁªìÊûúÊëòË¶Å„ÄÇ"""
        if not results:
            print("Ê≤°ÊúâÂèØ‰æõÂàÜÊûêÁöÑÂÆûÈ™åÁªìÊûú„ÄÇ"); return
        df = pd.DataFrame(results)
        detailed_csv_path = self.output_dir / "detailed_results.csv"
        df.to_csv(detailed_csv_path, index=False)
        print(f"‚úÖ ËØ¶ÁªÜÂÆûÈ™åÁªìÊûúÂ∑≤‰øùÂ≠òÂà∞: {detailed_csv_path}")
        # ÊéíÈô§Â§±Ë¥•ÁöÑËøêË°åÔºåÈÅøÂÖç inf Ê±°ÊüìÁªüËÆ°
        success_df = df[df['status'] == 'success'].copy()
        if success_df.empty:
            print("‚ö†Ô∏è ÂÖ®ÈÉ®ÂÆûÈ™åÂùáÂ§±Ë¥•ÔºåÊó†Ê≥ïÁîüÊàêÊàêÂäüÁªüËÆ°ÊëòË¶Å„ÄÇ")
            summary = pd.DataFrame({
                'Ë∞ÉÂ∫¶Âô®': df['scheduler'].unique(),
                'Âπ≥ÂùáMakespan': [float('inf')] * len(df['scheduler'].unique()),
                'Ê†áÂáÜÂ∑Æ': [None] * len(df['scheduler'].unique()),
                'ÊúÄ‰Ω≥': [float('inf')] * len(df['scheduler'].unique()),
                'ÂÆûÈ™åÊ¨°Êï∞': [0] * len(df['scheduler'].unique()),
                'ÊàêÂäüÊ¨°Êï∞': [0] * len(df['scheduler'].unique()),
                'Â§±Ë¥•Ê¨°Êï∞': [len(df[df['scheduler'] == s]) for s in df['scheduler'].unique()],
            })
        else:
            summary = success_df.groupby('scheduler')['makespan'].agg(['mean', 'std', 'min', 'count']).reset_index()
            summary = summary.rename(columns={'scheduler': 'Ë∞ÉÂ∫¶Âô®', 'mean': 'Âπ≥ÂùáMakespan', 'std': 'Ê†áÂáÜÂ∑Æ', 'min': 'ÊúÄ‰Ω≥', 'count': 'ÂÆûÈ™åÊ¨°Êï∞'})
            # ÈôÑÂä†ÊàêÂäü/Â§±Ë¥•ËÆ°Êï∞ (‰ΩøÁî®ÂéüÂßãÂàóÂêçËøõË°åÂàÜÁªÑ)
            success_counts = success_df.groupby('scheduler').size().to_dict()
            fail_counts = df[df['status'] != 'success'].groupby('scheduler').size().to_dict()
            summary['ÊàêÂäüÊ¨°Êï∞'] = summary['Ë∞ÉÂ∫¶Âô®'].map(success_counts).fillna(0).astype(int)
            summary['Â§±Ë¥•Ê¨°Êï∞'] = summary['Ë∞ÉÂ∫¶Âô®'].map(fail_counts).fillna(0).astype(int)
        print("\n" + "="*60); print("üìà ÂÆûÈ™åÁªìÊûúÂàÜÊûê:"); print(summary.to_string(index=False)); print("="*60 + "\n")
        summary_csv_path = self.output_dir / "summary_results.csv"
        summary.to_csv(summary_csv_path, index=False)
        print(f"‚úÖ ÂÆûÈ™åÁªìÊûúÊëòË¶ÅÂ∑≤‰øùÂ≠òÂà∞: {summary_csv_path}")