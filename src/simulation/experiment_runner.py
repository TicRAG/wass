# src/utils.py
"""å·¥å…·å‡½æ•°ã€æ—¥å¿—è®¾ç½®å’Œæœ€ç»ˆç‰ˆçš„å®éªŒè¿è¡Œå™¨ã€‚"""
from __future__ import annotations
import logging
import time
from contextlib import contextmanager
from typing import Dict, Any, Generator, List
from pathlib import Path
import sys
import wrench
import pandas as pd
import numpy as np
import json
import importlib 

reference_flop_rate = str(1000000000)+'Mf'

def get_logger(name, level=logging.INFO):
    """è·å–ä¸€ä¸ªå·²é…ç½®çš„æ—¥å¿—å™¨ã€‚"""
    logger = logging.getLogger(name)
    logger.setLevel(level)
    if not logger.handlers:
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler = logging.StreamHandler(sys.stdout)
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    return logger

class WrenchExperimentRunner:
    """å¤„ç†å¤šä¸ªWRENCHæ¨¡æ‹Ÿçš„æ‰§è¡Œï¼Œä»¥æ¯”è¾ƒä¸åŒçš„è°ƒåº¦å™¨ã€‚"""
    def __init__(self, schedulers: Dict[str, Any], config: Dict[str, Any]):
        self.schedulers_map = schedulers
        self.config = config
        self.platform_file = config.get("platform_file")
        self.workflow_dir = Path(config.get("workflow_dir", "workflows"))
        self.workflow_sizes = config.get("workflow_sizes", [20, 50, 100])
        self.repetitions = config.get("repetitions", 3)
        self.output_dir = Path(config.get("output_dir", "results/final_experiments"))
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def run_single_seeding_simulation(self, scheduler_class: Any, workflow_file: str) -> tuple[float, list]:
        """è¿è¡Œä¸€æ¬¡ç”¨äºçŸ¥è¯†åº“â€œæ’­ç§â€çš„æ¨¡æ‹Ÿã€‚"""
        workflow_filename = Path(workflow_file).name
        try:
            with open(self.platform_file, "r") as f:
                platform_xml = f.read()
            simulation = wrench.Simulation()
            controller_host = "ControllerHost"
            simulation.start(platform_xml, controller_host)
            all_hostnames = simulation.get_all_hostnames()
            compute_hosts = [host for host in all_hostnames if host not in [controller_host, "StorageHost"]]
            storage_service = simulation.create_simple_storage_service("StorageHost", ["/storage"])
            compute_services = {}
            for host in compute_hosts:
                compute_services[host] = simulation.create_bare_metal_compute_service(
                    host, {host: (-1, -1)}, "/scratch", {}, {})
            hosts_properties = {}
            for name, service in compute_services.items():
                flop_rates = service.get_core_flop_rates()
                speed = list(flop_rates.values())[0] if flop_rates else 0.0
                hosts_properties[name] = {"speed": speed}
            
            with open(workflow_file, 'r') as f:
                workflow_data = json.load(f)
            
            workflow = self.create_workflow_from_json_data(simulation, workflow_data, workflow_file)
            
            scheduler_args = {
                "simulation": simulation,
                "compute_services": compute_services,
                "hosts": hosts_properties,
                "workflow_obj": workflow,
                "workflow_file": workflow_file
            }
            scheduler_instance = scheduler_class(**scheduler_args)
            
            for file in workflow.get_input_files():
                storage_service.create_file_copy(file)
            if hasattr(scheduler_instance, 'schedule_ready_tasks'):
                scheduler_instance.schedule_ready_tasks(workflow, storage_service)
            while not workflow.is_done():
                event = simulation.wait_for_next_event()
                if event["event_type"] == "standard_job_completion":
                    job = event["standard_job"]
                    if hasattr(scheduler_instance, 'handle_completion'):
                        for task in job.get_tasks():
                            scheduler_instance.handle_completion(task)
                    if hasattr(scheduler_instance, 'schedule_ready_tasks'):
                        scheduler_instance.schedule_ready_tasks(workflow, storage_service)
                elif event["event_type"] == "simulation_termination":
                    break
            makespan = simulation.get_simulated_time()
            decisions = []
            if hasattr(scheduler_instance, 'get_recorded_decisions'):
                decisions = scheduler_instance.get_recorded_decisions()
            simulation.terminate()
            return makespan, decisions
        except Exception as e:
            import traceback
            print(f"ERROR running seeding simulation on {workflow_filename}: {e}")
            print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return -1.0, []

    def create_workflow_from_json_data(self, simulation, workflow_data, workflow_file_path):
        """ä» JSON æ•°æ®åˆ›å»º WRENCH å·¥ä½œæµå¯¹è±¡ã€‚

        æ”¯æŒä¸¤ç§æ ¼å¼:
          1. é¡¹ç›®å†…éƒ¨ç”Ÿæˆæ ¼å¼: workflow.tasks + metadata + workflow.files
          2. wfcommons è½¬æ¢æ ¼å¼: workflow.specification.tasks + workflow.specification.files + workflow.execution.tasks

        å¯¹äºæ ¼å¼2ï¼Œç›´æ¥ä½¿ç”¨å…¶ä»»åŠ¡å¹¶æ˜ å°„ parents->children å…³ç³»ï¼›å¯¹äºæ ¼å¼1ï¼Œæ„é€ ä¸€ä¸ª wfcommons å…¼å®¹å¯¹è±¡ã€‚
        """
        wf_section = workflow_data.get('workflow', {})
        is_wfcommons_like = 'specification' in wf_section and 'tasks' in wf_section.get('specification', {})

        if is_wfcommons_like:
            # å·²ç»æ˜¯è½¬æ¢åçš„ wfcommons æ ¼å¼ (scripts/0_convert_wfcommons.py ç”Ÿæˆ)
            spec = wf_section['specification']
            tasks = spec.get('tasks', [])
            # æ„å»º children å…³ç³»: ä¾æ®æ¯ä¸ªä»»åŠ¡çš„ parents åˆ—è¡¨
            task_children = {t.get('id'): [] for t in tasks if isinstance(t, dict)}
            for t in tasks:
                if not isinstance(t, dict):
                    continue
                for parent_id in t.get('parents', []) or []:
                    if parent_id in task_children:
                        task_children[parent_id].append(t.get('id'))
            # å°† children å†™å…¥ä»»åŠ¡ï¼ˆä¸ç ´ååŸæœ‰ç»“æ„ï¼‰
            for t in tasks:
                if not isinstance(t, dict):
                    continue
                tid = t.get('id')
                if tid in task_children:
                    t.setdefault('children', task_children[tid])
            wfcommons_data = workflow_data  # ç›´æ¥ä½¿ç”¨
        else:
            # åŸé¡¹ç›®å†…éƒ¨æ ¼å¼ï¼Œéœ€è½¬æ¢åˆ° wfcommons ç»“æ„
            internal_tasks = wf_section.get('tasks', [])
            task_children = {t.get('id'): [] for t in internal_tasks if isinstance(t, dict)}
            for t in internal_tasks:
                if not isinstance(t, dict):
                    continue
                for parent_id in t.get('dependencies', []) or []:
                    if parent_id in task_children:
                        task_children[parent_id].append(t.get('id'))
            metadata = workflow_data.get('metadata', {})
            wfcommons_data = {
                'name': metadata.get('name', 'unknown'),
                'workflow_name': metadata.get('name', 'unknown'),
                'description': metadata.get('description', ''),
                'schemaVersion': '1.5',
                'author': {'name': 'WRENCH Experiment', 'email': 'wrench@example.com'},
                'createdAt': metadata.get('generated_at', '2024-01-01T00:00:00Z'),
                'workflow': {'specification': {'tasks': [], 'files': []}, 'execution': {'tasks': []}}}
            for t in internal_tasks:
                if not isinstance(t, dict):
                    continue
                tid = t.get('id')
                wfcommons_data['workflow']['specification']['tasks'].append({
                    'name': t.get('name', tid), 'id': tid, 'children': task_children.get(tid, []),
                    'parents': t.get('dependencies', []), 'inputFiles': t.get('input_files', []),
                    'outputFiles': t.get('output_files', []), 'flops': t.get('flops', 0),
                    'memory': t.get('memory', 0)})
                wfcommons_data['workflow']['execution']['tasks'].append({
                    'id': tid, 'runtimeInSeconds': t.get('runtime', 1.0), 'cores': 1, 'avgCPU': 1.0})
            for f in wf_section.get('files', []):
                if not isinstance(f, dict):
                    continue
                wfcommons_data['workflow']['specification']['files'].append({
                    'id': f.get('id'), 'name': f.get('name'), 'sizeInBytes': f.get('size', 0)})
        if 'workflow_name' not in wfcommons_data:
            wfcommons_data['workflow_name'] = wfcommons_data.get('name', 'unknown')
        return simulation.create_workflow_from_json(
            wfcommons_data, reference_flop_rate=reference_flop_rate,
            ignore_machine_specs=False, redundant_dependencies=False,
            ignore_cycle_creating_dependencies=False, min_cores_per_task=1,
            max_cores_per_task=1, enforce_num_cores=True,
            ignore_avg_cpu=False, show_warnings=False)

    def _run_single_simulation(self, scheduler_name: str, scheduler_impl: Any, workflow_file: str) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªWRENCHæ¨¡æ‹Ÿå¹¶è¿”å›ç»“æœã€‚"""
        workflow_filename = Path(workflow_file).name
        try:
            with open(self.platform_file, "r") as f:
                platform_xml = f.read()

            simulation = wrench.Simulation()
            controller_host = "ControllerHost"
            simulation.start(platform_xml, controller_host)

            all_hostnames = simulation.get_all_hostnames()
            compute_hosts_names = [h for h in all_hostnames if h not in [controller_host, "StorageHost"]]
            storage_service = simulation.create_simple_storage_service("StorageHost", ["/storage"])

            compute_services = {}
            for host_name in compute_hosts_names:
                compute_services[host_name] = simulation.create_bare_metal_compute_service(
                    host_name, {host_name: (-1, -1)}, "/scratch", {}, {})

            hosts_properties = {}
            for name, service in compute_services.items():
                flop_rates = service.get_core_flop_rates()
                speed = list(flop_rates.values())[0] if flop_rates else 0.0
                hosts_properties[name] = {"speed": speed}
            
            with open(workflow_file, 'r') as f:
                workflow_data = json.load(f)
            
            workflow = self.create_workflow_from_json_data(simulation, workflow_data, workflow_file)
            
            scheduler_args = {
                "simulation": simulation,
                "compute_services": compute_services,
                "hosts": hosts_properties,
                "workflow_obj": workflow
            }
            
            # --- è¿™æ˜¯æ ¸å¿ƒä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰DRLè°ƒåº¦å™¨éƒ½èƒ½æ”¶åˆ° workflow_file ---
            if scheduler_name in ["WASS_DRL", "WASS_RAG"]:
                scheduler_args['workflow_file'] = workflow_file
            # --- ä¿®å¤ç»“æŸ ---

            scheduler_instance = scheduler_impl(**scheduler_args)

            for file in workflow.get_input_files():
                storage_service.create_file_copy(file)
            
            if hasattr(scheduler_instance, 'schedule_ready_tasks'):
                scheduler_instance.schedule_ready_tasks(workflow, storage_service)
            
            while not workflow.is_done():
                event = simulation.wait_for_next_event()
                if event["event_type"] == "standard_job_completion":
                    job = event["standard_job"]
                    if hasattr(scheduler_instance, 'handle_completion'):
                        for task in job.get_tasks():
                            scheduler_instance.handle_completion(task)
                    if hasattr(scheduler_instance, 'schedule_ready_tasks'):
                        scheduler_instance.schedule_ready_tasks(workflow, storage_service)
                elif event["event_type"] == "simulation_termination":
                    break
            
            makespan = simulation.get_simulated_time()
            wf_section = workflow_data.get('workflow', {})
            if 'tasks' in wf_section and isinstance(wf_section.get('tasks'), list):
                task_count_source = wf_section.get('tasks')
            else:
                spec_tasks = wf_section.get('specification', {}).get('tasks', [])
                task_count_source = spec_tasks if isinstance(spec_tasks, list) else []
            task_count = len(task_count_source)
            simulation.terminate()
            return {"scheduler": scheduler_name, "workflow": workflow_filename, "makespan": makespan, "status": "success", "task_count": task_count}
        except Exception as e:
            import traceback
            print(f"ERROR running {scheduler_name} on {workflow_filename}: {e}")
            print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return {"scheduler": scheduler_name, "workflow": workflow_filename, "makespan": float('inf'), "status": "failed", "task_count": 0}

    def analyze_results(self, results: List[Dict[str, Any]]):
        """åˆ†æã€æ‰“å°å¹¶ä¿å­˜å®éªŒç»“æœæ‘˜è¦ã€‚"""
        if not results:
            print("æ²¡æœ‰å¯ä¾›åˆ†æçš„å®éªŒç»“æœã€‚"); return
        df = pd.DataFrame(results)
        detailed_csv_path = self.output_dir / "detailed_results.csv"
        df.to_csv(detailed_csv_path, index=False)
        print(f"âœ… è¯¦ç»†å®éªŒç»“æœå·²ä¿å­˜åˆ°: {detailed_csv_path}")
        summary = df.groupby('scheduler')['makespan'].agg(['mean', 'std', 'min', 'count']).reset_index()
        summary = summary.rename(columns={'scheduler': 'è°ƒåº¦å™¨', 'mean': 'å¹³å‡Makespan', 'std': 'æ ‡å‡†å·®', 'min': 'æœ€ä½³', 'count': 'å®éªŒæ¬¡æ•°'})
        print("\n" + "="*60); print("ğŸ“ˆ å®éªŒç»“æœåˆ†æ:"); print(summary.to_string(index=False)); print("="*60 + "\n")
        summary_csv_path = self.output_dir / "summary_results.csv"
        summary.to_csv(summary_csv_path, index=False)
        print(f"âœ… å®éªŒç»“æœæ‘˜è¦å·²ä¿å­˜åˆ°: {summary_csv_path}")