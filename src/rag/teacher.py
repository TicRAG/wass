# src/drl/knowledge_teacher.py
import faiss
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from pathlib import Path
from typing import Any, Dict, Optional
import json

class KnowledgeBase:
    """å°è£…FAISSå‘é‡ç´¢å¼•å’Œå…ƒæ•°æ®çš„çŸ¥è¯†åº“ã€‚"""
    def __init__(self, dimension: int, storage_path: str = "data/knowledge_base"):
        self.dimension = dimension
        self.storage_path = Path(storage_path)
        self.storage_path.mkdir(parents=True, exist_ok=True)
        
        self.index_file = self.storage_path / "workflow_embeddings.index"
        self.meta_file = self.storage_path / "workflow_metadata.csv"
        
        if self.index_file.exists() and self.meta_file.exists():
            print("ğŸ§  [Teacher] Loading existing Knowledge Base...")
            self.index = faiss.read_index(str(self.index_file))
            self.metadata = pd.read_csv(self.meta_file)
            print("âœ… [Teacher] Knowledge Base loaded.")
        else:
            print("âš ï¸ [Teacher] No existing Knowledge Base found. Initializing a new one.")
            self.index = faiss.IndexFlatL2(dimension)
            self.metadata = pd.DataFrame()

    def add(self, vectors: np.ndarray, metadata_list: list[dict]):
        if not hasattr(vectors, 'shape') or vectors.shape[0] == 0:
            return
        vectors = np.ascontiguousarray(vectors, dtype=np.float32)
        self.index.add(vectors)
        new_metadata = pd.DataFrame(metadata_list)
        self.metadata = pd.concat([self.metadata, new_metadata], ignore_index=True)

    def search(self, query_vector: np.ndarray, k: int = 5) -> pd.DataFrame:
        if self.index.ntotal == 0:
            return pd.DataFrame()
        query_vector = np.ascontiguousarray(query_vector.reshape(1, -1), dtype=np.float32)
        distances, indices = self.index.search(query_vector, k)
        valid_indices = indices[0][indices[0] != -1]
        if len(valid_indices) == 0:
            return pd.DataFrame()
        return self.metadata.iloc[valid_indices]

    def save(self):
        print(f"ğŸ’¾ [KB] Saving Knowledge Base with {self.index.ntotal} entries...")
        faiss.write_index(self.index, str(self.index_file))
        self.metadata.to_csv(self.meta_file, index=False)
        print("âœ… [KB] Knowledge Base saved.")

class KnowledgeableTeacher:
    """çŸ¥è¯†å¼•å¯¼æ•™å¸ˆï¼Œè´Ÿè´£ç”ŸæˆRAGå¥–åŠ±ã€‚"""
    def __init__(self, state_dim: int, knowledge_base: KnowledgeBase, reward_config: Optional[Dict[str, Any]] = None):
        self.kb = knowledge_base
        cfg = reward_config or {}
        self.top_k = int(cfg.get("top_k", 10))
        self.scheduler_filter = cfg.get("scheduler_filter", "HEFT")
        normalizer = cfg.get("reward_normalizer", 1000.0)
        self.reward_normalizer = float(normalizer) if normalizer not in (None, 0) else 1.0

    # --- è¿™æ˜¯æœ€ç»ˆçš„å¥–åŠ±å‡½æ•° ---
    def generate_rag_reward(self, state_embedding: torch.Tensor, agent_eft: float, task_name: str) -> float:
        """
        ç”ŸæˆåŸºäºè¡Œä¸ºçš„ã€åŠ¨æ€çš„RAGå¥–åŠ±ã€‚
        å¥–åŠ± = (å†å²æœ€ä¼˜EFT - æ™ºèƒ½ä½“å½“å‰EFT)
        """
        # 1. æ£€ç´¢ç›¸ä¼¼æ¡ˆä¾‹
        similar_cases = self.kb.search(state_embedding.detach().cpu().numpy(), k=self.top_k)
        
        if similar_cases.empty:
            return 0.0

        # 2. ç­›é€‰å‡ºç”±HEFTè°ƒåº¦å™¨äº§ç”Ÿçš„ã€â€œå¥½çš„â€å†å²æ¡ˆä¾‹
        heft_cases = similar_cases[similar_cases['scheduler_used'] == self.scheduler_filter]
        if heft_cases.empty:
            return 0.0

        # 3. ä»è¿™äº›å¥½çš„æ¡ˆä¾‹ä¸­ï¼Œæ‰¾åˆ°é’ˆå¯¹å½“å‰ä»»åŠ¡çš„å†å²å†³ç­–
        historical_efts = []
        for _, row in heft_cases.iterrows():
            try:
                # decisionså­—æ®µæ˜¯JSONå­—ç¬¦ä¸²ï¼Œéœ€è¦è§£æ
                decisions = json.loads(row['decisions'])
                if task_name in decisions:
                    historical_efts.append(decisions[task_name]['finish_time'])
            except (json.JSONDecodeError, KeyError):
                continue
        
        if not historical_efts:
            return 0.0 # çŸ¥è¯†åº“ä¸­æ²¡æœ‰å…³äºè¿™ä¸ªä»»åŠ¡çš„HEFTå†³ç­–è®°å½•

        # 4. æ‰¾åˆ°å†å²ä¸Šçš„æœ€ä¼˜EFT
        best_historical_eft = np.min(historical_efts)

        # 5. è®¡ç®—å¥–åŠ± (å¹¶è¿›è¡Œç¼©æ”¾ï¼Œä½¿å…¶æ•°å€¼ç¨³å®š)
        reward = (best_historical_eft - agent_eft) / self.reward_normalizer
        
        return reward