"""å·¥å…·å‡½æ•°ã€æ—¥å¿—è®¾ç½®å’Œæœ€ç»ˆç‰ˆçš„å®éªŒè¿è¡Œå™¨ã€‚"""
from __future__ import annotations
import logging
import time
from contextlib import contextmanager
from typing import Dict, Any, Generator, List
from pathlib import Path
import sys
import wrench
import pandas as pd
import numpy as np
import json

def get_logger(name, level=logging.INFO):
    """è·å–ä¸€ä¸ªå·²é…ç½®çš„æ—¥å¿—å™¨ã€‚"""
    logger = logging.getLogger(name)
    logger.setLevel(level)
    if not logger.handlers:
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler = logging.StreamHandler(sys.stdout)
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    return logger

# --- æœ€ç»ˆç‰ˆå®éªŒè¿è¡Œå™¨ ---
class WrenchExperimentRunner:
    """å¤„ç†å¤šä¸ªWRENCHæ¨¡æ‹Ÿçš„æ‰§è¡Œï¼Œä»¥æ¯”è¾ƒä¸åŒçš„è°ƒåº¦å™¨ã€‚"""
    def __init__(self, schedulers: Dict[str, Any], config: Dict[str, Any]):
        self.schedulers_map = schedulers
        self.config = config
        self.platform_file = config.get("platform_file")
        self.workflow_dir = Path(config.get("workflow_dir", "workflows"))
        self.workflow_sizes = config.get("workflow_sizes", [20, 50, 100])
        self.repetitions = config.get("repetitions", 3)
        self.output_dir = Path(config.get("output_dir", "results/final_experiments"))
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def _run_single_simulation(self, scheduler_name: str, scheduler_impl: Any, workflow_file: str) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªWRENCHæ¨¡æ‹Ÿå¹¶è¿”å›ç»“æœã€‚"""
        try:
            # è¯»å–å¹³å°æ–‡ä»¶å†…å®¹
            with open(self.platform_file, "r") as f:
                platform_xml = f.read()
            
            # åˆ›å»ºæ¨¡æ‹Ÿå¯¹è±¡
            simulation = wrench.Simulation()
            
            # å¯åŠ¨ä»¿çœŸï¼ŒæŒ‡å®šæ§åˆ¶å™¨ä¸»æœº
            controller_host = "ControllerHost"
            simulation.start(platform_xml, controller_host)
            
            # è·å–æ‰€æœ‰ä¸»æœºå
            all_hostnames = simulation.get_all_hostnames()
            
            # è¿‡æ»¤å‡ºè®¡ç®—ä¸»æœºï¼ˆæ’é™¤æ§åˆ¶å™¨ä¸»æœºå’Œå­˜å‚¨ä¸»æœºï¼‰
            compute_hosts = [host for host in all_hostnames if host not in [controller_host, "StorageHost"]]
            
            # åˆ›å»ºå­˜å‚¨æœåŠ¡ï¼ˆåœ¨StorageHostä¸Šï¼ŒæŒ‚è½½ç‚¹ä¸º/storageï¼‰
            storage_service = simulation.create_simple_storage_service("StorageHost", ["/storage"])
            
            # ä¸ºæ¯ä¸ªè®¡ç®—ä¸»æœºåˆ›å»ºç‹¬ç«‹çš„è®¡ç®—æœåŠ¡
            compute_services = {}
            for host in compute_hosts:
                compute_services[host] = simulation.create_bare_metal_compute_service(
                    host, 
                    {host: (-1, -1)},  # åªæœ‰è¯¥ä¸»æœºçš„æ ¸å¿ƒ
                    "/scratch", 
                    {}, 
                    {}
                )
            
            # åˆ›å»ºä¸»æœºä¿¡æ¯å­—å…¸
            hosts_dict = {name: {} for name in compute_hosts}
            
            # å®ä¾‹åŒ–è°ƒåº¦å™¨
            if isinstance(scheduler_impl, str):
                # å­—ç¬¦ä¸²å½¢å¼çš„è°ƒåº¦å™¨ç±»åï¼Œéœ€è¦å¯¼å…¥å¯¹åº”çš„ç±»
                import importlib
                module_name, class_name = scheduler_impl.rsplit('.', 1) if '.' in scheduler_impl else ('wrench_schedulers', scheduler_impl)
                try:
                    module = importlib.import_module(module_name)
                    scheduler_class = getattr(module, class_name)
                    scheduler_instance = scheduler_class(simulation, compute_services, hosts_dict)
                except (ImportError, AttributeError) as e:
                    print(f"å¯¼å…¥è°ƒåº¦å™¨ç±»å¤±è´¥: {scheduler_impl}, é”™è¯¯: {e}")
                    # å›é€€åˆ°åŸºç¡€è°ƒåº¦å™¨
                    from wrench_schedulers import BaseScheduler
                    scheduler_instance = BaseScheduler(simulation, compute_services, hosts_dict)
            elif callable(scheduler_impl) and not isinstance(scheduler_impl, type):
                # å·¥å‚å‡½æ•°å½¢å¼çš„è°ƒåº¦å™¨
                scheduler_instance = scheduler_impl(simulation, compute_services, hosts_dict)
            else:
                # ç±»å½¢å¼çš„è°ƒåº¦å™¨
                scheduler_instance = scheduler_impl(simulation, compute_services, hosts_dict)
            
            # ä»JSONæ–‡ä»¶åˆ›å»ºå·¥ä½œæµ
            with open(workflow_file, 'r') as f:
                workflow_data = json.load(f)
            
            # è½¬æ¢å·¥ä½œæµæ•°æ®ä¸ºWfCommonsæ ¼å¼
            print(f"è½¬æ¢å·¥ä½œæµæ•°æ®ä¸ºWfCommonsæ ¼å¼...")
            
            # æ„å»ºä»»åŠ¡ä¾èµ–å…³ç³»æ˜ å°„
            task_children = {}
            for task in workflow_data['workflow']['tasks']:
                task_children[task['id']] = []
            for task in workflow_data['workflow']['tasks']:
                for parent_id in task.get('dependencies', []):
                    if parent_id in task_children:
                        task_children[parent_id].append(task['id'])
            
            # è½¬æ¢ä¸ºWfCommonsæ ¼å¼
            wfcommons_data = {
                'name': workflow_data['metadata']['name'],
                'workflow_name': workflow_data['metadata']['name'],
                'description': workflow_data['metadata']['description'],
                'schemaVersion': '1.5',
                'author': {
                    'name': 'WRENCH Experiment',
                    'email': 'wrench@example.com'
                },
                'createdAt': workflow_data['metadata'].get('generated_at', '2024-01-01T00:00:00Z'),
                'workflow': {
                    'specification': {
                        'tasks': [],
                        'files': []
                    },
                    'execution': {
                        'tasks': []
                    }
                }
            }
            
                    # è½¬æ¢ä»»åŠ¡
            for task in workflow_data['workflow']['tasks']:
                wfcommons_task = {
                    'name': task['name'],
                    'id': task['id'],
                    'children': task_children[task['id']],
                    'parents': task.get('dependencies', []),
                    'inputFiles': task.get('input_files', []),
                    'outputFiles': task.get('output_files', []),
                    'flops': task.get('flops', 0),
                    'memory': task.get('memory', 0)
                }
                wfcommons_data['workflow']['specification']['tasks'].append(wfcommons_task)
                
                # æ·»åŠ æ‰§è¡Œä¿¡æ¯
                execution_task = {
                    'id': task['id'],
                    'runtimeInSeconds': task.get('runtime', 1.0),
                    'cores': 1,
                    'avgCPU': 1.0
                }
                wfcommons_data['workflow']['execution']['tasks'].append(execution_task)
            
            # è½¬æ¢æ–‡ä»¶
            for file in workflow_data['workflow']['files']:
                wfcommons_file = {
                    'id': file['id'],
                    'name': file['name'],
                    'sizeInBytes': file.get('size', 0)
                }
                wfcommons_data['workflow']['specification']['files'].append(wfcommons_file)
            
            # åˆ›å»ºå·¥ä½œæµ - ä½¿ç”¨ç›´æ¥æ–¹æ³•
            print(f"åˆ›å»ºå·¥ä½œæµï¼Œåç§°: {wfcommons_data['name']}")
            
            # åˆ›å»ºå·¥ä½œæµ - ç¡®ä¿åŒ…å«workflow_nameå­—æ®µ
            if 'workflow_name' not in wfcommons_data:
                wfcommons_data['workflow_name'] = wfcommons_data.get('name', 'unknown')
            
            # ä½¿ç”¨ç›´æ¥çš„create_workflow_from_jsonæ–¹æ³•åˆ›å»ºå·¥ä½œæµ
            # ä¿®å¤ï¼šè®¾ç½®åˆé€‚çš„reference_flop_rateå€¼è®©FLOPSå€¼ç”Ÿæ•ˆ
            # ä½¿ç”¨è¾ƒå¤§çš„reference_flop_rateå€¼é¿å…å°FLOPSå€¼è¢«ç¼©æ”¾ä¸º0
            workflow = simulation.create_workflow_from_json(
                wfcommons_data,
                reference_flop_rate=str(task.get('flops', 0)/1000000)+'Mf',  # ä½¿ç”¨è¾ƒå°çš„å‚è€ƒå€¼ï¼Œè®©å®é™…FLOPSå€¼æ›´æœ‰æ„ä¹‰
                ignore_machine_specs=False,  # æ”¹ä¸ºFalseï¼Œè®©æœºå™¨è§„æ ¼å½±å“æ‰§è¡Œæ—¶é—´
                redundant_dependencies=False,
                ignore_cycle_creating_dependencies=False,
                min_cores_per_task=1,
                max_cores_per_task=1,
                enforce_num_cores=True,
                ignore_avg_cpu=True,
                show_warnings=False
            )
            
            print(f"å·¥ä½œæµåˆ›å»ºæˆåŠŸï¼å·¥ä½œæµåç§°: {workflow.get_name()}")
            print(f"å·¥ä½œæµä»»åŠ¡æ•°: {len(workflow.get_tasks())}")
            
            # åˆ›å»ºå·¥ä½œæµä¸­çš„æ‰€æœ‰æ–‡ä»¶å‰¯æœ¬
            try:
                for file in workflow.get_input_files():
                    storage_service.create_file_copy(file)
            except Exception as e:
                print(f"æ–‡ä»¶å‰¯æœ¬åˆ›å»ºå¤±è´¥: {e}")
                # è·³è¿‡æ–‡ä»¶å‰¯æœ¬åˆ›å»ºï¼Œç»§ç»­æ‰§è¡Œ
            
            # å¼€å§‹è°ƒåº¦
            if hasattr(scheduler_instance, 'schedule_ready_tasks'):
                scheduler_instance.schedule_ready_tasks(workflow, storage_service)
            
            # è¿è¡Œä»¿çœŸå¾ªç¯
            while not workflow.is_done():
                # ç­‰å¾…ä¸‹ä¸€ä¸ªäº‹ä»¶
                event = simulation.wait_for_next_event()
                
                # å¤„ç†ä»»åŠ¡å®Œæˆäº‹ä»¶
                if event["event_type"] == "standard_job_completion":
                    job = event["standard_job"]
                    if hasattr(scheduler_instance, 'handle_completion'):
                        for task in job.get_tasks():
                            scheduler_instance.handle_completion(task)
                    # è°ƒåº¦æ–°çš„å°±ç»ªä»»åŠ¡
                    if hasattr(scheduler_instance, 'schedule_ready_tasks'):
                        scheduler_instance.schedule_ready_tasks(workflow, storage_service)
                elif event["event_type"] == "simulation_termination":
                    break
            
            # è·å–makespan
            makespan = simulation.get_simulated_time()
            
            # ç»ˆæ­¢ä»¿çœŸ
            simulation.terminate()
            
            # ä¿®å¤: ä½¿ç”¨Pathå¯¹è±¡æ¥è·å–æ–‡ä»¶å
            workflow_filename = Path(workflow_file).name
            return {"scheduler": scheduler_name, "workflow": workflow_filename, "makespan": makespan, "status": "success"}
        except Exception as e:
            # ä¿®å¤: ä½¿ç”¨Pathå¯¹è±¡æ¥è·å–æ–‡ä»¶å
            workflow_filename = Path(workflow_file).name
            import traceback
            print(f"ERROR running {scheduler_name} on {workflow_filename}: {e}")
            print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return {"scheduler": scheduler_name, "workflow": workflow_filename, "makespan": float('inf'), "status": "failed"}

    def run_all(self) -> List[Dict[str, Any]]:
        """è¿è¡Œæ‰€æœ‰é…ç½®çš„å®éªŒã€‚"""
        results = []
        total_exps = len(self.schedulers_map) * len(self.workflow_sizes) * self.repetitions
        print(f"æ€»å®éªŒæ•°: {total_exps}")
        
        exp_count = 0
        for name, sched_impl in self.schedulers_map.items():
            for size in self.workflow_sizes:
                # [FIX] æ™ºèƒ½æ‰«æå·¥ä½œæµæ–‡ä»¶ï¼Œè€Œä¸æ˜¯ä¾èµ–å›ºå®šåç§°
                matching_files = list(self.workflow_dir.glob(f'*_{size}_*.json')) + \
                                 list(self.workflow_dir.glob(f'*_{size}.json')) + \
                                 list(self.workflow_dir.glob(f'*{size}-tasks-wf.json'))
                
                if not matching_files:
                    print(f"[è­¦å‘Š] åœ¨ {self.workflow_dir} ä¸­æœªæ‰¾åˆ°å¤§å°ä¸º {size} çš„å·¥ä½œæµæ–‡ä»¶ï¼Œè·³è¿‡...")
                    continue
                
                # ä¼˜å…ˆé€‰æ‹©compute_intensive_serialå·¥ä½œæµï¼Œå…¶æ¬¡serialï¼Œå†æ¬¡highly_parallelï¼Œæœ€åå›é€€åˆ°ç¬¬ä¸€ä¸ªåŒ¹é…æ–‡ä»¶
                compute_intensive_files = [f for f in matching_files if 'compute_intensive_serial' in f.name]
                if compute_intensive_files:
                    workflow_file_to_run = compute_intensive_files[0]
                    print(f"[ä¿¡æ¯] ä¼˜å…ˆä½¿ç”¨è®¡ç®—å¯†é›†å‹ä¸²è¡Œå·¥ä½œæµ: {workflow_file_to_run.name}")
                else:
                    serial_files = [f for f in matching_files if 'serial' in f.name]
                    if serial_files:
                        workflow_file_to_run = serial_files[0]
                        print(f"[ä¿¡æ¯] ä¼˜å…ˆä½¿ç”¨ä¸²è¡Œå·¥ä½œæµ: {workflow_file_to_run.name}")
                    else:
                        highly_parallel_files = [f for f in matching_files if 'highly_parallel' in f.name]
                        if highly_parallel_files:
                            workflow_file_to_run = highly_parallel_files[0]
                            print(f"[ä¿¡æ¯] ä¼˜å…ˆä½¿ç”¨é«˜åº¦å¹¶è¡Œå·¥ä½œæµ: {workflow_file_to_run.name}")
                        else:
                            workflow_file_to_run = matching_files[0]
                
                for rep in range(self.repetitions):
                    exp_count += 1
                    print(f"è¿è¡Œå®éªŒ [{exp_count}/{total_exps}]: {name} on {workflow_file_to_run.name}")
                    result = self._run_single_simulation(name, sched_impl, workflow_file_to_run)
                    results.append(result)
        return results

    def analyze_results(self, results: List[Dict[str, Any]]):
        """åˆ†æã€æ‰“å°å¹¶ä¿å­˜å®éªŒç»“æœæ‘˜è¦ã€‚"""
        if not results:
            print("æ²¡æœ‰å¯ä¾›åˆ†æçš„å®éªŒç»“æœã€‚"); return

        df = pd.DataFrame(results)
        
        # ä¿å­˜è¯¦ç»†çš„åŸå§‹ç»“æœ
        detailed_csv_path = self.output_dir / "detailed_results.csv"
        df.to_csv(detailed_csv_path, index=False)
        print(f"âœ… è¯¦ç»†å®éªŒç»“æœå·²ä¿å­˜åˆ°: {detailed_csv_path}")

        summary = df.groupby('scheduler')['makespan'].agg(['mean', 'std', 'min', 'count']).reset_index()
        summary = summary.rename(columns={'scheduler': 'è°ƒåº¦å™¨', 'mean': 'å¹³å‡Makespan', 'std': 'æ ‡å‡†å·®', 'min': 'æœ€ä½³', 'count': 'å®éªŒæ¬¡æ•°'})

        print("\n" + "="*60); print("ğŸ“ˆ å®éªŒç»“æœåˆ†æ:"); print(summary.to_string(index=False)); print("="*60 + "\n")
        
        # [FIX] ä¿å­˜æœ€ç»ˆçš„æ‘˜è¦ç»“æœ
        summary_csv_path = self.output_dir / "summary_results.csv"
        summary.to_csv(summary_csv_path, index=False)
        print(f"âœ… å®éªŒç»“æœæ‘˜è¦å·²ä¿å­˜åˆ°: {summary_csv_path}")