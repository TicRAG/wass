### **WASS-RAG 项目实验问题诊断与解决方案报告**

#### **摘要**

本文旨在解决 WASS-RAG 项目在实验阶段遇到的三个主要障碍：**1) 训练与实验规模不足**；**2) 实验结果未达预期**；**3) 关键基准 `WASS (Heuristic)` 缺失**。报告将对每个问题进行深入诊断，并提出具体、可操作的解决方案，以确保最终的实验结果能够有力支撑您在ACM顶级会议上的论文发表。

---

### **问题一：训练与实验规模不足**

**1.1 问题诊断**

通过分析您的代码库，尤其是 `run_complete_experiment.sh` 脚本和 `configs/` 目录下的配置文件，可以发现当前的实验流程在可扩展性上存在瓶颈：
* **硬编码配置**: 实验所用的工作流和集群平台（`platform.xml`）是固定的。每次想要测试不同规模的问题（例如，从50个任务增加到500个任务，或从16个节点扩大到64个节点），都需要手动修改文件和脚本，这非常低效且容易出错。
* **缺乏自动化**: 整个实验流程是一次性的，没有形成一个可以自动遍历不同参数组合的框架。这使得进行大规模、系统的对比实验（例如，测试不同任务数量在不同集群规模下的性能表现）变得几乎不可能。

**1.2 解决方案**

为了支撑一篇顶级论文，您需要展示算法在不同规模和复杂度下的鲁棒性和可扩展性。解决方案的核心是**自动化**和**参数化**。

* **步骤一：实现工作流生成器 (Workflow Generator)**
    * **目标**: 创建一个独立的Python脚本，该脚本可以根据输入的参数（如任务数量、依赖复杂度等）自动生成WRENCH可用的工作流文件（JSON或XML格式）。
    * **建议**: 不仅要生成随机结构的DAG，更应该参考HPC调度领域的**标准科学工作流模型**，如 Montage（天文学）、LIGO（物理学）、Cybershake（地震学）等。这样做可以让您的实验更具说服力和公信力。您可以先生成一系列不同规模的工作流（例如，任务数分别为 50, 100, 200, 500, 1000），并将它们分类存放在不同的目录中。

* **步骤二：创建多套平台配置文件 (Platform Configurations)**
    * **目标**: 在 `configs/` 目录下，根据您论文中希望论证的场景，创建多个集群配置文件。
    * **建议**: 至少创建三套配置：`platform_small.xml` (例如16个节点), `platform_medium.xml` (例如64个节点), 和 `platform_large.xml` (例如128个或更多节点)。这能让您在论文中分析调度器在不同资源规模下的表现。

* **步骤三：改造实验执行脚本 (Experiment Automation)**
    * **目标**: 将 `run_complete_experiment.sh` 或核心的Python实验脚本（如 `experiments/wrench_real_experiment.py`）改造为一个可以接受参数并自动执行多轮实验的控制器。
    * **建议**:
        1.  让脚本接受命令行参数，例如 `--workflow-set small` 和 `--platform medium`。
        2.  在脚本内部，编写嵌套循环，自动遍历所有工作流规模和所有平台配置的组合。
        3.  将每一轮实验的结果（makespan、CPU利用率等）以结构化的方式（如CSV或JSON）追加到一个总的结果文件中，而不是每次都覆盖。这样便于后续进行统一的数据分析和图表绘制。

---

### **问题二：实验结果不理想**

**2.1 问题诊断**

DRL（深度强化学习）模型的训练是一个复杂的过程，结果不佳通常由多个因素导致。
* **奖励信号过于稀疏 (Sparse Rewards)**: 您的论文提到最终奖励是 `1 / Makespan`。这意味着智能体（Agent）只有在整个工作流执行完毕后才能获得一次反馈。在一个包含数百个任务的工作流中，智能体很难知道早期成百上千个决策中，哪一个是好的，哪一个是坏的。这是导致模型难以收敛或收敛到次优策略的**最常见原因**。
* **超参数敏感性 (Hyperparameter Sensitivity)**: DRL对学习率（learning rate）、折扣因子（gamma）、神经网络结构等超参数极为敏感。`configs/drl.yaml` 中定义的很可能是一组未经优化的默认值。
* **知识库质量低下 (Poor Knowledge Base)**: 对于核心的WASS-RAG方法，其性能上限受到“知识库”质量的严重影响。如果用于生成知识库的调度器（例如HEFT）本身性能一般，那么RAG模块就相当于在向一个“平庸的老师”学习，无法提供高质量的指导，甚至可能误导DRL智能体的学习方向。

**2.2 解决方案**

* **步骤一：设计密集的奖励函数 (Reward Shaping)**
    * **目标**: 改造奖励机制，让智能体在每一步决策后都能得到即时反馈。
    * **建议**: 在 `src/ai_schedulers.py` 的DRL框架中，将奖励函数修改为两部分之和：`R_total = R_step + R_final`。
        * **`R_step` (中间奖励)**: 在每调度一个任务后立即计算。它可以是：
            * **数据局部性奖励**: 如果任务被分配到了其输入数据所在的节点，给予一个小的正奖励（例如 `+0.1`）。
            * **等待时间惩罚**: 根据任务在该节点上的预计等待时间，给予一个小的负奖励。
            * **关键路径奖励**: 如果被调度的任务位于工作流的关键路径上，并且它的完成时间被有效缩短，给予一个稍大的正奖励。
        * **`R_final` (最终奖励)**: 保持不变，在工作流结束后给予。
    * **注意**: 奖励函数的设计需要反复实验和调整，它是决定DRL性能最关键的环节之一。

* **步骤二：进行系统性的超参数调优 (Hyperparameter Tuning)**
    * **目标**: 找到能让您的模型在特定任务上表现最佳的超参数组合。
    * **建议**: 不要手动调整。使用专业的超参数搜索框架，如 **WandB (Weights & Biases) Sweeps** 或 **Ray Tune**。这些工具可以根据您在配置文件中定义的搜索范围（例如，学习率在0.0001到0.01之间），自动运行数十上百次训练，并找到最优组合。这是当前AI学术研究的**标准实践**。

* **步骤三：提升知识库的质量 (Knowledge Base Curation)**
    * **目标**: 确保RAG模块学习的是高质量的调度经验。
    * **建议**: 在运行 `scripts/generate_kb_dataset.py` 来构建知识库时，必须使用当前**最强的非学习型调度器**来生成调度轨迹。在您实现了`WASS (Heuristic)`之后，如果它的性能优于`HEFT`，那么就应该用`WASS (Heuristic)`来生成您的知识库。

---

### **问题三：缺少 `WASS (Heuristic)` 调度器**

**3.1 问题诊断**

如我们之前所讨论，缺少 `WASS (Heuristic)` 这个中间基准，会让您的论文故事线出现断层。它不仅是一个对比项，更是证明您从启发式方法到AI方法演进逻辑的关键一环，能显著增强论文的深度和说服力。

**3.2 解决方案**

您不需要从零开始。`WASS (Heuristic)` 的核心思想是在 `HEFT` 的基础上进行改进。

* **步骤一：明确设计思路**
    * **复用任务排序**: 完全复用 `HEFT` 的**任务排序阶段**。即，通过计算“向上排名”（upward rank）来决定任务的执行优先级。您现有的 `HeftScheduler` 中已经包含了这部分逻辑。
    * **改进节点选择**: 核心区别在于**节点选择阶段**。`HEFT` 的目标是为任务选择一个能让其“最早完成时间”（EFT）的节点。`WASS (Heuristic)` 的目标则是选择一个能让**综合成本**最低的节点。

* **步骤二：定义综合成本函数 (Scoring Function)**
    * 为每一个“（任务，可用节点）”对，计算一个综合分数 `Score`。
    * `Score = (1 - w) * EFT + w * DRT`
        * **`EFT` (Earliest Finish Time)**: 任务在该节点上的最早完成时间。您可以直接调用 `HEFT` 中已有的 `get_earliest_finish_time` 方法来计算。
        * **`DRT` (Data Ready Time)**: 所有该任务的输入数据完全传输到该节点并准备就绪的时刻。这个值直接反映了数据传输的开销。如果一个节点上已经有了大部分数据，它的DRT就会很低。
        * **`w` (权重因子)**: 一个介于0和1之间的超参数，用于平衡“计算效率”和“数据局部性”。您可以将 `w=0.5` 作为一个默认值。当 `w=0` 时，`WASS (Heuristic)` 就等价于 `HEFT`。

* **步骤三：在代码中实现**
    * 在 `src/wrench_schedulers.py` 文件中，创建一个新类 `WassHeuristicScheduler`，并让它继承自 `wrench.Scheduler`。
    * 在其 `make_scheduling_decision` 方法中，实现上述的评分和选择逻辑。

### **总结与行动路线图**

为了最高效地推进项目，建议您按以下顺序执行：
1.  **优先实现 `WASS (Heuristic)`**: 这是最直接、最明确的任务。完成后，您的基准对比就完整了。
2.  **改造实验框架以支持扩展**: 在进行耗时的DRL训练之前，先把实验的“骨架”搭建好，确保能够轻松地进行大规模测试。
3.  **迭代优化DRL模型**: 这是最耗时、最需要耐心的部分。从**奖励函数整形**开始，然后系统地进行**超参数调优**。

遵循此报告的建议，您将能够构建一个稳健、可重复的实验流程，并获得足以支撑一篇高质量学术论文的扎实结果。