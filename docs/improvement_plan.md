### WASS 代码改进计划

**目标**: 本计划旨在提供一份**完全独立**的技术实施指南，使开发人员在**不参考原始论文**的情况下，能够将现有 `wass` 代码库升级为 `WASS-RAG` 框架并完成核心实验。

---

#### **第一阶段：核心功能对齐 (P0 - 最高优先级)**

此阶段的目标是实现 `WASS-RAG` 的核心算法逻辑。

**1. 实现双 GNN 编码器架构 (Decoupled GNN State Encoding)**

*   **目标**: 将状态编码分解为两个独立的 GNN，以解决“语义漂移”问题，确保检索的有效性。
*   **位置**: `src/drl/gnn_encoder.py`
*   **任务**:
    1.  在 `GNNEncoder` 类或相关模块中，创建两个独立的 GNN 实例：
        *   `policy_encoder`: 用于 DRL Agent 的决策。**其参数在训练过程中持续更新**。
        *   `retrieval_encoder`: 仅用于知识库的检索。**其参数在训练开始前从 `policy_encoder` 复制一次，然后必须被完全冻结，不参与任何梯度更新**。
    2.  确保 DRL Agent 的 Actor 和 Critic 网络接收的图嵌入来自 `policy_encoder`。
    3.  确保用于生成知识库“键”（Key）和后续检索“查询”（Query）的图嵌入**全部**来自**冻结**的 `retrieval_encoder`。
*   **参考**: `paperdraft.md` - 第四节 B. "解耦的GNN状态编码架构"。

**2. 改造知识库 (Scheduling Knowledge Base)**

*   **目标**: 构建一个存储高质量历史经验的“键-值”知识库，为 RAG 提供数据源。
*   **位置**: `scripts/1_seed_knowledge_base.py`, `data/knowledge_base/`, `src/rag/teacher.py`
*   **任务**:
    1.  **定义知识库模式**: 知识库 `K` 是一个键值对的集合：`K = {(s_key_i, q_i)}`。
        *   **键 (Key) `s_key_i`**: 一个向量。它是历史状态 `s_i` 经过**冻结的 `retrieval_encoder`** 编码后得到的图嵌入。
        *   **值 (Value) `q_i`**: 一个标量。它代表从状态 `s_i` 出发，遵循某个特定策略直到结束所产生的**剩余完工时间 (Remaining Makespan)** 的归一化估计。**注意：这不再是整个 episode 的最终奖励**。
    2.  **实现“多教师”填充策略**: 修改 `1_seed_knowledge_base.py`，使其可以运行以下三种不同的“教师”策略来生成初始经验数据，并填充知识库：
        *   **HEFT**: 一个强大的启发式算法，提供高质量经验。
        *   **MIN-MIN**: 另一个启发式算法，提供不同视角的经验。
        *   **Random Policy**: 随机调度策略，提供低质量的“负面”经验。
    3.  对于每个教师策略在调度过程中产生的每一个状态 `s_i`，计算从该点到调度结束的**剩余完工时间**，并将其作为 `q_i` 存入知识库。
*   **参考**: `paperdraft.md` - 第四节 C. "调度知识库"。

**3. 实现基于 RAG 的势函数奖励塑造 (RAG-based PBRS)**

*   **目标**: 将稀疏的最终奖励信号转化为稠密的、有指导意义的即时奖励，以加速 DRL Agent 的收敛。
*   **位置**: `src/rag/teacher.py`, `src/drl/agent.py` (或 DRL 训练循环)
*   **任务**:
    1.  在 `src/rag/teacher.py` 中，实现一个名为 `calculate_potential(state)` 的函数，该函数计算给定状态 `s_t` 的势函数值 `Φ_RAG(s_t)`。此过程遵循以下步骤：
        *   **a. 检索 (Retrieve)**:
            *   使用**冻结的 `retrieval_encoder`** 将当前状态 `s_t` 编码为查询嵌入 `s_key_t`。
            *   使用高效的向量相似度搜索（如 FAISS 或 ScaNN），在知识库 `K` 中找出与 `s_key_t` 最相似的 `k` 个历史状态键。
        *   **b. 插值 (Interpolate)**:
            *   根据检索到的 `k` 个近邻 `{ (s_key_j, q_j) }`，计算它们与查询 `s_key_t` 的相似度 `sim_j` (推荐使用余弦相似度)。
            *   通过 Softmax 函数计算每个近邻的权重 `w_j`：
                `w_j = exp(sim_j / τ) / Σ_i exp(sim_i / τ)`
                (其中 `τ` 是温度超参数，例如 0.1)。
            *   势函数值 `Φ_RAG(s_t)` 是这些近邻 `q_j` 值的加权平均：
                `Φ_RAG(s_t) = Σ_j w_j * q_j`
    2.  修改 DRL 智能体的奖励计算逻辑。在智能体执行一个动作 `a_t` 从状态 `s_t` 转移到 `s_{t+1}` 后，计算其收到的**新奖励 `r'_t`**：
        *   首先，计算 `s_t` 和 `s_{t+1}` 的势函数值：`Φ_t = calculate_potential(s_t)` 和 `Φ_{t+1} = calculate_potential(s_{t+1})`。
        *   原始奖励 `r_t` 通常是负的时间流逝 `-Δt`。
        *   **塑造后的奖励 `r'_t = r_t + λ * (γ * Φ_{t+1} - Φ_t)`**
            *   `λ` (lambda) 是一个超参数，用于控制塑造奖励的强度。
            *   `γ` (gamma) 是 DRL 的折扣因子。
*   **参考**: `paperdraft.md` - 第四节 D. "基于RAG的势函数奖励塑造"。

---

#### **第二阶段：实验与消融研究 (P1 - 高优先级)**

此阶段的目标是运行核心实验，生成论文关键图表所需的数据。

**1. 实现实验运行器与基线 (Experiment Runner & Baselines)**

*   **目标**: 建立一个统一的实验框架，用于公平比较不同调度策略。
*   **位置**: `scripts/4_run_experiments.py`, `src/simulation/experiment_runner.py`
*   **任务**:
    1.  在 `experiment_runner.py` 中，确保可以配置并运行以下**五种**策略：
        *   **`WASS-RAG (Full)`**: 使用第一阶段实现的所有功能的完整版本。
        *   **`WASS-DRL (Vanilla)`**: 一个**消融版本**。它使用与 `Full` 版本完全相同的 GNN 编码器和 DRL 智能体，但**完全禁用 RAG 模块**。即，不执行奖励塑造，奖励就是原始的 `r_t`。
        *   **`WASS-RAG (HEFT-only)`**: 第二个**消融版本**。它使用完整的 RAG-PBRS 机制，但其知识库 `K` **仅由 HEFT 单一教师**进行填充。
        *   **`HEFT`**: 经典的启发式算法基线。
        *   **`MIN-MIN`**: 另一个经典的启发式算法基线。
    2.  确保 `4_run_experiments.py` 脚本可以接受命令行参数来选择运行哪个策略、使用哪个工作流 (Montage, LIGO, CyberShake)，并将结果（如 Makespan、每一步的奖励、总回报等）以结构化格式（如 CSV 或 JSON）保存到 `results/` 目录。
*   **参考**: `paperdraft.md` - 第五节 C. "对比基线"。

**2. 执行主实验并绘图**

*   **目标**: 生成论文所需的核心图表，验证方法的有效性。
*   **位置**: `analysis/plot_results.py`
*   **任务**:
    1.  使用 `4_run_experiments.py` 运行所有策略。**关键**: 为了保证结果的统计显著性，每个（策略，工作流）组合都必须使用**至少 5 个不同的随机种子**运行。
    2.  修改 `analysis/plot_results.py` 脚本，使其能够读取实验结果并生成：
        *   **训练曲线图**: 比较 `Full`, `HEFT-only`, `Vanilla` 的收敛过程。横轴为训练周期 (Episodes)，纵轴为平滑后的平均总回报。图中应包含均值和表示标准差的阴影区域。
        *   **性能对比图**: 为每个工作流绘制条形图，比较所有五种策略的最终平均 Makespan。条形图应包含误差棒（基于不同随机种子的标准差）。
        *   **消融研究表**: 创建一个表格，清晰地量化 `Full` 版本相对于 `Vanilla` 和 `HEFT-only` 在 Makespan 和收敛速度上的提升百分比。
*   **参考**: `keyimage.md` - 图表 1) "训练曲线", 2) "主性能对比图", 3) "消融研究表格"。

---

#### **第三阶段：深度分析与验证 (P2 - 中等优先级)**

此阶段的目标是完成可解释性、鲁棒性等分析，为论文提供更深入的论据。

**1. 实现可解释性案例分析**

*   **目标**: 定性地展示 RAG 机制如何为“黑盒”决策提供可追溯的依据。
*   **任务**:
    1.  在 `WASS-RAG` 的决策逻辑中，增加日志记录功能：当计算 `Φ_RAG` 时，不仅返回其值，还要记录下被检索到的 top-k 个历史经验的详细信息（特别是它们的 `q_i` 值和它们的来源“教师”，如 HEFT 或 Random）。
    2.  编写一个新脚本 `analysis/interpretability_case_study.py`。该脚本可以加载一个完整的调度日志，并生成一个可视化图表（如甘特图）。在图上，可以选择一个关键的决策点，并标注出：
        *   智能体当时做出的决策（例如，将任务 T5 分配给机器 M3）。
        *   被检索到的、支持此决策的历史经验（例如，“历史案例 A (来自 HEFT) 显示，在类似状态下这样做，剩余时间很短（好结果）”；“历史案例 B (来自 Random) 显示，若分配给 M1，剩余时间很长（坏结果）”）。
*   **参考**: `paperdraft.md` - 第六节 C. "可解释性案例研究" & `keyimage.md` - 图表 5) "可解释性案例图"。

**2. 验证双编码器的有效性**

*   **目标**: 通过实验数据证明，冻结 `retrieval_encoder` 对于维持一个稳定的检索空间至关重要。
*   **任务**:
    1.  专门运行一个**不冻结** `retrieval_encoder`（即两个编码器都随训练更新）的实验作为对比。
    2.  编写 `analysis/embedding_drift_analysis.py` 脚本，使用 t-SNE 或 UMAP 等降维可视化技术，对比“冻结”和“不冻结”两种情况下嵌入空间的变化。预期结果：“冻结”版的知识库嵌入在空间中分布稳定，而“不冻结”版的嵌入则会随训练不断“漂移”，导致早期存入的键在后期变得无法被正确检索。
*   **参考**: `keyimage.md` - 图表 6) "嵌入空间稳定性 / 检索键一致性"。

---

#### **第四阶段：清理与归档 (P3 - 低优先级)**

1.  **代码重构与文档**
    *   **任务**: 清理实验脚本，为第一阶段实现的核心模块和函数（如 `calculate_potential`）添加详细的文档字符串（Docstrings），解释输入、输出和内部逻辑。
2.  **最终结果归档**
    *   **任务**: 整理所有最终的实验数据、图表和日志，存放在一个名为 `results/final_paper_experiments/` 的新目录中，并更新项目根目录下的 `README.md` 文件，简要说明如何运行核心实验来复现论文结果。
