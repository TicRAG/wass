# Training and inference hyperparameters for WASS agents

# Shared model architecture and PPO optimizer defaults.
common:
  gnn:
    in_channels: 4
    hidden_channels: 64
    out_channels: 32
  ppo:
    learning_rate: 0.0003
    gamma: 0.99
    epochs: 10
    eps_clip: 0.2

# Settings specific to the RAG-enabled PPO training loop.
rag_training:
  total_episodes: 200
  save_interval: 50
  model:
    save_dir: models/saved_models
    filename: drl_agent.pth
  reward_scaling:
    rag_multiplier: 1.5
    # Retain final makespan penalty scaling; higher rag weight handled via teacher.lambda
    final_normalizer: 5000.0
  teacher:
    top_k: 6
    temperature: 0.05
    scheduler_filter: null
    lambda: 0.8
    gamma: 0.995
    debug: false
    fallback_use_all_if_empty: false

# Settings specific to the DRL-only PPO training loop (no teacher).
drl_training:
  total_episodes: 200
  save_interval: 50
  model:
    save_dir: models/saved_models
    filename: drl_agent_no_rag.pth
  reward_scaling:
    makespan_normalizer: 1000.0
