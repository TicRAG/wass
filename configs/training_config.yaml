# Training and inference hyperparameters for WASS agents

# Shared model architecture and PPO optimizer defaults.
common:
  gnn:
    in_channels: 4
    hidden_channels: 64
    out_channels: 32
  ppo:
    learning_rate: 0.0003
    gamma: 0.99
    epochs: 10
    eps_clip: 0.2

# Settings specific to the RAG-enabled PPO training loop.
rag_training:
  total_episodes: 200
  save_interval: 50
  model:
    save_dir: models/saved_models
    filename: drl_agent.pth
  reward_scaling:
    # rag_multiplier set to 1.0 (no secondary scaling; Teacher already normalizes)
    rag_multiplier: 1.0
    # Lower final normalizer if needed later; keep for now
    final_normalizer: 5000.0
  teacher:
    top_k: 10
    scheduler_filter: HEFT
    # Further reduced to amplify RAG signal; will produce ~10x larger magnitudes
    reward_normalizer: 20.0
    use_median: true  # if true, use median historical EFT instead of min to soften always-negative rewards
    use_ratio: true   # if true, compute proportional improvement reward instead of absolute difference
    ratio_scale: 0.3        # scaling factor for ratio-based reward (reduced to temper large negatives)
    beat_tolerance: 0.03    # within +3% of historical agg considered a beat for bonus (slightly easier)
    beat_bonus: 0.008       # reduced bonus to avoid overpowering small improvements
    min_clamped_reward: -0.05  # floor to prevent extremely negative values (stability)
    debug: false
    fallback_use_all_if_empty: true

# Settings specific to the DRL-only PPO training loop (no teacher).
drl_training:
  total_episodes: 200
  save_interval: 50
  model:
    save_dir: models/saved_models
    filename: drl_agent_no_rag.pth
  reward_scaling:
    makespan_normalizer: 1000.0
