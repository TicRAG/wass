drl:
  # WRENCH环境配置
  environment: "wrench"
  episodes: 1000  # 扩展再训练规模
  max_steps: 50    # 与训练脚本上限保持一致
  
  # DQN智能体参数
  network:
    hidden_dim: 128
    learning_rate: 0.001
  
  # 训练参数
  epsilon_start: 1.0
  epsilon_decay: 0.995
  epsilon_min: 0.1
  gamma: 0.99
  batch_size: 32
  memory_size: 10000
  target_update_freq: 10
  checkpoint_interval: 100   # 每100 episodes 保存一次
  eval_interval: 100         # 周期性评估周期
  log_interval: 50           # 控制台打印间隔
  rolling_window: 100        # 滚动统计窗口
  
  # 任务生成参数
  task_range: [5, 15]  # 最小和最大任务数
  dependency_prob: 0.3  # 任务依赖概率
  
  # 奖励函数权重
  rewards:
    time_weight: -1.0
    efficiency_weight: 2.0
    balance_weight: -0.5
  reward_framework: "makespan_shaping_v2"  # 标记使用新奖励框架
  final_reward: "normalized_makespan"      # 终局奖励策略

checkpoint:
  dir: models/checkpoints/
  keep_last: 5
  save_best: true

logging:
  metrics_file: results/training_metrics.jsonl
  fusion_debug: results/fusion_debug.log
  reward_debug: results/reward_debug.log
