#!/usr/bin/env python3
"""
åŸºäºWRENCHçš„çœŸå®WASS-RAGå®éªŒæ¡†æ¶
ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨çœŸå®WRENCHç¯å¢ƒä¸­è¿›è¡Œæ€§èƒ½å¯¹æ¯”å®éªŒ
"""

import sys
import os
import json
import time
import random
import numpy as np
import torch
import torch.nn as nn
import pickle
from pathlib import Path
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Tuple
import yaml
from datetime import datetime

# ç¡®ä¿èƒ½å¯¼å…¥WRENCH
try:
    import wrench
except ImportError:
    print("Error: WRENCH not available. Please install wrench-python-api.")
    sys.exit(1)

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, str(parent_dir))

def load_config(cfg_path: str) -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(cfg_path, 'r', encoding='utf-8') as f:
        cfg = yaml.safe_load(f) or {}
    
    # Process includes
    if 'include' in cfg:
        base_dir = os.path.dirname(cfg_path)
        for include_file in cfg['include']:
            include_path = os.path.join(base_dir, include_file)
            if os.path.exists(include_path):
                with open(include_path, 'r', encoding='utf-8') as f:
                    include_cfg = yaml.safe_load(f) or {}
                    for key, value in include_cfg.items():
                        if key not in cfg:
                            cfg[key] = value
    return cfg

@dataclass
class WRENCHExperimentResult:
    """å•æ¬¡WRENCHå®éªŒç»“æœ"""
    scheduler_name: str
    workflow_id: str
    task_count: int
    dependency_count: int
    makespan: float
    cpu_utilization: Dict[str, float]
    task_execution_times: Dict[str, float]
    scheduling_decisions: List[Dict[str, Any]]
    experiment_metadata: Dict[str, Any]

class WRENCHScheduler:
    """åŸºç¡€WRENCHè°ƒåº¦å™¨æ¥å£"""
    
    def __init__(self, name: str):
        self.name = name
    
    def schedule_task(self, task, available_nodes: List[str], node_capacities: Dict, 
                     node_loads: Dict, compute_service) -> str:
        """è°ƒåº¦å•ä¸ªä»»åŠ¡ï¼Œè¿”å›é€‰æ‹©çš„èŠ‚ç‚¹"""
        raise NotImplementedError

class FIFOScheduler(WRENCHScheduler):
    """å…ˆè¿›å…ˆå‡ºè°ƒåº¦å™¨"""
    
    def __init__(self):
        super().__init__("FIFO")
    
    def schedule_task(self, task, available_nodes, node_capacities, node_loads, compute_service):
        # é€‰æ‹©è´Ÿè½½æœ€å°çš„èŠ‚ç‚¹
        return min(available_nodes, key=lambda x: node_loads.get(x, 0))

class HEFTScheduler(WRENCHScheduler):
    """å¼‚æ„æœ€æ—©å®Œæˆæ—¶é—´è°ƒåº¦å™¨"""
    
    def __init__(self):
        super().__init__("HEFT")
    
    def schedule_task(self, task, available_nodes, node_capacities, node_loads, compute_service):
        # é€‰æ‹©èƒ½æœ€æ—©å®Œæˆä»»åŠ¡çš„èŠ‚ç‚¹
        best_node = None
        best_finish_time = float('inf')
        
        for node in available_nodes:
            capacity = node_capacities.get(node, 1.0)
            load = node_loads.get(node, 0.0)
            exec_time = task.get_flops() / (capacity * 1e9)
            finish_time = load + exec_time
            
            if finish_time < best_finish_time:
                best_finish_time = finish_time
                best_node = node
        
        return best_node or available_nodes[0]

class WASSHeuristicScheduler(WRENCHScheduler):
    """WASSå¯å‘å¼è°ƒåº¦å™¨ - åœ¨HEFTåŸºç¡€ä¸Šè€ƒè™‘æ•°æ®å±€éƒ¨æ€§"""
    
    def __init__(self, data_locality_weight: float = 0.5):
        super().__init__("WASS-Heuristic")
        self.data_locality_weight = data_locality_weight  # wå‚æ•°
        self.data_location_cache = {}  # æ¨¡æ‹Ÿæ•°æ®ä½ç½®ç¼“å­˜
        
        # èŠ‚ç‚¹æ€§èƒ½å‚æ•°
        self.node_capacities = {
            "ComputeHost1": 2.0,
            "ComputeHost2": 3.0,
            "ComputeHost3": 2.5,
            "ComputeHost4": 4.0
        }
    
    def schedule_task(self, task, available_nodes, node_capacities, node_loads, compute_service):
        """ä½¿ç”¨WASSå¯å‘å¼è¿›è¡Œä»»åŠ¡è°ƒåº¦"""
        best_node = None
        best_score = float('inf')
        
        for node in available_nodes:
            # è®¡ç®—EFT (æœ€æ—©å®Œæˆæ—¶é—´)
            eft = self._calculate_eft(task, node, node_capacities, node_loads)
            
            # è®¡ç®—DRT (æ•°æ®å°±ç»ªæ—¶é—´)
            drt = self._calculate_drt(task, node)
            
            # è®¡ç®—WASSç»¼åˆè¯„åˆ†
            w = self.data_locality_weight
            score = (1 - w) * eft + w * drt
            
            if score < best_score:
                best_score = score
                best_node = node
        
        # æ›´æ–°æ•°æ®ä½ç½®ç¼“å­˜ï¼ˆå‡è®¾ä»»åŠ¡è¾“å‡ºæ•°æ®å­˜å‚¨åœ¨æ‰§è¡ŒèŠ‚ç‚¹ï¼‰
        if best_node:
            self._update_data_location(task, best_node)
        
        return best_node or available_nodes[0]
    
    def _calculate_eft(self, task, node, node_capacities, node_loads):
        """è®¡ç®—ä»»åŠ¡åœ¨æŒ‡å®šèŠ‚ç‚¹ä¸Šçš„æœ€æ—©å®Œæˆæ—¶é—´"""
        capacity = node_capacities.get(node, 1.0)
        load = node_loads.get(node, 0.0)
        exec_time = task.get_flops() / (capacity * 1e9)
        return load + exec_time
    
    def predict_makespan(self, task, available_nodes, node_capacities, node_loads):
        """é¢„æµ‹ä»»åŠ¡åœ¨ç»™å®šèŠ‚ç‚¹é…ç½®ä¸‹çš„makespan"""
        try:
            # ç®€åŒ–çš„makespané¢„æµ‹ï¼šåŸºäºä»»åŠ¡å¤§å°å’ŒèŠ‚ç‚¹è´Ÿè½½
            task_flops = float(getattr(task, 'get_flops', lambda: 1e9)())
            
            # è®¡ç®—å¹³å‡èŠ‚ç‚¹æ€§èƒ½
            total_capacity = sum(node_capacities.get(node, 1.0) for node in available_nodes)
            total_load = sum(node_loads.get(node, 0.0) for node in available_nodes)
            avg_capacity = total_capacity / len(available_nodes) if available_nodes else 1.0
            avg_load = total_load / len(available_nodes) if available_nodes else 0.0
            
            # åŸºç¡€æ‰§è¡Œæ—¶é—´
            base_time = task_flops / (avg_capacity * 1e9)
            
            # è€ƒè™‘è´Ÿè½½å½±å“
            load_factor = 1.0 + avg_load / max(avg_capacity, 0.1)
            
            # è€ƒè™‘ä»»åŠ¡ä¾èµ–ï¼ˆç®€åŒ–å¤„ç†ï¼‰
            try:
                children_count = len(getattr(task, 'get_children', lambda: [])())
                dependency_factor = 1.0 + 0.1 * children_count  # æ¯ä¸ªå­ä»»åŠ¡å¢åŠ 10%çš„æ—¶é—´
            except Exception:
                dependency_factor = 1.0
            
            # é¢„æµ‹çš„makespan
            predicted_makespan = base_time * load_factor * dependency_factor
            
            return predicted_makespan
            
        except Exception as e:
            print(f"é¢„æµ‹makespanå¤±è´¥: {e}")
            return 100.0  # é»˜è®¤å€¼
    
    def _calculate_drt(self, task, node):
        """è®¡ç®—æ•°æ®å°±ç»ªæ—¶é—´ - è€ƒè™‘æ•°æ®ä¼ è¾“å¼€é”€"""
        total_transfer_time = 0.0
        
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶çš„æ•°æ®ä½ç½®
        for input_file in task.get_input_files():
            # ä½¿ç”¨æ–‡ä»¶åè€Œä¸æ˜¯get_id()æ–¹æ³•
            file_id = input_file.get_name() if hasattr(input_file, 'get_name') else str(input_file)
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦åœ¨ç›®æ ‡èŠ‚ç‚¹ä¸Š
            data_location = self._get_data_location(file_id)
            if data_location != node:
                # éœ€è¦ä¼ è¾“æ•°æ®
                file_size = input_file.get_size() if hasattr(input_file, 'get_size') else 1024
                network_bandwidth = 1e9  # 1GB/s å‡è®¾ç½‘ç»œå¸¦å®½
                transfer_time = file_size / network_bandwidth
                total_transfer_time += transfer_time
        
        return total_transfer_time
    
    def _get_data_location(self, file_id):
        """è·å–æ–‡ä»¶çš„æ•°æ®ä½ç½®"""
        if file_id not in self.data_location_cache:
            # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œéšæœºé€‰æ‹©ä¸€ä¸ªä½ç½®ï¼ˆæ¨¡æ‹Ÿåˆå§‹æ•°æ®åˆ†å¸ƒï¼‰
            import random
            self.data_location_cache[file_id] = random.choice(
                ["ComputeHost1", "ComputeHost2", "ComputeHost3", "ComputeHost4"]
            )
        return self.data_location_cache[file_id]
    
    def _update_data_location(self, task, node):
        """æ›´æ–°ä»»åŠ¡è¾“å‡ºæ•°æ®çš„ä½ç½®"""
        for output_file in task.get_output_files():
            # ä½¿ç”¨æ–‡ä»¶åè€Œä¸æ˜¯get_id()æ–¹æ³•
            file_id = output_file.get_name() if hasattr(output_file, 'get_name') else str(output_file)
            self.data_location_cache[file_id] = node

# å®šä¹‰DRLç½‘ç»œç»“æ„
class SimpleDQN(nn.Module):
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):
        super(SimpleDQN, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )
    
    def forward(self, x):
        return self.network(x)

class WASSDRLScheduler(WRENCHScheduler):
    """åŸºäºè®­ç»ƒå¥½çš„DRLæ¨¡å‹çš„è°ƒåº¦å™¨"""
    
    def __init__(self, model_path: str, config_path: str = "configs/experiment.yaml"):
        """åˆå§‹åŒ–WASS-DRLè°ƒåº¦å™¨"""
        self.config = load_config(config_path)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        # å®šä¹‰ç½‘ç»œç»“æ„ (ä¸æ”¹è¿›è®­ç»ƒå™¨ä¿æŒä¸€è‡´)
        self.state_dim = 17  # éœ€ä¸è®­ç»ƒä¿æŒä¸€è‡´
        self.action_dim = 4   # èŠ‚ç‚¹æ•°
        self.hidden_dims = [256, 128, 64]
        self.model = self._create_model()
        self.epsilon = 0.1

        # åŠ è½½æ¨¡å‹
        self._load_model(model_path)

        # èŠ‚ç‚¹æ˜ å°„
        self.compute_nodes = ["ComputeHost1", "ComputeHost2", "ComputeHost3", "ComputeHost4"]
    
    def _create_model(self):
        # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…åœ¨æ— torchç¯å¢ƒæ—¶æŠ¥é”™
        from scripts.improved_drl_trainer import ImprovedDQN  # å¤ç”¨å·²å®šä¹‰ç»“æ„
        return ImprovedDQN(self.state_dim, self.action_dim, self.hidden_dims).to(self.device)
    
    def _load_model(self, model_path: str):
        """åŠ è½½è®­ç»ƒå¥½çš„DRLæ¨¡å‹"""
        try:
            # ä¿®å¤PyTorch 2.6å…¼å®¹æ€§é—®é¢˜
            checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
            
            # æ£€æŸ¥æ¨¡å‹æ ¼å¼
            if 'drl_agent' in checkpoint:
                agent_state = checkpoint['drl_agent']
                self.model.load_state_dict(agent_state['model_state_dict'])
                self.epsilon = agent_state.get('epsilon', 0.1)
                print(f"âœ… DRLæ¨¡å‹åŠ è½½æˆåŠŸ (è®­ç»ƒè½®æ•°: {agent_state.get('training_episodes', 'unknown')})")
            elif 'q_network_state_dict' in checkpoint:
                # æ”¹è¿›è®­ç»ƒå™¨æ ¼å¼
                q_state = checkpoint['q_network_state_dict']
                self.epsilon = checkpoint.get('epsilon', 0.1)
                meta = checkpoint.get('metadata', {})
                ck_sd = meta.get('state_dim'); ck_ad = meta.get('action_dim')
                rebuild_needed = False
                if ck_sd and ck_sd != self.state_dim:
                    print(f"â„¹ï¸ é‡æ–°æ„å»ºæ¨¡å‹ä»¥åŒ¹é…checkpoint state_dim={ck_sd}")
                    self.state_dim = ck_sd
                    rebuild_needed = True
                if ck_ad and ck_ad != self.action_dim:
                    print(f"â„¹ï¸ é‡æ–°æ„å»ºæ¨¡å‹ä»¥åŒ¹é…checkpoint action_dim={ck_ad}")
                    self.action_dim = ck_ad
                    rebuild_needed = True
                # å°è¯•ä»q_stateæ¨æ–­éšè—å±‚ç»“æ„
                if rebuild_needed:
                    # çº¿æ€§å±‚æƒé‡é”®æŒ‰é¡ºåº network.X.weight
                    linear_layers = [k for k in q_state.keys() if k.endswith('.weight')]
                    # æ’åºä¿è¯é¡ºåºç¨³å®š
                    linear_layers.sort(key=lambda x: int(x.split('.')[1]))
                    inferred_hidden = []
                    for idx, name in enumerate(linear_layers[:-1]):  # æœ€åä¸€å±‚æ˜¯è¾“å‡ºå±‚
                        shape = q_state[name].shape
                        if idx == 0:
                            # shape: (H1, state_dim)
                            pass  # è¾“å…¥ç»´åº¦æ— éœ€å­˜
                        inferred_hidden.append(shape[0])
                    # å¦‚æœæ£€æµ‹åˆ°æ›´æ·±å±‚ (ä¾‹å¦‚ network.9.*) åˆ™ä½¿ç”¨è®­ç»ƒé»˜è®¤ç»“æ„
                    if any(k.startswith('network.9') for k in q_state.keys()):
                        self.hidden_dims = [256,128,64]
                        print("â„¹ï¸ æ£€æµ‹åˆ°æ·±å±‚ç»“æ„æ ‡è®° network.9.* -> ä½¿ç”¨é»˜è®¤éšè—å±‚ [256,128,64]")
                    elif inferred_hidden:
                        # å»æ‰è¾“å‡ºå±‚å‰ä¸€å±‚å³å®Œæ•´éšè—åˆ—è¡¨
                        self.hidden_dims = inferred_hidden[:-1] if len(inferred_hidden) > 1 else inferred_hidden
                        print(f"â„¹ï¸ æ¨æ–­éšè—å±‚ç»“æ„: {self.hidden_dims}")
                    # é‡æ–°åˆ›å»ºæ¨¡å‹
                    self.model = self._create_model()
                # å°è¯•ç›´æ¥åŠ è½½ï¼›è‹¥å¤±è´¥æ‰§è¡Œéƒ¨åˆ†åŒ¹é…
                try:
                    self.model.load_state_dict(q_state, strict=True)
                    print("âœ… DRLæ¨¡å‹åŠ è½½æˆåŠŸ (æ”¹è¿›æ ¼å¼ strict)")
                except Exception as strict_e:
                    print(f"âš ï¸ ä¸¥æ ¼åŠ è½½å¤±è´¥ï¼Œå°è¯•éƒ¨åˆ†åŠ è½½: {strict_e}")
                    model_dict = self.model.state_dict()
                    filtered = {k: v for k, v in q_state.items() if k in model_dict and model_dict[k].shape == v.shape}
                    model_dict.update(filtered)
                    self.model.load_state_dict(model_dict, strict=False)
                    missing = [k for k in self.model.state_dict().keys() if k not in filtered]
                    print(f"âœ… éƒ¨åˆ†åŠ è½½å®Œæˆï¼ŒåŒ¹é…å‚æ•°æ•°: {len(filtered)}, ç¼ºå¤±å‚æ•°æ•°: {len(missing)}")
            else:
                # å…¼å®¹æ—§æ ¼å¼
                self.model.load_state_dict(checkpoint)
                print("âœ… DRLæ¨¡å‹åŠ è½½æˆåŠŸ (æ—§æ ¼å¼)")
                
            self.model.eval()
            
        except Exception as e:
            print(f"âŒ DRLæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.model = None
    
    def _get_state(self, task, available_nodes, node_capacities, node_loads):
        """è·å–DRLçš„çŠ¶æ€å‘é‡ (å…¼å®¹æ”¹è¿›è®­ç»ƒå™¨ 24 ç»´: 5 + 4*4 + 3)"""
        feats = []
        # ä»»åŠ¡ç‰¹å¾ 5: flops(GF), memory(GB), cores, children, est_runtime(ä¼°è®¡æ‰§è¡Œæ—¶é—´=flops/æœ€å¤§èŠ‚ç‚¹å®¹é‡)
        try:
            task_flops_raw = float(getattr(task, 'get_flops', lambda: 1e9)())
            task_flops = task_flops_raw / 1e9
            try:
                task_memory = float(task.get_memory_requirement()) / 1e9
            except Exception:
                task_memory = 1.0
            task_cores = float(getattr(task, 'get_min_num_cores', lambda: 1)())
            task_children = float(len(getattr(task, 'get_children', lambda: [])()))
            max_cap = max([node_capacities.get(n,1.0) for n in self.compute_nodes]) if hasattr(self,'compute_nodes') else 4.0
            est_runtime = (task_flops_raw / 1e9) / max_cap
            feats.extend([task_flops, task_memory, task_cores, task_children, est_runtime])
        except Exception:
            feats.extend([1.0,1.0,1.0,0.0,1.0])

        # èŠ‚ç‚¹ç‰¹å¾: æ¯èŠ‚ç‚¹4ç»´ (capacity, load, load_ratio, capacity_remaining)
        for node in self.compute_nodes[:4]:
            cap = float(node_capacities.get(node,1.0))
            load = float(node_loads.get(node,0.0))
            ratio = load / max(cap,1e-6)
            remaining = max(cap - load, 0.0)
            feats.extend([cap, load, ratio, remaining])
        # ä¸è¶³èŠ‚ç‚¹å¡«å……
        while len(feats) < 5 + 4*4:
            feats.append(0.0)

        # ç¯å¢ƒç‰¹å¾ 3: avg_load, max_load, std_load
        loads = [node_loads.get(n,0.0) for n in self.compute_nodes]
        if loads:
            avg_load = float(np.mean(loads))
            max_load = float(np.max(loads))
            std_load = float(np.std(loads))
        else:
            avg_load=max_load=std_load=0.0
        feats.extend([avg_load, max_load, std_load])

        # ç›®æ ‡ç»´åº¦ 24
        if len(feats) < 24:
            feats.extend([0.0]*(24-len(feats)))
        elif len(feats) > 24:
            feats = feats[:24]
        return np.array(feats, dtype=np.float32)
    
    def schedule_task(self, task, available_nodes, node_capacities, node_loads, compute_service):
        if self.model is None:
            # æ¨¡å‹æœªåŠ è½½ï¼Œæ”¹ä¸ºå¯å‘å¼å›é€€è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            print("âš ï¸ DRLæ¨¡å‹æœªæ­£ç¡®åŠ è½½ï¼Œä½¿ç”¨å¯å‘å¼å›é€€è°ƒåº¦")
            return self._heuristic_fallback(task, available_nodes, node_capacities, node_loads)

        try:
            state = self._get_state(task, available_nodes, node_capacities, node_loads)
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)

            with torch.no_grad():
                q_values = self.model(state_tensor)
                action = q_values.argmax().item()

            # åŠ¨ä½œæ˜ å°„åˆ°èŠ‚ç‚¹
            if action < len(self.compute_nodes):
                chosen_node = self.compute_nodes[action]
                if chosen_node in available_nodes:
                    return chosen_node

            # å¦‚æœé€‰æ‹©çš„èŠ‚ç‚¹ä¸å¯ç”¨ï¼Œä½¿ç”¨å¯å‘å¼å›é€€
            print("âš ï¸ DRLé€‰æ‹©èŠ‚ç‚¹ä¸å¯ç”¨ï¼Œä½¿ç”¨å¯å‘å¼å›é€€")
            return self._heuristic_fallback(task, available_nodes, node_capacities, node_loads)

        except Exception as e:
            print(f"âš ï¸ DRLè°ƒåº¦å¤±è´¥ï¼Œå°†ä½¿ç”¨å¯å‘å¼å›é€€: {e}")
            return self._heuristic_fallback(task, available_nodes, node_capacities, node_loads)
    
    def _heuristic_fallback(self, task, available_nodes, node_capacities, node_loads):
        """å¯å‘å¼å›é€€è°ƒåº¦ç­–ç•¥"""
        try:
            # è·å–ä»»åŠ¡ç‰¹å¾
            task_flops = float(getattr(task, 'get_flops', lambda: 1e9)())
            
            # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„å¾—åˆ†ï¼ˆè€ƒè™‘å®¹é‡å’Œè´Ÿè½½ï¼‰
            best_node = None
            best_score = -float('inf')
            
            for node in available_nodes:
                capacity = node_capacities.get(node, 1.0)
                load = node_loads.get(node, 0.0)
                
                # è®¡ç®—å¾—åˆ†ï¼šå®¹é‡è¶Šé«˜è¶Šå¥½ï¼Œè´Ÿè½½è¶Šä½è¶Šå¥½
                score = capacity - load * 2.0  # è´Ÿè½½æƒé‡æ›´é«˜
                
                if score > best_score:
                    best_score = score
                    best_node = node
            
            return best_node if best_node else available_nodes[0]
            
        except Exception as e:
            print(f"âš ï¸ å¯å‘å¼å›é€€ä¹Ÿå¤±è´¥: {e}ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨èŠ‚ç‚¹")
            return available_nodes[0]

class WASSRAGScheduler(WRENCHScheduler):
    """åŸºäºRAGçŸ¥è¯†åº“å¢å¼ºçš„è°ƒåº¦å™¨ (é‡æ„: ä¼˜å…ˆç»Ÿä¸€JSONæ ¼å¼)."""

    def __init__(self, model_path: str, rag_path: str, enable_fusion: bool = True):
        super().__init__("WASS-RAG")
        self.drl_scheduler = WASSDRLScheduler(model_path)
        self.knowledge_base = []  # list[dict]
        self.rag_source = None
        self.enable_fusion = enable_fusion
        self._load_rag_knowledge(rag_path)

    def _load_rag_knowledge(self, rag_path: str):
        # 1. æ ‡å‡† JSON (æ–°æ ¼å¼)
        preferred_json = "data/wrench_rag_knowledge_base.json"
        candidate_jsons = [preferred_json, "data/enhanced_rag_knowledge.json", "data/extended_rag_knowledge.json", rag_path.replace('.pkl', '.json')]
        for jpath in candidate_jsons:
            if not os.path.exists(jpath):
                continue
            try:
                with open(jpath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                cases = []
                if isinstance(data, dict):
                    if 'cases' in data:
                        cases = data['cases']
                    elif 'sample_cases' in data:
                        cases = data['sample_cases']
                elif isinstance(data, list):
                    cases = data
                norm = []
                for c in cases:
                    if not isinstance(c, dict):
                        continue
                    norm.append({
                        'task_flops': float(c.get('task_flops', c.get('task_execution_time', 1.0) * 2e9)),
                        'chosen_node': str(c.get('chosen_node', 'ComputeHost1')),
                        'scheduler_type': str(c.get('scheduler_type', 'unknown')),
                        'task_execution_time': float(c.get('task_execution_time', 0.0)),
                        'workflow_makespan': float(c.get('workflow_makespan', 0.0)),
                        'total_workflow_flops': float(c.get('total_workflow_flops', c.get('task_flops', 1e9))),
                        'workflow_size': int(c.get('workflow_size', 0))
                    })
                if norm:
                    self.knowledge_base = norm
                    self.rag_source = jpath
                    print(f"âœ… RAGçŸ¥è¯†åº“(JSON)å·²åŠ è½½: {len(self.knowledge_base)} æ¡ˆä¾‹ (æº: {jpath})")
                    return
            except Exception as e:
                print(f"JSONåŠ è½½å¤±è´¥({jpath}): {e}")

        # 2. æ—§ pickle (å‘åå…¼å®¹)
        if os.path.exists(rag_path):
            try:
                import pickle
                with open(rag_path, 'rb') as f:
                    pdata = pickle.load(f)
                if isinstance(pdata, dict):
                    pcases = pdata.get('cases', [])
                elif isinstance(pdata, list):
                    pcases = pdata
                else:
                    pcases = [pdata]
                norm = []
                for c in pcases:
                    if hasattr(c, '__dict__'):
                        c = c.__dict__
                    if not isinstance(c, dict):
                        continue
                    norm.append({
                        'task_flops': float(c.get('task_flops', c.get('task_execution_time', 1.0) * 2e9)),
                        'chosen_node': str(c.get('chosen_node', 'ComputeHost1')),
                        'scheduler_type': str(c.get('scheduler_type', 'unknown')),
                        'task_execution_time': float(c.get('task_execution_time', 0.0)),
                        'workflow_makespan': float(c.get('workflow_makespan', 0.0))
                    })
                if norm:
                    self.knowledge_base = norm
                    self.rag_source = rag_path
                    print(f"âœ… RAGçŸ¥è¯†åº“(PKL)å·²åŠ è½½: {len(self.knowledge_base)} æ¡ˆä¾‹ (æº: {rag_path})")
                    return
            except Exception as e:
                print(f"PKLåŠ è½½å¤±è´¥({rag_path}): {e}")

        # 3. é»˜è®¤çŸ¥è¯†åº“
        print("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆRAGçŸ¥è¯†åº“ï¼Œä½¿ç”¨é»˜è®¤å†…ç½®æ¡ˆä¾‹ã€‚")
        self._create_default_knowledge_base()
    
    def _create_default_knowledge_base(self):
        """åˆ›å»ºé»˜è®¤çš„RAGçŸ¥è¯†åº“ï¼ˆå¢å¼ºè´Ÿè½½å‡è¡¡æ¡ˆä¾‹ï¼‰"""
        # åŸºäºèŠ‚ç‚¹æ€§èƒ½å’Œä»»åŠ¡ç‰¹å¾çš„å¢å¼ºå¯å‘å¼è§„åˆ™ï¼Œé‡ç‚¹è€ƒè™‘è´Ÿè½½å‡è¡¡
        default_cases = [
            # å°ä»»åŠ¡ä¼˜å…ˆåˆ†é…åˆ°è´Ÿè½½ä½çš„èŠ‚ç‚¹ï¼ˆä¸ä¸€å®šæ˜¯æœ€é«˜å®¹é‡èŠ‚ç‚¹ï¼‰
            {'task_flops': 1e9, 'chosen_node': 'ComputeHost1', 'scheduler_type': 'heuristic', 
             'task_execution_time': 0.5, 'workflow_makespan': 4.5, 'node_capacity': 2.0,
             'performance_ratio': 0.9, 'total_workflow_flops': 5e9, 'workflow_size': 5,
             'load_balance_factor': 0.8, 'node_load': 0.2},
            
            # ä¸­ç­‰ä»»åŠ¡åˆ†é…åˆ°ä¸­ç­‰å®¹é‡ä¸”è´Ÿè½½é€‚ä¸­çš„èŠ‚ç‚¹
            {'task_flops': 5e9, 'chosen_node': 'ComputeHost2', 'scheduler_type': 'heuristic',
             'task_execution_time': 1.67, 'workflow_makespan': 9.5, 'node_capacity': 3.0,
             'performance_ratio': 0.85, 'total_workflow_flops': 20e9, 'workflow_size': 10,
             'load_balance_factor': 0.75, 'node_load': 0.4},
            
            # å¤§ä»»åŠ¡åˆ†é…åˆ°é«˜å®¹é‡èŠ‚ç‚¹ï¼Œä½†è¦è€ƒè™‘è´Ÿè½½å‡è¡¡
            {'task_flops': 10e9, 'chosen_node': 'ComputeHost4', 'scheduler_type': 'heuristic',
             'task_execution_time': 2.5, 'workflow_makespan': 14.0, 'node_capacity': 4.0,
             'performance_ratio': 0.9, 'total_workflow_flops': 50e9, 'workflow_size': 15,
             'load_balance_factor': 0.7, 'node_load': 0.3},
            
            # è´Ÿè½½å‡è¡¡æ¡ˆä¾‹ï¼šä¼˜å…ˆé€‰æ‹©è´Ÿè½½ä½çš„èŠ‚ç‚¹
            {'task_flops': 3e9, 'chosen_node': 'ComputeHost3', 'scheduler_type': 'heuristic',
             'task_execution_time': 1.2, 'workflow_makespan': 7.5, 'node_capacity': 2.5,
             'performance_ratio': 0.88, 'total_workflow_flops': 15e9, 'workflow_size': 8,
             'load_balance_factor': 0.95, 'node_load': 0.1},
            
            # é«˜è´Ÿè½½æƒ…å†µä¸‹é€‰æ‹©ç©ºé—²èŠ‚ç‚¹
            {'task_flops': 7e9, 'chosen_node': 'ComputeHost1', 'scheduler_type': 'heuristic',
             'task_execution_time': 3.5, 'workflow_makespan': 11.0, 'node_capacity': 2.0,
             'performance_ratio': 0.82, 'total_workflow_flops': 30e9, 'workflow_size': 12,
             'load_balance_factor': 0.9, 'node_load': 0.05},
            
            # æ–°å¢ï¼šè´Ÿè½½å‡è¡¡ä¼˜å…ˆæ¡ˆä¾‹
            {'task_flops': 2e9, 'chosen_node': 'ComputeHost2', 'scheduler_type': 'load_balance',
             'task_execution_time': 0.67, 'workflow_makespan': 6.0, 'node_capacity': 3.0,
             'performance_ratio': 0.87, 'total_workflow_flops': 12e9, 'workflow_size': 6,
             'load_balance_factor': 0.92, 'node_load': 0.15},
            
            # æ–°å¢ï¼šé¿å…é«˜è´Ÿè½½èŠ‚ç‚¹
            {'task_flops': 4e9, 'chosen_node': 'ComputeHost3', 'scheduler_type': 'load_balance',
             'task_execution_time': 1.6, 'workflow_makespan': 8.5, 'node_capacity': 2.5,
             'performance_ratio': 0.83, 'total_workflow_flops': 18e9, 'workflow_size': 9,
             'load_balance_factor': 0.88, 'node_load': 0.12},
            
            # æ–°å¢ï¼šå¤§ä»»åŠ¡åœ¨é«˜è´Ÿè½½ç¯å¢ƒä¸‹é€‰æ‹©æ¬¡ä¼˜å®¹é‡ä½†ä½è´Ÿè½½çš„èŠ‚ç‚¹
            {'task_flops': 8e9, 'chosen_node': 'ComputeHost3', 'scheduler_type': 'load_balance',
             'task_execution_time': 3.2, 'workflow_makespan': 12.5, 'node_capacity': 2.5,
             'performance_ratio': 0.8, 'total_workflow_flops': 40e9, 'workflow_size': 14,
             'load_balance_factor': 0.85, 'node_load': 0.18},
            
            # æ–°å¢ï¼šä¸­ç­‰ä»»åŠ¡åœ¨å‡è¡¡ç¯å¢ƒä¸‹çš„åˆ†é…
            {'task_flops': 6e9, 'chosen_node': 'ComputeHost4', 'scheduler_type': 'balanced',
             'task_execution_time': 1.5, 'workflow_makespan': 10.0, 'node_capacity': 4.0,
             'performance_ratio': 0.92, 'total_workflow_flops': 25e9, 'workflow_size': 11,
             'load_balance_factor': 0.8, 'node_load': 0.25},
            
            # æ–°å¢ï¼šå°ä»»åŠ¡åœ¨é«˜å®¹é‡ä½†é«˜è´Ÿè½½èŠ‚ç‚¹ vs ä½å®¹é‡ä½è´Ÿè½½èŠ‚ç‚¹çš„é€‰æ‹©
            {'task_flops': 1.5e9, 'chosen_node': 'ComputeHost1', 'scheduler_type': 'balanced',
             'task_execution_time': 0.75, 'workflow_makespan': 5.5, 'node_capacity': 2.0,
             'performance_ratio': 0.86, 'total_workflow_flops': 8e9, 'workflow_size': 7,
             'load_balance_factor': 0.93, 'node_load': 0.08}
        ]
        
        # æ·»åŠ æ›´å¤šå¤šæ ·åŒ–æ¡ˆä¾‹ï¼Œé‡ç‚¹è€ƒè™‘è´Ÿè½½å‡è¡¡
        import random
        random.seed(42)  # ç¡®ä¿å¯é‡ç°
        
        # ç”Ÿæˆè´Ÿè½½å‡è¡¡å¯¼å‘çš„æ¡ˆä¾‹
        for _ in range(30):  # ç”Ÿæˆ30ä¸ªé¢å¤–æ¡ˆä¾‹
            base_case = random.choice(default_cases)
            variation = base_case.copy()
            
            # æ·»åŠ ä¸€äº›éšæœºå˜åŒ–
            variation['task_flops'] *= random.uniform(0.7, 1.3)
            variation['total_workflow_flops'] *= random.uniform(0.8, 1.2)
            variation['workflow_size'] = max(3, int(variation['workflow_size'] * random.uniform(0.7, 1.3)))
            
            # æ ¹æ®è´Ÿè½½å‡è¡¡åŸåˆ™é€‰æ‹©èŠ‚ç‚¹
            node_loads = {
                'ComputeHost1': random.uniform(0.0, 0.8),
                'ComputeHost2': random.uniform(0.0, 0.8),
                'ComputeHost3': random.uniform(0.0, 0.8),
                'ComputeHost4': random.uniform(0.0, 0.8)
            }
            
            # é€‰æ‹©è´Ÿè½½æœ€ä½çš„èŠ‚ç‚¹ï¼Œä½†è€ƒè™‘ä»»åŠ¡å¤§å°
            if variation['task_flops'] < 3e9:
                # å°ä»»åŠ¡ï¼šä¼˜å…ˆé€‰æ‹©è´Ÿè½½ä½çš„èŠ‚ç‚¹
                sorted_nodes = sorted(node_loads.keys(), key=lambda x: node_loads[x])
                variation['chosen_node'] = sorted_nodes[0]
            elif variation['task_flops'] < 7e9:
                # ä¸­ç­‰ä»»åŠ¡ï¼šåœ¨è´Ÿè½½è¾ƒä½çš„èŠ‚ç‚¹ä¸­é€‰æ‹©å®¹é‡é€‚ä¸­çš„
                low_load_nodes = [n for n in node_loads.keys() if node_loads[n] < 0.5]
                if low_load_nodes:
                    medium_capacity_nodes = [n for n in low_load_nodes if n in ['ComputeHost2', 'ComputeHost3']]
                    if medium_capacity_nodes:
                        variation['chosen_node'] = random.choice(medium_capacity_nodes)
                    else:
                        variation['chosen_node'] = random.choice(low_load_nodes)
                else:
                    variation['chosen_node'] = 'ComputeHost2'
            else:
                # å¤§ä»»åŠ¡ï¼šåœ¨é«˜å®¹é‡èŠ‚ç‚¹ä¸­é€‰æ‹©è´Ÿè½½è¾ƒä½çš„
                high_capacity_nodes = ['ComputeHost3', 'ComputeHost4']
                low_load_high_cap = [n for n in high_capacity_nodes if node_loads[n] < 0.6]
                if low_load_high_cap:
                    variation['chosen_node'] = random.choice(low_load_high_cap)
                else:
                    # å¦‚æœé«˜å®¹é‡èŠ‚ç‚¹éƒ½è´Ÿè½½é«˜ï¼Œé€‰æ‹©è´Ÿè½½æœ€ä½çš„
                    sorted_nodes = sorted(high_capacity_nodes, key=lambda x: node_loads[x])
                    variation['chosen_node'] = sorted_nodes[0]
            
            # æ›´æ–°è´Ÿè½½å‡è¡¡å› å­
            variation['load_balance_factor'] = 1.0 - node_loads[variation['chosen_node']]
            variation['node_load'] = node_loads[variation['chosen_node']]
            
            self.knowledge_base.append(variation)
        
        # æ·»åŠ åŸå§‹é»˜è®¤æ¡ˆä¾‹
        self.knowledge_base.extend(default_cases)
        
        print(f"âœ… å¢å¼ºé»˜è®¤RAGçŸ¥è¯†åº“å·²åˆ›å»º: {len(self.knowledge_base)} ä¸ªæ¡ˆä¾‹ï¼ˆé‡ç‚¹è€ƒè™‘è´Ÿè½½å‡è¡¡ï¼‰")
    
    def schedule_task(self, task, available_nodes, node_capacities, node_loads, compute_service):
        """åŸºäºRAGçŸ¥è¯†åº“å¢å¼ºçš„è°ƒåº¦å†³ç­– - ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆé‡ç‚¹è§£å†³è´Ÿè½½å‡è¡¡é—®é¢˜ï¼‰"""
        try:
            # é¦–å…ˆä½¿ç”¨DRLè¿›è¡ŒåŸºç¡€è°ƒåº¦å†³ç­–
            drl_node = self.drl_scheduler.schedule_task(
                task, available_nodes, node_capacities, node_loads, compute_service
            )
            # å¦‚æœæ²¡æœ‰çŸ¥è¯†åº“ï¼Œç›´æ¥å›é€€åˆ°DRLå†³ç­–
            if not self.knowledge_base:
                print("âš ï¸ RAGçŸ¥è¯†åº“ä¸ºç©ºï¼Œå›é€€åˆ°DRLå†³ç­–")
                return drl_node
            
            # è·å–æ›´ä¸°å¯Œçš„ä»»åŠ¡ç‰¹å¾ç”¨äºRAGåŒ¹é…
            try:
                task_flops = float(getattr(task, 'get_flops', lambda: 1e9)())
                task_memory = float(getattr(task, 'get_memory_requirement', lambda: 1024)())
                
                # è®¡ç®—å·¥ä½œæµç‰¹å¾
                total_workflow_flops = sum(
                    float(getattr(t, 'get_flops', lambda: 1e9)()) 
                    for t in [task]  # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥è·å–æ•´ä¸ªå·¥ä½œæµ
                )
                
                # è®¡ç®—å½“å‰èŠ‚ç‚¹è´Ÿè½½ç‰¹å¾
                avg_load = np.mean([node_loads.get(node, 0) for node in available_nodes])
                max_load = max([node_loads.get(node, 0) for node in available_nodes])
                
                # è®¡ç®—è´Ÿè½½å‡è¡¡æŒ‡æ ‡
                load_variance = np.var([node_loads.get(node, 0) for node in available_nodes])
                load_std = np.sqrt(load_variance)
                
            except Exception as e:
                task_flops = 1e9
                task_memory = 1024
                total_workflow_flops = task_flops
                avg_load = 0
                max_load = 0
                load_variance = 0
                load_std = 0
            
            # å¢å¼ºçš„ç›¸ä¼¼åº¦åŒ¹é… - è¿›ä¸€æ­¥é™ä½é˜ˆå€¼ä»¥è·å–æ›´å¤šåŒ¹é…
            best_matches = []
            min_similarity_threshold = 0.01  # ä»0.05è¿›ä¸€æ­¥é™ä½åˆ°0.01ï¼Œæå¤§å¢åŠ åŒ¹é…æœºä¼š
            
            for case in self.knowledge_base:
                # å¤šç»´ç‰¹å¾ç›¸ä¼¼åº¦è®¡ç®—
                case_flops = float(case.get('task_flops', 1e9))
                case_workflow_flops = float(case.get('total_workflow_flops', case_flops))
                
                # è®¡ç®—å½’ä¸€åŒ–è·ç¦»
                flops_distance = abs(task_flops - case_flops) / max(task_flops, case_flops, 1e-6)
                workflow_distance = abs(total_workflow_flops - case_workflow_flops) / max(total_workflow_flops, case_workflow_flops, 1e-6)
                
                # ç»¼åˆç›¸ä¼¼åº¦
                similarity = 1.0 - (flops_distance * 0.6 + workflow_distance * 0.4)
                
                if similarity >= min_similarity_threshold:
                    best_matches.append({
                        'case': case,
                        'similarity': similarity,
                        'suggested_node': str(case.get('chosen_node', drl_node))
                    })
            
            # å¦‚æœæœ‰é«˜è´¨é‡åŒ¹é…ï¼Œå‡†å¤‡èåˆæ‰€éœ€ rag_scores
            rag_scores = []
            match_node_scores = {}
            if best_matches:
                # æŒ‰makespanæ’åºï¼Œé€‰æ‹©makespanæœ€ä½çš„æ¡ˆä¾‹
                best_matches.sort(key=lambda x: float(x['case'].get('makespan', float('inf'))))
                top_matches = best_matches[:8]
                for match in top_matches:
                    node = match['suggested_node']
                    match_node_scores.setdefault(node, 0.0)
                    # ç»¼åˆè€ƒè™‘ç›¸ä¼¼åº¦å’Œmakespanï¼ˆmakespanè¶Šä½ï¼Œæƒé‡è¶Šé«˜ï¼‰
                    makespan = float(match['case'].get('makespan', 100.0))
                    makespan_weight = 1.0 / (1.0 + makespan / 100.0)  # å½’ä¸€åŒ–makespanæƒé‡
                    match_node_scores[node] += match['similarity'] * makespan_weight
                    
                    # è´Ÿè½½å‡è¡¡è°ƒæ•´ï¼šå¦‚æœèŠ‚ç‚¹è´Ÿè½½è¿‡é«˜ï¼Œæå¤§å¹…é™ä½å…¶å¾—åˆ†
                    node_load = node_loads.get(node, 0.0)
                    if node_load > avg_load * 1.05:  # è¿›ä¸€æ­¥é™ä½é˜ˆå€¼ï¼Œä»1.1å€æ”¹ä¸º1.05å€
                        load_penalty = 0.01  # é™ä½99%çš„å¾—åˆ†ï¼ˆä»90%è¿›ä¸€æ­¥å¢å¼ºåˆ°99%ï¼‰
                        match_node_scores[node] *= load_penalty
                        
            for node in available_nodes:
                rag_scores.append(match_node_scores.get(node, 0.0))

            # èåˆå†³ç­–
            if self.enable_fusion and rag_scores:
                try:
                    from src.scheduling.hybrid_fusion import fuse_decision
                    # è·å– DRL æ¨¡å‹çœŸå® Q-values
                    q_values = []
                    try:
                        state = self.drl_scheduler._get_state(task, available_nodes, node_capacities, node_loads)
                        if self.drl_scheduler.model is not None:
                            import torch
                            st = torch.FloatTensor(state).unsqueeze(0).to(self.drl_scheduler.device)
                            with torch.no_grad():
                                q_tensor = self.drl_scheduler.model(st)
                            q_list = q_tensor.squeeze(0).cpu().tolist()
                            # åªå–ä¸ available_nodes å¯¹åº”çš„å‰ len(available_nodes) ä¸ªåŠ¨ä½œ
                            q_values = q_list[:len(available_nodes)]
                    except Exception as qe:
                        print(f"è·å–çœŸå®Qå€¼å¤±è´¥ï¼Œå›é€€ä¼ªQ: {qe}")
                    if not q_values:
                        for node in available_nodes:
                            cap = node_capacities.get(node, 1.0)
                            load = node_loads.get(node, 0.0)
                            # æå¼ºå¢å¼ºè´Ÿè½½å‡è¡¡è€ƒè™‘
                            load_balance_factor = 1.0 / (1.0 + 20.0 * load)  # æå¼ºå¢å¼ºè´Ÿè½½å‡è¡¡å› å­
                            q_values.append(cap * load_balance_factor)
                    load_vals = [node_loads.get(n, 0.0) for n in available_nodes]
                    progress = 0.5  # TODO: ä½¿ç”¨çœŸå®è®­ç»ƒè¿›åº¦
                    
                    # è®¡ç®—makespané¢„æµ‹ï¼ˆåŸºäºå†å²æ¡ˆä¾‹å’Œå½“å‰çŠ¶æ€ï¼‰
                    makespan_predictions = []
                    baseline_makespan = None
                    
                    # è·å–åŸºå‡†makespanï¼ˆHEFTç®—æ³•é¢„æµ‹ï¼‰
                    try:
                        heft_scheduler = HEFTScheduler()
                        heft_prediction = heft_scheduler.predict_makespan(task, available_nodes, node_capacities, node_loads)
                        if heft_prediction > 0:
                            baseline_makespan = heft_prediction
                    except Exception as e:
                        print(f"è·å–HEFTåŸºå‡†makespanå¤±è´¥: {e}")
                    
                    # ä¸ºæ¯ä¸ªèŠ‚ç‚¹é¢„æµ‹makespan
                    for node in available_nodes:
                        predicted_makespan = baseline_makespan or 100.0  # é»˜è®¤å€¼
                        
                        # åŸºäºå½“å‰è´Ÿè½½å’Œå®¹é‡è°ƒæ•´é¢„æµ‹
                        node_load = node_loads.get(node, 0.0)
                        node_capacity = node_capacities.get(node, 1.0)
                        load_factor = 1.0 + node_load / max(node_capacity, 0.1)
                        predicted_makespan *= load_factor
                        
                        # åŸºäºRAGåŒ¹é…åº¦è°ƒæ•´é¢„æµ‹ï¼ˆåŒ¹é…åº¦è¶Šé«˜ï¼Œé¢„æµ‹makespanè¶Šä½ï¼‰
                        rag_score = match_node_scores.get(node, 0.0)
                        if rag_score > 0:
                            rag_factor = 1.0 - 0.5 * rag_score  # æœ€é«˜å¯å‡å°‘50%çš„makespan
                            predicted_makespan *= rag_factor
                        
                        makespan_predictions.append(predicted_makespan)
                    
                    # å¢å¼ºè´Ÿè½½å‡è¡¡æƒé‡ï¼ŒåŠ å…¥makespané¢„æµ‹
                    fusion = fuse_decision(
                        q_values, 
                        rag_scores, 
                        load_vals, 
                        progress, 
                        rag_confidence_threshold=0.0001,  # è¿›ä¸€æ­¥é™ä½é˜ˆå€¼
                        makespan_predictions=makespan_predictions,
                        baseline_makespan=baseline_makespan
                    )
                    fused_idx = fusion['index']
                    fused_node = available_nodes[fused_idx]
                    print(f"ğŸ”€ èåˆå†³ç­–: {fused_node} (Î±={fusion['alpha']:.2f}, Î²={fusion['beta']:.2f}, Î³={fusion['gamma']:.2f}, Î´={fusion.get('delta', 0.0):.2f})")
                    
                    # è®°å½•èåˆå†³ç­–çš„è¯¦ç»†ä¿¡æ¯
                    try:
                        import json, os
                        os.makedirs('results', exist_ok=True)
                        with open('results/fusion_debug.log', 'a', encoding='utf-8') as fdbg:
                            record = {
                                'node': fused_node,
                                'alpha': fusion['alpha'],
                                'beta': fusion['beta'],
                                'gamma': fusion['gamma'],
                                'delta': fusion.get('delta', 0.0),
                                'q_norm': fusion['q_norm'],
                                'rag_norm': fusion['rag_norm'],
                                'load_pref': fusion['load_pref'],
                                'makespan_scores': fusion.get('makespan_scores', []),
                                'fused': fusion['fused'],
                                'load_variance': load_variance,
                                'load_std': load_std,
                                'avg_load': avg_load,
                                'max_load': max_load,
                                'makespan_predictions': makespan_predictions,
                                'baseline_makespan': baseline_makespan
                            }
                            fdbg.write(json.dumps(record, ensure_ascii=False) + '\n')
                    except Exception as le:
                        print(f"èåˆè°ƒè¯•æ—¥å¿—å†™å…¥å¤±è´¥: {le}")
                    return fused_node
                except Exception as fe:
                    print(f"èåˆå¤±è´¥ï¼Œå›é€€RAG/DRL: {fe}")

            # æ— èåˆæˆ–å¤±è´¥ï¼šä½¿ç”¨å¢å¼ºçš„è´Ÿè½½å‡è¡¡ç­–ç•¥
            if match_node_scores:
                # ç»“åˆç›¸ä¼¼åº¦å’Œè´Ÿè½½å‡è¡¡é€‰æ‹©èŠ‚ç‚¹
                best_node = None
                best_score = -float('inf')
                
                for node in available_nodes:
                    # åŸºç¡€å¾—åˆ†ï¼šRAGç›¸ä¼¼åº¦
                    node_score = match_node_scores.get(node, 0.0)
                    
                    # è´Ÿè½½å‡è¡¡è°ƒæ•´
                    node_load = node_loads.get(node, 0.0)
                    load_balance_factor = 1.0 / (1.0 + 20.0 * load)  # æå¼ºå¢å¼ºè´Ÿè½½å‡è¡¡å› å­
                    
                    # èŠ‚ç‚¹å®¹é‡è€ƒè™‘
                    node_capacity = node_capacities.get(node, 1.0)
                    
                    # ç»¼åˆå¾—åˆ†
                    combined_score = node_score * load_balance_factor * node_capacity
                    
                    if combined_score > best_score:
                        best_score = combined_score
                        best_node = node
                
                if best_node:
                    return best_node
            
            # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„åŒ¹é…ï¼Œä½¿ç”¨å¢å¼ºçš„å¯å‘å¼ç­–ç•¥
            print("âš ï¸ æ— è¶³å¤ŸRAGåŒ¹é…æ¡ˆä¾‹ï¼Œä½¿ç”¨å¢å¼ºå¯å‘å¼ç­–ç•¥")
            best_node = None
            best_score = -float('inf')
            
            for node in available_nodes:
                capacity = node_capacities.get(node, 1.0)
                load = node_loads.get(node, 0.0)
                
                # æå¼ºå¢å¼ºè´Ÿè½½å‡è¡¡å› å­
                load_balance_factor = 1.0 / (1.0 + 20.0 * load)  # æå¼ºå¢å¼ºè´Ÿè½½å‡è¡¡å› å­
                score = capacity * load_balance_factor
                
                if score > best_score:
                    best_score = score
                    best_node = node
            
            return best_node if best_node else available_nodes[0]

        except Exception as e:
            print(f"âš ï¸ RAGè°ƒåº¦å¤±è´¥: {e}ï¼Œå°è¯•å›é€€")
            # ä¼˜å…ˆå°è¯•ç›´æ¥ä½¿ç”¨å·²è·å¾—çš„drl_nodeï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            try:
                if 'drl_node' in locals() and drl_node in available_nodes:
                    return drl_node
            except Exception:
                pass
            # æœ€ç»ˆå›é€€åˆ°å¯å‘å¼
            try:
                return self.drl_scheduler._heuristic_fallback(task, available_nodes, node_capacities, node_loads)
            except Exception:
                # å…œåº•ï¼šè¿”å›ç¬¬ä¸€ä¸ªå¯ç”¨èŠ‚ç‚¹
                return available_nodes[0]

class WRENCHExperimentRunner:
    """åŸºäºçœŸå®WRENCHçš„å®éªŒè¿è¡Œå™¨"""
    
    def __init__(self, config_path: str = "configs/experiment.yaml"):
        self.config = load_config(config_path)
        
        # WRENCHå¹³å°é…ç½®
        self.platform_file = self.config['platform']['platform_file']
        self.controller_host = "ControllerHost"
        
        # èŠ‚ç‚¹é…ç½®
        self.compute_nodes = ["ComputeHost1", "ComputeHost2", "ComputeHost3", "ComputeHost4"]
        self.node_capacities = {
            "ComputeHost1": 2.0,
            "ComputeHost2": 3.0,
            "ComputeHost3": 2.5,
            "ComputeHost4": 4.0
        }
        
        # è°ƒåº¦å™¨é…ç½®
        self.schedulers = self._initialize_schedulers()
        
        # å®éªŒå‚æ•°
        self.workflow_sizes = [5, 10, 15, 20]
        self.repetitions = 3
        
        # ç»“æœå­˜å‚¨
        self.results = []
        
        print(f"ğŸš€ WRENCHå®éªŒè¿è¡Œå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_schedulers(self):
        """åˆå§‹åŒ–æ‰€æœ‰è°ƒåº¦å™¨"""
        schedulers = {
            "FIFO": FIFOScheduler(),
            "HEFT": HEFTScheduler(),
            "WASS-Heuristic": WASSHeuristicScheduler(),  # æ–°å¢WASSå¯å‘å¼è°ƒåº¦å™¨
        }
        
        # æ¨¡å‹æ–‡ä»¶ä¼˜å…ˆçº§ï¼šå…¼å®¹æ¨¡å‹ > åŸå§‹ä¼˜åŒ–æ¨¡å‹ > åŸºç¡€æ¨¡å‹
        model_candidates = [
            "models/improved_wass_drl.pth",  # æ–°è®­ç»ƒæ”¹è¿›æ¨¡å‹
            "models/wass_optimized_models_compatible.pth",
            "models/wass_optimized_models.pth",
            "models/wass_models.pth"
        ]

        # ç¯å¢ƒå˜é‡ä¼˜å…ˆ (WASS_DRL_MODEL)
        env_model = os.environ.get("WASS_DRL_MODEL")
        if env_model:
            if os.path.exists(env_model):
                if env_model not in model_candidates:
                    model_candidates.insert(0, env_model)
                else:
                    # ç¡®ä¿ç¯å¢ƒå˜é‡è·¯å¾„æ’åˆ°é¦–ä½
                    model_candidates.remove(env_model)
                    model_candidates.insert(0, env_model)
                print(f"ğŸ” é€šè¿‡ç¯å¢ƒå˜é‡æŒ‡å®šæ¨¡å‹: {env_model}")
            else:
                print(f"âš ï¸ ç¯å¢ƒå˜é‡WASS_DRL_MODELæŒ‡å®šçš„æ¨¡å‹ä¸å­˜åœ¨: {env_model}")
        
        model_path = None
        for candidate in model_candidates:
            if os.path.exists(candidate):
                model_path = candidate
                break
        
        rag_path = "data/wrench_rag_knowledge_base.pkl"
        
        if model_path:
            print(f"ğŸ“ ä½¿ç”¨æ¨¡å‹æ–‡ä»¶: {model_path}")
            
            # å¼ºåˆ¶å¯ç”¨WASS-DRLè°ƒåº¦å™¨
            try:
                drl_scheduler = WASSDRLScheduler(model_path)
                schedulers["WASS-DRL"] = drl_scheduler
                print("âœ… WASS-DRLè°ƒåº¦å™¨å·²å¼ºåˆ¶å¯ç”¨")
                
                # å¼ºåˆ¶å¯ç”¨WASS-RAGè°ƒåº¦å™¨
                rag_candidates = [
                    rag_path,
                    "data/wrench_rag_knowledge_base.json",
                    "data/extended_rag_knowledge.json"
                ]
                
                rag_available = False
                for rag_candidate in rag_candidates:
                    if os.path.exists(rag_candidate):
                        try:
                            rag_scheduler = WASSRAGScheduler(model_path, rag_candidate)
                            schedulers["WASS-RAG"] = rag_scheduler
                            print(f"âœ… WASS-RAGè°ƒåº¦å™¨å·²å¯ç”¨ (çŸ¥è¯†åº“: {rag_candidate})")
                            rag_available = True
                            break
                        except Exception as e:
                            print(f"âš ï¸  WASS-RAGä»{rag_candidate}åŠ è½½å¤±è´¥: {e}")
                            continue
                
                if not rag_available:
                    # å³ä½¿æ²¡æœ‰çŸ¥è¯†åº“ï¼Œä¹Ÿåˆ›å»ºç©ºçš„RAGè°ƒåº¦å™¨
                    rag_scheduler = WASSRAGScheduler(model_path, rag_path)
                    schedulers["WASS-RAG"] = rag_scheduler
                    print("âš ï¸  WASS-RAGè°ƒåº¦å™¨å·²åˆ›å»º (çŸ¥è¯†åº“ä¸ºç©º)")
                    
            except Exception as e:
                print(f"âŒ DRL/RAGè°ƒåº¦å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        else:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ¨¡å‹æ–‡ä»¶ï¼Œä»…ä½¿ç”¨åŸºç¡€è°ƒåº¦å™¨")
        
        print(f"ğŸ”§ å·²å¯ç”¨è°ƒåº¦å™¨: {list(schedulers.keys())}")
        return schedulers
    
    def run_single_experiment_with_workflow(self, scheduler_name: str, workflow, workflow_size: int, experiment_id: int) -> WRENCHExperimentResult:
        """ä½¿ç”¨é¢„ç”Ÿæˆçš„å·¥ä½œæµè¿è¡Œå•ä¸ªå®éªŒ"""
        print(f"    ğŸ”¬ è¿è¡Œå®éªŒ: {scheduler_name} (å·¥ä½œæµå¤§å°: {workflow_size})")
        
        start_time = time.time()
        
        try:
            # è·å–è°ƒåº¦å™¨
            scheduler = self.schedulers[scheduler_name]
            
            # æ¨¡æ‹ŸWRENCHå®éªŒæ‰§è¡Œ
            # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œåº”è¯¥è°ƒç”¨çœŸå®çš„WRENCH API
            simulation_result = self._simulate_wrench_execution(scheduler, workflow, workflow_size)
            
            # åˆ›å»ºå®éªŒç»“æœ
            result = WRENCHExperimentResult(
                scheduler_name=scheduler_name,
                workflow_id=f"workflow_{workflow_size}_{experiment_id}",
                task_count=workflow_size,
                dependency_count=int(workflow_size * 0.8),  # å‡è®¾80%çš„ä»»åŠ¡æœ‰ä¾èµ–
                makespan=simulation_result['makespan'],
                cpu_utilization=simulation_result['cpu_utilization'],
                task_execution_times=simulation_result['task_times'],
                scheduling_decisions=simulation_result['decisions'],
                experiment_metadata={
                    'experiment_id': experiment_id,
                    'workflow_size': workflow_size,
                    'execution_time': time.time() - start_time,
                    'timestamp': datetime.now().isoformat()
                }
            )
            
            return result
            
        except Exception as e:
            # è¿”å›å¤±è´¥ç»“æœ
            return WRENCHExperimentResult(
                scheduler_name=scheduler_name,
                workflow_id=f"workflow_{workflow_size}_{experiment_id}",
                task_count=workflow_size,
                dependency_count=0,
                makespan=float('inf'),
                cpu_utilization={},
                task_execution_times={},
                scheduling_decisions=[],
                experiment_metadata={
                    'experiment_id': experiment_id,
                    'workflow_size': workflow_size,
                    'execution_time': time.time() - start_time,
                    'timestamp': datetime.now().isoformat(),
                    'error': str(e)
                }
            )
    
    def _generate_workflow(self, workflow_size: int, repetition: int) -> Dict:
        """ç”Ÿæˆå›ºå®šçš„å·¥ä½œæµï¼ˆåŸºäºéšæœºç§å­ç¡®ä¿å¯é‡ç°ï¼‰"""
        # è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ç›¸åŒå‚æ•°ç”Ÿæˆç›¸åŒå·¥ä½œæµ
        seed = 42 + workflow_size * 100 + repetition
        random.seed(seed)
        np.random.seed(seed)
        
        # ç”Ÿæˆå·¥ä½œæµç»“æ„
        workflow = {
            'tasks': [],
            'dependencies': [],
            'seed': seed
        }
        
        # ç”Ÿæˆä»»åŠ¡
        for i in range(workflow_size):
            task = {
                'id': f"task_{i}",
                'flops': random.uniform(1e9, 10e9),  # 1-10 GFLOPS
                'memory': random.uniform(1, 8),       # 1-8 GB
                'cores': random.randint(1, 4)         # 1-4 cores
            }
            workflow['tasks'].append(task)
        
        # ç”Ÿæˆä¾èµ–å…³ç³»ï¼ˆDAGç»“æ„ï¼‰
        # ç®€å•å®ç°ï¼šæ¯ä¸ªä»»åŠ¡ä¾èµ–äºä¹‹å‰çš„1-3ä¸ªä»»åŠ¡
        for i in range(1, workflow_size):
            num_deps = min(random.randint(1, 3), i)  # æœ€å¤šä¾èµ–å‰é¢çš„3ä¸ªä»»åŠ¡
            deps = random.sample(range(i), num_deps)
            
            for dep in deps:
                workflow['dependencies'].append({
                    'from': f"task_{dep}",
                    'to': f"task_{i}"
                })
        
        return workflow
    
    def _simulate_wrench_execution(self, scheduler, workflow: Dict, workflow_size: int) -> Dict:
        """æ¨¡æ‹ŸWRENCHæ‰§è¡Œï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # æ¨¡æ‹ŸèŠ‚ç‚¹è´Ÿè½½
        node_loads = {node: 0.0 for node in self.compute_nodes}
        
        # æ¨¡æ‹Ÿä»»åŠ¡æ‰§è¡Œ
        task_times = {}
        decisions = []
        
        # æŒ‰æ‹“æ‰‘é¡ºåºæ‰§è¡Œä»»åŠ¡ï¼ˆç®€åŒ–å¤„ç†ï¼‰
        task_order = list(range(workflow_size))
        
        # éšæœºæ‰“ä¹±ä»»åŠ¡é¡ºåºï¼ˆæ¨¡æ‹ŸçœŸå®è°ƒåº¦ï¼‰
        random.shuffle(task_order)
        
        total_makespan = 0.0
        
        for task_id in task_order:
            task = workflow['tasks'][task_id]
            
            # è·å–å¯ç”¨èŠ‚ç‚¹
            available_nodes = list(self.compute_nodes)
            
            # ä½¿ç”¨è°ƒåº¦å™¨é€‰æ‹©èŠ‚ç‚¹
            try:
                # åˆ›å»ºæ¨¡æ‹Ÿä»»åŠ¡å¯¹è±¡
                class MockTask:
                    def __init__(self, flops, memory, cores):
                        self._flops = flops
                        self._memory = memory
                        self._cores = cores
                    
                    def get_flops(self):
                        return self._flops
                    
                    def get_memory_requirement(self):
                        return self._memory * 1024 * 1024 * 1024  # è½¬æ¢ä¸ºå­—èŠ‚
                    
                    def get_min_num_cores(self):
                        return self._cores
                    
                    def get_input_files(self):
                        return []  # ç®€åŒ–å¤„ç†
                    
                    def get_output_files(self):
                        return []  # ç®€åŒ–å¤„ç†
                
                mock_task = MockTask(task['flops'], task['memory'], task['cores'])
                
                # è°ƒç”¨è°ƒåº¦å™¨
                chosen_node = scheduler.schedule_task(
                    mock_task, available_nodes, self.node_capacities, node_loads, None
                )
                
                # è®¡ç®—æ‰§è¡Œæ—¶é—´
                capacity = self.node_capacities[chosen_node]
                exec_time = task['flops'] / (capacity * 1e9)
                
                # æ›´æ–°èŠ‚ç‚¹è´Ÿè½½
                node_loads[chosen_node] += exec_time
                
                # è®°å½•ä»»åŠ¡æ‰§è¡Œæ—¶é—´
                task_times[f"task_{task_id}"] = exec_time
                
                # è®°å½•è°ƒåº¦å†³ç­–
                decisions.append({
                    'task_id': f"task_{task_id}",
                    'chosen_node': chosen_node,
                    'execution_time': exec_time,
                    'start_time': node_loads[chosen_node] - exec_time,
                    'end_time': node_loads[chosen_node]
                })
                
                # æ›´æ–°æ€»makespan
                total_makespan = max(total_makespan, node_loads[chosen_node])
                
            except Exception as e:
                print(f"      âš ï¸ ä»»åŠ¡è°ƒåº¦å¤±è´¥: {e}")
                # ä½¿ç”¨é»˜è®¤èŠ‚ç‚¹
                chosen_node = self.compute_nodes[0]
                exec_time = task['flops'] / (self.node_capacities[chosen_node] * 1e9)
                node_loads[chosen_node] += exec_time
                task_times[f"task_{task_id}"] = exec_time
                total_makespan = max(total_makespan, node_loads[chosen_node])
        
        # è®¡ç®—CPUåˆ©ç”¨ç‡
        cpu_utilization = {}
        for node in self.compute_nodes:
            if total_makespan > 0:
                utilization = node_loads[node] / total_makespan
                cpu_utilization[node] = min(utilization, 1.0)
            else:
                cpu_utilization[node] = 0.0
        
        return {
            'makespan': total_makespan,
            'cpu_utilization': cpu_utilization,
            'task_times': task_times,
            'decisions': decisions
        }
    
    def run_single_experiment(self, scheduler_name: str, workflow_size: int, experiment_id: int) -> WRENCHExperimentResult:
        """è¿è¡Œå•æ¬¡WRENCHå®éªŒï¼ˆä½¿ç”¨é¢„ç”Ÿæˆçš„å·¥ä½œæµï¼‰"""
        print(f"  è¿è¡Œå®éªŒ: {scheduler_name}, {workflow_size}ä»»åŠ¡, å®éªŒ#{experiment_id}")
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰é¢„ç”Ÿæˆçš„å·¥ä½œæµ
        workflow_key = f"{workflow_size}_{experiment_id}"
        if workflow_key in self.workflow_cache:
            workflow = self.workflow_cache[workflow_key]
            print(f"    ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„å·¥ä½œæµ: {workflow_key}")
        else:
            # ç”Ÿæˆæ–°çš„å·¥ä½œæµå¹¶ç¼“å­˜
            workflow = self._generate_workflow(workflow_size, experiment_id)
            self.workflow_cache[workflow_key] = workflow
            print(f"    ğŸ“‹ ç”Ÿæˆå¹¶ç¼“å­˜æ–°å·¥ä½œæµ: {workflow_key}")
        
        # ä½¿ç”¨é¢„ç”Ÿæˆçš„å·¥ä½œæµè¿è¡Œå®éªŒ
        return self.run_single_experiment_with_workflow(scheduler_name, workflow, workflow_size, experiment_id)
        
    def run_all_experiments(self):
        """è¿è¡Œæ‰€æœ‰å®éªŒé…ç½®ï¼ˆå…¬å¹³å®éªŒè®¾è®¡ï¼‰"""
        print(f"ğŸ”¬ å¼€å§‹å®Œæ•´WRENCHå®éªŒ...")
        print(f"è°ƒåº¦å™¨: {list(self.schedulers.keys())}")
        print(f"å·¥ä½œæµè§„æ¨¡: {self.workflow_sizes}")
        print(f"é‡å¤æ¬¡æ•°: {self.repetitions}")
        
        total_experiments = len(self.schedulers) * len(self.workflow_sizes) * self.repetitions
        print(f"æ€»å®éªŒæ•°: {total_experiments} = {len(self.schedulers)}è°ƒåº¦å™¨ Ã— {len(self.workflow_sizes)}ä»»åŠ¡è§„æ¨¡ Ã— {self.repetitions}æ¬¡é‡å¤")
        
        # å…¬å¹³å®éªŒè®¾è®¡ï¼šé¢„ç”Ÿæˆå·¥ä½œæµï¼Œç¡®ä¿æ‰€æœ‰è°ƒåº¦å™¨åœ¨ç›¸åŒå·¥ä½œæµä¸Šæµ‹è¯•
        print("\nğŸ“ é¢„ç”Ÿæˆå·¥ä½œæµï¼ˆç¡®ä¿å…¬å¹³æ¯”è¾ƒï¼‰...")
        workflow_cache = {}
        
        for workflow_size in self.workflow_sizes:
            for rep in range(self.repetitions):
                # ä¸ºæ¯ä¸ªå·¥ä½œæµå¤§å°å’Œé‡å¤æ¬¡æ•°ç”Ÿæˆå›ºå®šçš„å·¥ä½œæµ
                workflow_key = (workflow_size, rep)
                print(f"   ç”Ÿæˆå·¥ä½œæµ: {workflow_size}ä¸ªä»»åŠ¡, é‡å¤{rep+1}")
                
                try:
                    # ç”Ÿæˆå·¥ä½œæµå¹¶ç¼“å­˜
                    workflow = self._generate_workflow(workflow_size, rep)
                    workflow_cache[workflow_key] = workflow
                    print(f"   âœ… å·¥ä½œæµç”ŸæˆæˆåŠŸ")
                except Exception as e:
                    print(f"   âŒ å·¥ä½œæµç”Ÿæˆå¤±è´¥: {e}")
                    workflow_cache[workflow_key] = None
        
        print("\nğŸš€ å¼€å§‹è¿è¡Œå®éªŒ...")
        current_exp = 0
        
        # æŒ‰å·¥ä½œæµå¤§å°å’Œé‡å¤æ¬¡æ•°åˆ†ç»„ï¼Œç¡®ä¿å…¬å¹³æ€§
        for workflow_size in self.workflow_sizes:
            for rep in range(self.repetitions):
                # è·å–é¢„ç”Ÿæˆçš„å·¥ä½œæµ
                workflow_key = (workflow_size, rep)
                workflow = workflow_cache[workflow_key]
                
                if workflow is None:
                    print(f"   âš ï¸ è·³è¿‡æ— æ•ˆå·¥ä½œæµ: {workflow_key}")
                    continue
                
                # å¯¹åŒä¸€å·¥ä½œæµæµ‹è¯•æ‰€æœ‰è°ƒåº¦å™¨
                for scheduler_name in self.schedulers.keys():
                    current_exp += 1
                    print(f"\nè¿›åº¦: {current_exp}/{total_experiments}")
                    print(f"   å·¥ä½œæµ: {workflow_size}ä¸ªä»»åŠ¡, é‡å¤{rep+1}")
                    print(f"   è°ƒåº¦å™¨: {scheduler_name}")
                    
                    try:
                        # ä½¿ç”¨é¢„ç”Ÿæˆçš„å·¥ä½œæµè¿è¡Œå®éªŒ
                        result = self.run_single_experiment_with_workflow(
                            scheduler_name, workflow, workflow_size, current_exp
                        )
                        self.results.append(result)
                        print(f"   âœ… å®Œæˆ: {result.makespan:.2f}s")
                    except Exception as e:
                        print(f"   âŒ å®éªŒå¤±è´¥: {e}")
        
        # ä¿å­˜ç»“æœ
        self._save_results()
        self._analyze_results()
        
        print(f"\nğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆï¼å…±è¿è¡Œ {len(self.results)} æ¬¡å®éªŒ")
        return self.results
    
    def _save_results(self):
        """ä¿å­˜å®éªŒç»“æœ"""
        results_dir = Path("results/wrench_experiments")
        results_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_data = {
            "experiment_config": {
                "schedulers": list(self.schedulers.keys()),
                "workflow_sizes": self.workflow_sizes,
                "repetitions": self.repetitions,
                "total_experiments": len(self.results)
            },
            "results": [asdict(result) for result in self.results]
        }
        
        with open(results_dir / "detailed_results.json", 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š å®éªŒç»“æœå·²ä¿å­˜åˆ° {results_dir}")
    
    def _analyze_results(self):
        """åˆ†æå®éªŒç»“æœ"""
        if not self.results:
            print("âŒ æ²¡æœ‰å®éªŒç»“æœå¯åˆ†æ")
            return
        
        print(f"\nğŸ“ˆ å®éªŒç»“æœåˆ†æ:")
        print("=" * 60)
        
        # æŒ‰è°ƒåº¦å™¨åˆ†ç»„ç»Ÿè®¡
        scheduler_stats = {}
        for result in self.results:
            name = result.scheduler_name
            if name not in scheduler_stats:
                scheduler_stats[name] = []
            scheduler_stats[name].append(result.makespan)
        
        # æ˜¾ç¤ºç»Ÿè®¡ç»“æœ
        print(f"{'è°ƒåº¦å™¨':<15} {'å¹³å‡Makespan':<15} {'æ ‡å‡†å·®':<10} {'æœ€ä½³':<10} {'å®éªŒæ¬¡æ•°':<8}")
        print("-" * 60)
        
        for scheduler_name, makespans in scheduler_stats.items():
            avg_makespan = np.mean(makespans)
            std_makespan = np.std(makespans)
            best_makespan = min(makespans)
            count = len(makespans)
            
            print(f"{scheduler_name:<15} {avg_makespan:<15.2f} {std_makespan:<10.2f} {best_makespan:<10.2f} {count:<8}")
        
        # æ‰¾å‡ºæœ€ä½³è°ƒåº¦å™¨
        best_scheduler = min(scheduler_stats.keys(), 
                           key=lambda x: np.mean(scheduler_stats[x]))
        best_avg = np.mean(scheduler_stats[best_scheduler])
        
        print(f"\nğŸ† æœ€ä½³è°ƒåº¦å™¨: {best_scheduler} (å¹³å‡Makespan: {best_avg:.2f}s)")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹åŸºäºWRENCHçš„çœŸå®WASS-RAGå®éªŒ...")
    
    runner = WRENCHExperimentRunner()
    runner.run_all_experiments()
    
    print("\nğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆ!")

if __name__ == "__main__":
    main()
