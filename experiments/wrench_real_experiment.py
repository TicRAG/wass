#!/usr/bin/env python3
"""
åŸºäºWRENCHçš„çœŸå®WASS-RAGå®éªŒæ¡†æ¶
ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨çœŸå®WRENCHç¯å¢ƒä¸­è¿›è¡Œæ€§èƒ½å¯¹æ¯”å®éªŒ
"""

import sys
import os
import json
import time
import random
import numpy as np
import torch
import pickle
from pathlib import Path
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Tuple
import yaml

# ç¡®ä¿èƒ½å¯¼å…¥WRENCH
try:
    import wrench
except ImportError:
    print("Error: WRENCH not available. Please install wrench-python-api.")
    sys.exit(1)

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, str(parent_dir))

def load_config(cfg_path: str) -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(cfg_path, 'r', encoding='utf-8') as f:
        cfg = yaml.safe_load(f) or {}
    
    # Process includes
    if 'include' in cfg:
        base_dir = os.path.dirname(cfg_path)
        for include_file in cfg['include']:
            include_path = os.path.join(base_dir, include_file)
            if os.path.exists(include_path):
                with open(include_path, 'r', encoding='utf-8') as f:
                    include_cfg = yaml.safe_load(f) or {}
                    for key, value in include_cfg.items():
                        if key not in cfg:
                            cfg[key] = value
    return cfg

@dataclass
class WRENCHExperimentResult:
    """å•æ¬¡WRENCHå®éªŒç»“æœ"""
    scheduler_name: str
    workflow_id: str
    task_count: int
    dependency_count: int
    makespan: float
    cpu_utilization: Dict[str, float]
    task_execution_times: Dict[str, float]
    scheduling_decisions: List[Dict[str, Any]]
    experiment_metadata: Dict[str, Any]

class WRENCHScheduler:
    """åŸºç¡€WRENCHè°ƒåº¦å™¨æ¥å£"""
    
    def __init__(self, name: str):
        self.name = name
    
    def schedule_task(self, task, available_nodes: List[str], node_capacities: Dict, 
                     node_loads: Dict, compute_service) -> str:
        """è°ƒåº¦å•ä¸ªä»»åŠ¡ï¼Œè¿”å›é€‰æ‹©çš„èŠ‚ç‚¹"""
        raise NotImplementedError

class FIFOScheduler(WRENCHScheduler):
    """å…ˆè¿›å…ˆå‡ºè°ƒåº¦å™¨"""
    
    def __init__(self):
        super().__init__("FIFO")
    
    def schedule_task(self, task, available_nodes, node_capacities, node_loads, compute_service):
        # é€‰æ‹©è´Ÿè½½æœ€å°çš„èŠ‚ç‚¹
        return min(available_nodes, key=lambda x: node_loads.get(x, 0))

class HEFTScheduler(WRENCHScheduler):
    """å¼‚æ„æœ€æ—©å®Œæˆæ—¶é—´è°ƒåº¦å™¨"""
    
    def __init__(self):
        super().__init__("HEFT")
    
    def schedule_task(self, task, available_nodes, node_capacities, node_loads, compute_service):
        # é€‰æ‹©èƒ½æœ€æ—©å®Œæˆä»»åŠ¡çš„èŠ‚ç‚¹
        best_node = None
        best_finish_time = float('inf')
        
        for node in available_nodes:
            capacity = node_capacities.get(node, 1.0)
            load = node_loads.get(node, 0.0)
            exec_time = task.get_flops() / (capacity * 1e9)
            finish_time = load + exec_time
            
            if finish_time < best_finish_time:
                best_finish_time = finish_time
                best_node = node
        
        return best_node or available_nodes[0]

class WASSHeuristicScheduler(WRENCHScheduler):
    """WASSå¯å‘å¼è°ƒåº¦å™¨ - åœ¨HEFTåŸºç¡€ä¸Šè€ƒè™‘æ•°æ®å±€éƒ¨æ€§"""
    
    def __init__(self, data_locality_weight: float = 0.5):
        super().__init__("WASS-Heuristic")
        self.data_locality_weight = data_locality_weight  # wå‚æ•°
        self.data_location_cache = {}  # æ¨¡æ‹Ÿæ•°æ®ä½ç½®ç¼“å­˜
        
        # èŠ‚ç‚¹æ€§èƒ½å‚æ•°
        self.node_capacities = {
            "ComputeHost1": 2.0,
            "ComputeHost2": 3.0,
            "ComputeHost3": 2.5,
            "ComputeHost4": 4.0
        }
    
    def schedule_task(self, task, available_nodes, node_capacities, node_loads, compute_service):
        """ä½¿ç”¨WASSå¯å‘å¼è¿›è¡Œä»»åŠ¡è°ƒåº¦"""
        best_node = None
        best_score = float('inf')
        
        for node in available_nodes:
            # è®¡ç®—EFT (æœ€æ—©å®Œæˆæ—¶é—´)
            eft = self._calculate_eft(task, node, node_capacities, node_loads)
            
            # è®¡ç®—DRT (æ•°æ®å°±ç»ªæ—¶é—´)
            drt = self._calculate_drt(task, node)
            
            # è®¡ç®—WASSç»¼åˆè¯„åˆ†
            w = self.data_locality_weight
            score = (1 - w) * eft + w * drt
            
            if score < best_score:
                best_score = score
                best_node = node
        
        # æ›´æ–°æ•°æ®ä½ç½®ç¼“å­˜ï¼ˆå‡è®¾ä»»åŠ¡è¾“å‡ºæ•°æ®å­˜å‚¨åœ¨æ‰§è¡ŒèŠ‚ç‚¹ï¼‰
        if best_node:
            self._update_data_location(task, best_node)
        
        return best_node or available_nodes[0]
    
    def _calculate_eft(self, task, node, node_capacities, node_loads):
        """è®¡ç®—ä»»åŠ¡åœ¨æŒ‡å®šèŠ‚ç‚¹ä¸Šçš„æœ€æ—©å®Œæˆæ—¶é—´"""
        capacity = node_capacities.get(node, 1.0)
        load = node_loads.get(node, 0.0)
        exec_time = task.get_flops() / (capacity * 1e9)
        return load + exec_time
    
    def _calculate_drt(self, task, node):
        """è®¡ç®—æ•°æ®å°±ç»ªæ—¶é—´ - è€ƒè™‘æ•°æ®ä¼ è¾“å¼€é”€"""
        total_transfer_time = 0.0
        
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶çš„æ•°æ®ä½ç½®
        for input_file in task.get_input_files():
            # ä½¿ç”¨æ–‡ä»¶åè€Œä¸æ˜¯get_id()æ–¹æ³•
            file_id = input_file.get_name() if hasattr(input_file, 'get_name') else str(input_file)
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦åœ¨ç›®æ ‡èŠ‚ç‚¹ä¸Š
            data_location = self._get_data_location(file_id)
            if data_location != node:
                # éœ€è¦ä¼ è¾“æ•°æ®
                file_size = input_file.get_size() if hasattr(input_file, 'get_size') else 1024
                network_bandwidth = 1e9  # 1GB/s å‡è®¾ç½‘ç»œå¸¦å®½
                transfer_time = file_size / network_bandwidth
                total_transfer_time += transfer_time
        
        return total_transfer_time
    
    def _get_data_location(self, file_id):
        """è·å–æ–‡ä»¶çš„æ•°æ®ä½ç½®"""
        if file_id not in self.data_location_cache:
            # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œéšæœºé€‰æ‹©ä¸€ä¸ªä½ç½®ï¼ˆæ¨¡æ‹Ÿåˆå§‹æ•°æ®åˆ†å¸ƒï¼‰
            import random
            self.data_location_cache[file_id] = random.choice(
                ["ComputeHost1", "ComputeHost2", "ComputeHost3", "ComputeHost4"]
            )
        return self.data_location_cache[file_id]
    
    def _update_data_location(self, task, node):
        """æ›´æ–°ä»»åŠ¡è¾“å‡ºæ•°æ®çš„ä½ç½®"""
        for output_file in task.get_output_files():
            # ä½¿ç”¨æ–‡ä»¶åè€Œä¸æ˜¯get_id()æ–¹æ³•
            file_id = output_file.get_name() if hasattr(output_file, 'get_name') else str(output_file)
            self.data_location_cache[file_id] = node

class WASSDRLScheduler(WRENCHScheduler):
    """åŸºäºè®­ç»ƒå¥½çš„DRLæ¨¡å‹çš„è°ƒåº¦å™¨"""
    
    def __init__(self, model_path: str):
        super().__init__("WASS-DRL")
        self.model = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self._load_model(model_path)
        
        # èŠ‚ç‚¹æ˜ å°„
        self.compute_nodes = ["ComputeHost1", "ComputeHost2", "ComputeHost3", "ComputeHost4"]
        self.node_capacities = {
            "ComputeHost1": 2.0,
            "ComputeHost2": 3.0, 
            "ComputeHost3": 2.5,
            "ComputeHost4": 4.0
        }
    
    def _load_model(self, model_path: str):
        """åŠ è½½è®­ç»ƒå¥½çš„DRLæ¨¡å‹"""
        try:
            checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
            
            if "drl_agent" in checkpoint:
                # ç›´æ¥å®šä¹‰ç½‘ç»œç±»ï¼Œé¿å…å¯¼å…¥é—®é¢˜
                import torch.nn as nn
                
                class SimpleDQN(nn.Module):
                    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):
                        super().__init__()
                        self.network = nn.Sequential(
                            nn.Linear(state_dim, hidden_dim),
                            nn.ReLU(),
                            nn.Linear(hidden_dim, hidden_dim),
                            nn.ReLU(),
                            nn.Linear(hidden_dim, action_dim)
                        )
                    
                    def forward(self, x):
                        return self.network(x)
                
                state_dim = 17  # ä¸è®­ç»ƒæ—¶ä¸€è‡´
                action_dim = 4   # 4ä¸ªèŠ‚ç‚¹
                self.model = SimpleDQN(state_dim, action_dim).to(self.device)
                self.model.load_state_dict(checkpoint["drl_agent"])
                self.model.eval()
                print(f"âœ… DRLæ¨¡å‹å·²åŠ è½½: {model_path}")
            else:
                print(f"âŒ æœªæ‰¾åˆ°DRLæ¨¡å‹å‚æ•°")
                self.model = None
        except Exception as e:
            print(f"âŒ DRLæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.model = None
    
    def _get_state(self, task, available_nodes, node_capacities, node_loads):
        """æ„é€ çŠ¶æ€å‘é‡"""
        # ä»»åŠ¡ç‰¹å¾
        task_features = [
            task.get_flops() / 1e9,  # æ ‡å‡†åŒ–åˆ°GFlops
            len(task.get_input_files()),
            task.get_number_of_children(),
            4,  # æ€»èŠ‚ç‚¹æ•°
            0.5  # å‡è®¾å®Œæˆè¿›åº¦
        ]
        
        # èŠ‚ç‚¹ç‰¹å¾
        node_features = []
        for node in self.compute_nodes:
            if node in available_nodes:
                capacity = node_capacities.get(node, 0.0)
                load = node_loads.get(node, 0.0)
                availability = max(0.0, capacity - load)
                
                node_features.extend([
                    capacity / 4.0,      # æ ‡å‡†åŒ–å®¹é‡
                    load / 4.0,          # æ ‡å‡†åŒ–è´Ÿè½½
                    availability / 4.0   # æ ‡å‡†åŒ–å¯ç”¨æ€§
                ])
            else:
                node_features.extend([0.0, 0.0, 0.0])
        
        state = np.array(task_features + node_features, dtype=np.float32)
        return state
    
    def schedule_task(self, task, available_nodes, node_capacities, node_loads, compute_service):
        if self.model is None:
            # æ¨¡å‹æœªåŠ è½½ï¼ŒæŠ›å‡ºå¼‚å¸¸
            raise RuntimeError("DRLæ¨¡å‹æœªæ­£ç¡®åŠ è½½ï¼Œæ— æ³•è¿›è¡Œè°ƒåº¦")
        
        try:
            state = self._get_state(task, available_nodes, node_capacities, node_loads)
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                q_values = self.model(state_tensor)
                action = q_values.argmax().item()
            
            # æ˜ å°„åŠ¨ä½œåˆ°èŠ‚ç‚¹
            if action < len(self.compute_nodes):
                chosen_node = self.compute_nodes[action]
                if chosen_node in available_nodes:
                    return chosen_node
            
            # å¦‚æœé€‰æ‹©çš„èŠ‚ç‚¹ä¸å¯ç”¨ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨èŠ‚ç‚¹
            return available_nodes[0]
            
        except Exception as e:
            raise RuntimeError(f"DRLè°ƒåº¦å¤±è´¥: {e}")

class WASSRAGScheduler(WRENCHScheduler):
    """åŸºäºRAGçŸ¥è¯†åº“å¢å¼ºçš„è°ƒåº¦å™¨"""
    
    def __init__(self, model_path: str, rag_path: str):
        super().__init__("WASS-RAG")
        self.drl_scheduler = WASSDRLScheduler(model_path)
        self.knowledge_base = None
        self._load_rag_knowledge(rag_path)
    
    def _load_rag_knowledge(self, rag_path: str):
        """åŠ è½½RAGçŸ¥è¯†åº“"""
        # ä¼˜å…ˆä½¿ç”¨æ‰©å±•çš„JSONçŸ¥è¯†åº“
        extended_json_path = "data/extended_rag_knowledge.json"
        if os.path.exists(extended_json_path):
            try:
                with open(extended_json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # ä»JSONæ•°æ®æ„å»ºçŸ¥è¯†åº“
                self.knowledge_base = []
                if 'cases' in data:
                    for case in data['cases']:
                        simple_case = {
                            'task_flops': case.get('task_flops', case.get('task_execution_time', 1.0) * 2e9),
                            'chosen_node': case.get('chosen_node', 'ComputeHost1'),
                            'scheduler_type': case.get('scheduler_type', 'unknown'),
                            'task_execution_time': case.get('task_execution_time', 0.0),
                            'workflow_makespan': case.get('workflow_makespan', 0.0),
                            'node_capacity': case.get('node_capacity', 2.0),
                            'performance_ratio': case.get('performance_ratio', 1.0)
                        }
                        self.knowledge_base.append(simple_case)
                
                print(f"âœ… RAGçŸ¥è¯†åº“å·²ä»æ‰©å±•JSONåŠ è½½: {len(self.knowledge_base)} ä¸ªæ¡ˆä¾‹")
                return
            except Exception as e:
                print(f"æ‰©å±•JSONåŠ è½½å¤±è´¥: {e}")
        
        # å›é€€åˆ°åŸå§‹JSONæ ¼å¼
        json_path = rag_path.replace('.pkl', '.json')
        if os.path.exists(json_path):
            try:
                with open(json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # ä»JSONæ•°æ®æ„å»ºçŸ¥è¯†åº“
                self.knowledge_base = []
                if 'sample_cases' in data:
                    for case in data['sample_cases']:
                        # ä¼°ç®—task_flopsï¼ˆåŸºäºæ‰§è¡Œæ—¶é—´å’ŒèŠ‚ç‚¹æ€§èƒ½ï¼‰
                        exec_time = case.get('task_execution_time', 1.0)
                        node = case.get('chosen_node', 'ComputeHost4')
                        node_capacity = self.get_node_capacity(node)
                        estimated_flops = exec_time * node_capacity * 1e9
                        
                        simple_case = {
                            'task_flops': estimated_flops,
                            'chosen_node': case.get('chosen_node', 'ComputeHost1'),
                            'scheduler_type': case.get('scheduler_type', 'unknown'),
                            'task_execution_time': case.get('task_execution_time', 0.0),
                            'workflow_makespan': case.get('workflow_makespan', 0.0)
                        }
                        self.knowledge_base.append(simple_case)
                
                print(f"âœ… RAGçŸ¥è¯†åº“å·²ä»åŸå§‹JSONåŠ è½½: {len(self.knowledge_base)} ä¸ªæ¡ˆä¾‹")
                return
            except Exception as e:
                print(f"åŸå§‹JSONåŠ è½½å¤±è´¥: {e}")
        
        # æœ€åå›é€€åˆ°pickleæ ¼å¼
        try:
            with open(rag_path, 'rb') as f:
                data = pickle.load(f)
                # æå–æ¡ˆä¾‹æ•°æ®ï¼Œå¿½ç•¥ç±»å‹é—®é¢˜
                if 'cases' in data and data['cases']:
                    # è½¬æ¢ä¸ºç®€åŒ–æ ¼å¼
                    self.knowledge_base = []
                    for case in data['cases']:
                        # å‡è®¾caseæœ‰è¿™äº›å±æ€§ï¼Œç”¨å­—å…¸æ ¼å¼å­˜å‚¨
                        if hasattr(case, 'task_flops') and hasattr(case, 'chosen_node'):
                            simple_case = {
                                'task_flops': case.task_flops,
                                'chosen_node': case.chosen_node,
                                'scheduler_type': getattr(case, 'scheduler_type', 'unknown'),
                                'task_execution_time': getattr(case, 'task_execution_time', 0.0)
                            }
                            self.knowledge_base.append(simple_case)
                    print(f"âœ… RAGçŸ¥è¯†åº“å·²ä»PKLåŠ è½½: {len(self.knowledge_base)} ä¸ªæ¡ˆä¾‹")
                else:
                    print(f"âŒ RAGçŸ¥è¯†åº“æ ¼å¼é”™è¯¯")
                    self.knowledge_base = []
        except Exception as e:
            print(f"âŒ RAGçŸ¥è¯†åº“åŠ è½½å¤±è´¥: {e}")
            self.knowledge_base = []
    
    def _retrieve_similar_cases(self, task, k=3):
        """æ£€ç´¢ç›¸ä¼¼æ¡ˆä¾‹"""
        if not self.knowledge_base:
            return []
        
        # ç®€åŒ–çš„ç›¸ä¼¼åº¦è®¡ç®—
        task_flops = task.get_flops()
        similar_cases = []
        
        for case in self.knowledge_base:
            # åŸºäºä»»åŠ¡è®¡ç®—é‡çš„ç›¸ä¼¼åº¦
            case_flops = case.get('task_flops', 0)
            if case_flops > 0:
                flops_diff = abs(case_flops - task_flops) / max(case_flops, task_flops)
                similarity = 1.0 - flops_diff
                similar_cases.append((case, similarity))
        
        # è¿”å›æœ€ç›¸ä¼¼çš„kä¸ªæ¡ˆä¾‹
        similar_cases.sort(key=lambda x: x[1], reverse=True)
        return similar_cases[:k]
    
    def schedule_task(self, task, available_nodes, node_capacities, node_loads, compute_service):
        # æ£€æŸ¥DRLè°ƒåº¦å™¨æ˜¯å¦å¯ç”¨
        if self.drl_scheduler.model is None:
            raise RuntimeError("DRLæ¨¡å‹æœªåŠ è½½ï¼ŒWASS-RAGæ— æ³•è¿è¡Œ")
        
        # æ£€æŸ¥RAGçŸ¥è¯†åº“æ˜¯å¦å¯ç”¨
        if not self.knowledge_base:
            raise RuntimeError("RAGçŸ¥è¯†åº“æœªåŠ è½½ï¼ŒWASS-RAGæ— æ³•è¿è¡Œ")
        
        # é¦–å…ˆä½¿ç”¨DRLè·å¾—åŸºç¡€å†³ç­–
        drl_choice = self.drl_scheduler.schedule_task(task, available_nodes, node_capacities, node_loads, compute_service)
        
        # ä½¿ç”¨RAGçŸ¥è¯†å¢å¼ºå†³ç­–
        similar_cases = self._retrieve_similar_cases(task)
        
        if similar_cases:
            # åˆ†æç›¸ä¼¼æ¡ˆä¾‹çš„è°ƒåº¦å†³ç­–
            node_votes = {}
            for case, similarity in similar_cases:
                node = case.get('chosen_node', '')
                if node in available_nodes:
                    node_votes[node] = node_votes.get(node, 0) + similarity
            
            if node_votes:
                # é€‰æ‹©æŠ•ç¥¨æœ€é«˜çš„èŠ‚ç‚¹
                rag_choice = max(node_votes.keys(), key=lambda x: node_votes[x])
                
                # ç»“åˆDRLå’ŒRAGçš„å†³ç­–ï¼ˆåå‘RAGæ¨èï¼‰
                if rag_choice in available_nodes and random.random() < 0.7:
                    return rag_choice
        
        return drl_choice

class WRENCHExperimentRunner:
    """åŸºäºçœŸå®WRENCHçš„å®éªŒè¿è¡Œå™¨"""
    
    def __init__(self, config_path: str = "configs/experiment.yaml"):
        self.config = load_config(config_path)
        
        # WRENCHå¹³å°é…ç½®
        self.platform_file = self.config['platform']['platform_file']
        self.controller_host = "ControllerHost"
        
        # èŠ‚ç‚¹é…ç½®
        self.compute_nodes = ["ComputeHost1", "ComputeHost2", "ComputeHost3", "ComputeHost4"]
        self.node_capacities = {
            "ComputeHost1": 2.0,
            "ComputeHost2": 3.0,
            "ComputeHost3": 2.5,
            "ComputeHost4": 4.0
        }
        
        # è°ƒåº¦å™¨é…ç½®
        self.schedulers = self._initialize_schedulers()
        
        # å®éªŒå‚æ•°
        self.workflow_sizes = [5, 10, 15, 20]
        self.repetitions = 3
        
        # ç»“æœå­˜å‚¨
        self.results = []
        
        print(f"ğŸš€ WRENCHå®éªŒè¿è¡Œå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_schedulers(self):
        """åˆå§‹åŒ–æ‰€æœ‰è°ƒåº¦å™¨"""
        schedulers = {
            "FIFO": FIFOScheduler(),
            "HEFT": HEFTScheduler(),
            "WASS-Heuristic": WASSHeuristicScheduler(),  # æ–°å¢WASSå¯å‘å¼è°ƒåº¦å™¨
        }
        
        # æ£€æŸ¥è®­ç»ƒå¥½çš„æ¨¡å‹
        model_path = "models/wass_models.pth"
        rag_path = "data/wrench_rag_knowledge_base.pkl"
        
        if os.path.exists(model_path):
            # å°è¯•åŠ è½½DRLè°ƒåº¦å™¨
            try:
                drl_scheduler = WASSDRLScheduler(model_path)
                if drl_scheduler.model is not None:
                    schedulers["WASS-DRL"] = drl_scheduler
                    print("âœ… WASS-DRLè°ƒåº¦å™¨å·²å¯ç”¨")
                    
                    # åªæœ‰åœ¨DRLæˆåŠŸåŠ è½½åæ‰å°è¯•RAG
                    if os.path.exists(rag_path):
                        try:
                            rag_scheduler = WASSRAGScheduler(model_path, rag_path)
                            if rag_scheduler.knowledge_base:
                                schedulers["WASS-RAG"] = rag_scheduler
                                print("âœ… WASS-RAGè°ƒåº¦å™¨å·²å¯ç”¨")
                            else:
                                print("âš ï¸  RAGçŸ¥è¯†åº“ä¸ºç©ºï¼Œè·³è¿‡WASS-RAG")
                        except Exception as e:
                            print(f"âš ï¸  WASS-RAGåˆå§‹åŒ–å¤±è´¥: {e}")
                    else:
                        print(f"âš ï¸  RAGçŸ¥è¯†åº“æ–‡ä»¶æœªæ‰¾åˆ°: {rag_path}")
                else:
                    print("âš ï¸  DRLæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè·³è¿‡WASS-DRLå’ŒWASS-RAG")
            except Exception as e:
                print(f"âš ï¸  DRLè°ƒåº¦å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        else:
            print(f"âš ï¸  è®­ç»ƒæ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {model_path}")
        
        print(f"ğŸ”§ å·²å¯ç”¨è°ƒåº¦å™¨: {list(schedulers.keys())}")
        return schedulers
    
    def run_single_experiment(self, scheduler_name: str, workflow_size: int, experiment_id: int) -> WRENCHExperimentResult:
        """è¿è¡Œå•æ¬¡WRENCHå®éªŒ"""
        print(f"  è¿è¡Œå®éªŒ: {scheduler_name}, {workflow_size}ä»»åŠ¡, å®éªŒ#{experiment_id}")
        
        with open(self.platform_file, 'r', encoding='utf-8') as f:
            platform_xml = f.read()
        
        # åˆ›å»ºä»¿çœŸ
        sim = wrench.Simulation()
        sim.start(platform_xml, self.controller_host)
        
        try:
            # åˆ›å»ºæœåŠ¡
            storage_service = sim.create_simple_storage_service("StorageHost", ["/storage"])
            
            compute_resources = {}
            for node in self.compute_nodes:
                compute_resources[node] = (4, 8_589_934_592)  # 4æ ¸, 8GBå†…å­˜
            
            compute_service = sim.create_bare_metal_compute_service(
                "ComputeHost1", compute_resources, "/scratch", {}, {}
            )
            
            # åˆ›å»ºå·¥ä½œæµ
            workflow = sim.create_workflow()
            tasks = []
            files = []
            
            # åˆ›å»ºä»»åŠ¡
            for i in range(workflow_size):
                flops = random.uniform(2e9, 10e9)
                task = workflow.add_task(f"task_{experiment_id}_{i}", flops, 1, 1, 0)
                tasks.append(task)
                
                # åˆ›å»ºè¾“å‡ºæ–‡ä»¶
                if i < workflow_size - 1:
                    output_file = sim.add_file(f"output_{experiment_id}_{i}", random.randint(1024, 10240))
                    task.add_output_file(output_file)
                    files.append(output_file)
            
            # åˆ›å»ºä¾èµ–å…³ç³»
            dependency_count = 0
            for i in range(1, min(workflow_size, len(files) + 1)):
                if i > 1 and random.random() < 0.3:  # 30%æ¦‚ç‡æœ‰ä¾èµ–
                    dep_idx = random.randint(0, i-2)
                    if dep_idx < len(files):
                        tasks[i].add_input_file(files[dep_idx])
                        dependency_count += 1
            
            # ä¸ºæ–‡ä»¶åˆ›å»ºå‰¯æœ¬
            for file in files:
                storage_service.create_file_copy(file)
            
            # è·å–è°ƒåº¦å™¨
            scheduler = self.schedulers[scheduler_name]
            
            # æ‰§è¡Œè°ƒåº¦
            node_loads = {node: 0.0 for node in self.compute_nodes}
            task_execution_times = {}
            scheduling_decisions = []
            
            # æ¨¡æ‹Ÿè°ƒåº¦è¿‡ç¨‹
            ready_tasks = workflow.get_ready_tasks()
            while ready_tasks:
                current_task = ready_tasks[0]
                
                # è°ƒåº¦å†³ç­–
                chosen_node = scheduler.schedule_task(
                    current_task, self.compute_nodes, self.node_capacities, node_loads, compute_service
                )
                
                # è®°å½•è°ƒåº¦å†³ç­–
                scheduling_decisions.append({
                    "task": current_task.get_name(),
                    "node": chosen_node,
                    "scheduler": scheduler_name,
                    "task_flops": current_task.get_flops()
                })
                
                # æäº¤ä½œä¸š
                file_locations = {}
                for f in current_task.get_input_files():
                    file_locations[f] = storage_service
                for f in current_task.get_output_files():
                    file_locations[f] = storage_service
                
                job = sim.create_standard_job([current_task], file_locations)
                compute_service.submit_standard_job(job)
                
                # ç­‰å¾…ä½œä¸šå®Œæˆ
                start_time = sim.get_simulated_time()
                while True:
                    event = sim.wait_for_next_event()
                    if event["event_type"] == "standard_job_completion":
                        completed_job = event["standard_job"]
                        if completed_job == job:
                            break
                    elif event["event_type"] == "simulation_termination":
                        break
                
                end_time = sim.get_simulated_time()
                execution_time = end_time - start_time
                
                # è®°å½•æ‰§è¡Œæ—¶é—´
                task_execution_times[current_task.get_name()] = execution_time
                node_loads[chosen_node] += execution_time
                
                # è·å–ä¸‹ä¸€æ‰¹å°±ç»ªä»»åŠ¡
                ready_tasks = workflow.get_ready_tasks()
            
            # è®¡ç®—æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡
            makespan = sim.get_simulated_time()
            
            # è®¡ç®—CPUåˆ©ç”¨ç‡
            total_work = sum(task_execution_times.values())
            cpu_utilization = {}
            for node in self.compute_nodes:
                node_work = sum(execution_time for task_name, execution_time in task_execution_times.items() 
                               if any(d["task"] == task_name and d["node"] == node for d in scheduling_decisions))
                cpu_utilization[node] = node_work / makespan if makespan > 0 else 0.0
            
            return WRENCHExperimentResult(
                scheduler_name=scheduler_name,
                workflow_id=f"workflow_{experiment_id}",
                task_count=workflow_size,
                dependency_count=dependency_count,
                makespan=makespan,
                cpu_utilization=cpu_utilization,
                task_execution_times=task_execution_times,
                scheduling_decisions=scheduling_decisions,
                experiment_metadata={
                    "experiment_id": experiment_id,
                    "platform": self.platform_file,
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
                }
            )
        
        finally:
            sim.terminate()
    
    def run_all_experiments(self):
        """è¿è¡Œæ‰€æœ‰å®éªŒé…ç½®"""
        print(f"ğŸ”¬ å¼€å§‹å®Œæ•´WRENCHå®éªŒ...")
        print(f"è°ƒåº¦å™¨: {list(self.schedulers.keys())}")
        print(f"å·¥ä½œæµè§„æ¨¡: {self.workflow_sizes}")
        print(f"é‡å¤æ¬¡æ•°: {self.repetitions}")
        
        total_experiments = len(self.schedulers) * len(self.workflow_sizes) * self.repetitions
        current_exp = 0
        
        for scheduler_name in self.schedulers.keys():
            for workflow_size in self.workflow_sizes:
                for rep in range(self.repetitions):
                    current_exp += 1
                    print(f"\nè¿›åº¦: {current_exp}/{total_experiments}")
                    
                    try:
                        result = self.run_single_experiment(scheduler_name, workflow_size, current_exp)
                        self.results.append(result)
                        print(f"  âœ… å®Œæˆ: {result.makespan:.2f}s")
                    except Exception as e:
                        print(f"  âŒ å®éªŒå¤±è´¥: {e}")
        
        # ä¿å­˜ç»“æœ
        self._save_results()
        self._analyze_results()
    
    def _save_results(self):
        """ä¿å­˜å®éªŒç»“æœ"""
        results_dir = Path("results/wrench_experiments")
        results_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_data = {
            "experiment_config": {
                "schedulers": list(self.schedulers.keys()),
                "workflow_sizes": self.workflow_sizes,
                "repetitions": self.repetitions,
                "total_experiments": len(self.results)
            },
            "results": [asdict(result) for result in self.results]
        }
        
        with open(results_dir / "detailed_results.json", 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š å®éªŒç»“æœå·²ä¿å­˜åˆ° {results_dir}")
    
    def _analyze_results(self):
        """åˆ†æå®éªŒç»“æœ"""
        if not self.results:
            print("âŒ æ²¡æœ‰å®éªŒç»“æœå¯åˆ†æ")
            return
        
        print(f"\nğŸ“ˆ å®éªŒç»“æœåˆ†æ:")
        print("=" * 60)
        
        # æŒ‰è°ƒåº¦å™¨åˆ†ç»„ç»Ÿè®¡
        scheduler_stats = {}
        for result in self.results:
            name = result.scheduler_name
            if name not in scheduler_stats:
                scheduler_stats[name] = []
            scheduler_stats[name].append(result.makespan)
        
        # æ˜¾ç¤ºç»Ÿè®¡ç»“æœ
        print(f"{'è°ƒåº¦å™¨':<15} {'å¹³å‡Makespan':<15} {'æ ‡å‡†å·®':<10} {'æœ€ä½³':<10} {'å®éªŒæ¬¡æ•°':<8}")
        print("-" * 60)
        
        for scheduler_name, makespans in scheduler_stats.items():
            avg_makespan = np.mean(makespans)
            std_makespan = np.std(makespans)
            best_makespan = min(makespans)
            count = len(makespans)
            
            print(f"{scheduler_name:<15} {avg_makespan:<15.2f} {std_makespan:<10.2f} {best_makespan:<10.2f} {count:<8}")
        
        # æ‰¾å‡ºæœ€ä½³è°ƒåº¦å™¨
        best_scheduler = min(scheduler_stats.keys(), 
                           key=lambda x: np.mean(scheduler_stats[x]))
        best_avg = np.mean(scheduler_stats[best_scheduler])
        
        print(f"\nğŸ† æœ€ä½³è°ƒåº¦å™¨: {best_scheduler} (å¹³å‡Makespan: {best_avg:.2f}s)")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹åŸºäºWRENCHçš„çœŸå®WASS-RAGå®éªŒ...")
    
    runner = WRENCHExperimentRunner()
    runner.run_all_experiments()
    
    print("\nğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆ!")

if __name__ == "__main__":
    main()
