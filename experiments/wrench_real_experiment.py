#!/usr/bin/env python3
"""
åŸºäºWRENCHçš„çœŸå®WASS-RAGå®éªŒæ¡†æ¶
ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨çœŸå®WRENCHç¯å¢ƒä¸­è¿›è¡Œæ€§èƒ½å¯¹æ¯”å®éªŒ
"""

import sys
import os
import json
import time
import random
import numpy as np
import torch
import torch.nn as nn
import pickle
from pathlib import Path
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Tuple
import yaml
from datetime import datetime

# ç¡®ä¿èƒ½å¯¼å…¥WRENCH
try:
    import wrench
except ImportError:
    print("Error: WRENCH not available. Please install wrench-python-api.")
    sys.exit(1)

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, str(parent_dir))

def load_config(cfg_path: str) -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(cfg_path, 'r', encoding='utf-8') as f:
        cfg = yaml.safe_load(f) or {}
    
    # Process includes
    if 'include' in cfg:
        base_dir = os.path.dirname(cfg_path)
        for include_file in cfg['include']:
            include_path = os.path.join(base_dir, include_file)
            if os.path.exists(include_path):
                with open(include_path, 'r', encoding='utf-8') as f:
                    include_cfg = yaml.safe_load(f) or {}
                    for key, value in include_cfg.items():
                        if key not in cfg:
                            cfg[key] = value
    return cfg

@dataclass
class WRENCHExperimentResult:
    """å•æ¬¡WRENCHå®éªŒç»“æœ"""
    scheduler_name: str
    workflow_id: str
    task_count: int
    dependency_count: int
    makespan: float
    cpu_utilization: Dict[str, float]
    task_execution_times: Dict[str, float]
    scheduling_decisions: List[Dict[str, Any]]
    experiment_metadata: Dict[str, Any]

class WRENCHScheduler:
    """åŸºç¡€WRENCHè°ƒåº¦å™¨æ¥å£"""
    
    def __init__(self, name: str):
        self.name = name
    
    def schedule_task(self, task, available_nodes: List[str], node_capacities: Dict, 
                     node_loads: Dict, compute_service) -> str:
        """è°ƒåº¦å•ä¸ªä»»åŠ¡ï¼Œè¿”å›é€‰æ‹©çš„èŠ‚ç‚¹"""
        raise NotImplementedError

class FIFOScheduler(WRENCHScheduler):
    """å…ˆè¿›å…ˆå‡ºè°ƒåº¦å™¨"""
    
    def __init__(self):
        super().__init__("FIFO")
    
    def schedule_task(self, task, available_nodes, node_capacities, node_loads, compute_service):
        # é€‰æ‹©è´Ÿè½½æœ€å°çš„èŠ‚ç‚¹
        return min(available_nodes, key=lambda x: node_loads.get(x, 0))

class HEFTScheduler(WRENCHScheduler):
    """å¼‚æ„æœ€æ—©å®Œæˆæ—¶é—´è°ƒåº¦å™¨"""
    
    def __init__(self):
        super().__init__("HEFT")
    
    def schedule_task(self, task, available_nodes, node_capacities, node_loads, compute_service):
        # é€‰æ‹©èƒ½æœ€æ—©å®Œæˆä»»åŠ¡çš„èŠ‚ç‚¹
        best_node = None
        best_finish_time = float('inf')
        
        for node in available_nodes:
            capacity = node_capacities.get(node, 1.0)
            load = node_loads.get(node, 0.0)
            exec_time = task.get_flops() / (capacity * 1e9)
            finish_time = load + exec_time
            
            if finish_time < best_finish_time:
                best_finish_time = finish_time
                best_node = node
        
        return best_node or available_nodes[0]

class WASSHeuristicScheduler(WRENCHScheduler):
    """WASSå¯å‘å¼è°ƒåº¦å™¨ - åœ¨HEFTåŸºç¡€ä¸Šè€ƒè™‘æ•°æ®å±€éƒ¨æ€§"""
    
    def __init__(self, data_locality_weight: float = 0.5):
        super().__init__("WASS-Heuristic")
        self.data_locality_weight = data_locality_weight  # wå‚æ•°
        self.data_location_cache = {}  # æ¨¡æ‹Ÿæ•°æ®ä½ç½®ç¼“å­˜
        
        # èŠ‚ç‚¹æ€§èƒ½å‚æ•°
        self.node_capacities = {
            "ComputeHost1": 2.0,
            "ComputeHost2": 3.0,
            "ComputeHost3": 2.5,
            "ComputeHost4": 4.0
        }
    
    def schedule_task(self, task, available_nodes, node_capacities, node_loads, compute_service):
        """ä½¿ç”¨WASSå¯å‘å¼è¿›è¡Œä»»åŠ¡è°ƒåº¦"""
        best_node = None
        best_score = float('inf')
        
        for node in available_nodes:
            # è®¡ç®—EFT (æœ€æ—©å®Œæˆæ—¶é—´)
            eft = self._calculate_eft(task, node, node_capacities, node_loads)
            
            # è®¡ç®—DRT (æ•°æ®å°±ç»ªæ—¶é—´)
            drt = self._calculate_drt(task, node)
            
            # è®¡ç®—WASSç»¼åˆè¯„åˆ†
            w = self.data_locality_weight
            score = (1 - w) * eft + w * drt
            
            if score < best_score:
                best_score = score
                best_node = node
        
        # æ›´æ–°æ•°æ®ä½ç½®ç¼“å­˜ï¼ˆå‡è®¾ä»»åŠ¡è¾“å‡ºæ•°æ®å­˜å‚¨åœ¨æ‰§è¡ŒèŠ‚ç‚¹ï¼‰
        if best_node:
            self._update_data_location(task, best_node)
        
        return best_node or available_nodes[0]
    
    def _calculate_eft(self, task, node, node_capacities, node_loads):
        """è®¡ç®—ä»»åŠ¡åœ¨æŒ‡å®šèŠ‚ç‚¹ä¸Šçš„æœ€æ—©å®Œæˆæ—¶é—´"""
        capacity = node_capacities.get(node, 1.0)
        load = node_loads.get(node, 0.0)
        exec_time = task.get_flops() / (capacity * 1e9)
        return load + exec_time
    
    def _calculate_drt(self, task, node):
        """è®¡ç®—æ•°æ®å°±ç»ªæ—¶é—´ - è€ƒè™‘æ•°æ®ä¼ è¾“å¼€é”€"""
        total_transfer_time = 0.0
        
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶çš„æ•°æ®ä½ç½®
        for input_file in task.get_input_files():
            # ä½¿ç”¨æ–‡ä»¶åè€Œä¸æ˜¯get_id()æ–¹æ³•
            file_id = input_file.get_name() if hasattr(input_file, 'get_name') else str(input_file)
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦åœ¨ç›®æ ‡èŠ‚ç‚¹ä¸Š
            data_location = self._get_data_location(file_id)
            if data_location != node:
                # éœ€è¦ä¼ è¾“æ•°æ®
                file_size = input_file.get_size() if hasattr(input_file, 'get_size') else 1024
                network_bandwidth = 1e9  # 1GB/s å‡è®¾ç½‘ç»œå¸¦å®½
                transfer_time = file_size / network_bandwidth
                total_transfer_time += transfer_time
        
        return total_transfer_time
    
    def _get_data_location(self, file_id):
        """è·å–æ–‡ä»¶çš„æ•°æ®ä½ç½®"""
        if file_id not in self.data_location_cache:
            # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œéšæœºé€‰æ‹©ä¸€ä¸ªä½ç½®ï¼ˆæ¨¡æ‹Ÿåˆå§‹æ•°æ®åˆ†å¸ƒï¼‰
            import random
            self.data_location_cache[file_id] = random.choice(
                ["ComputeHost1", "ComputeHost2", "ComputeHost3", "ComputeHost4"]
            )
        return self.data_location_cache[file_id]
    
    def _update_data_location(self, task, node):
        """æ›´æ–°ä»»åŠ¡è¾“å‡ºæ•°æ®çš„ä½ç½®"""
        for output_file in task.get_output_files():
            # ä½¿ç”¨æ–‡ä»¶åè€Œä¸æ˜¯get_id()æ–¹æ³•
            file_id = output_file.get_name() if hasattr(output_file, 'get_name') else str(output_file)
            self.data_location_cache[file_id] = node

# å®šä¹‰DRLç½‘ç»œç»“æ„
class SimpleDQN(nn.Module):
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):
        super(SimpleDQN, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )
    
    def forward(self, x):
        return self.network(x)

class WASSDRLScheduler(WRENCHScheduler):
    """åŸºäºè®­ç»ƒå¥½çš„DRLæ¨¡å‹çš„è°ƒåº¦å™¨"""
    
    def __init__(self, model_path: str, config_path: str = "configs/experiment.yaml"):
        """åˆå§‹åŒ–WASS-DRLè°ƒåº¦å™¨"""
        self.config = load_config(config_path)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # å®šä¹‰ç½‘ç»œç»“æ„
        self.state_dim = 17  # ä¸è®­ç»ƒæ—¶ä¸€è‡´
        self.action_dim = 4   # 4ä¸ªèŠ‚ç‚¹
        
        # å…ˆåˆ›å»ºæ¨¡å‹ï¼Œå†åŠ è½½æƒé‡
        self.model = self._create_model()
        self.epsilon = 0.1
        
        # åŠ è½½æ¨¡å‹
        self._load_model(model_path)
        
        # èŠ‚ç‚¹æ˜ å°„
        self.compute_nodes = ["ComputeHost1", "ComputeHost2", "ComputeHost3", "ComputeHost4"]
    
    def _create_model(self):
        """åˆ›å»ºDRLæ¨¡å‹"""
        return SimpleDQN(self.state_dim, self.action_dim).to(self.device)
    
    def _load_model(self, model_path: str):
        """åŠ è½½è®­ç»ƒå¥½çš„DRLæ¨¡å‹"""
        try:
            # ä¿®å¤PyTorch 2.6å…¼å®¹æ€§é—®é¢˜
            checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
            
            # æ£€æŸ¥æ¨¡å‹æ ¼å¼
            if 'drl_agent' in checkpoint:
                agent_state = checkpoint['drl_agent']
                self.model.load_state_dict(agent_state['model_state_dict'])
                self.epsilon = agent_state.get('epsilon', 0.1)
                print(f"âœ… DRLæ¨¡å‹åŠ è½½æˆåŠŸ (è®­ç»ƒè½®æ•°: {agent_state.get('training_episodes', 'unknown')})")
            else:
                # å…¼å®¹æ—§æ ¼å¼
                self.model.load_state_dict(checkpoint)
                print("âœ… DRLæ¨¡å‹åŠ è½½æˆåŠŸ (æ—§æ ¼å¼)")
                
            self.model.eval()
            
        except Exception as e:
            print(f"âŒ DRLæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.model = None
    
    def _get_state(self, task, available_nodes, node_capacities, node_loads):
        """è·å–DRLçš„çŠ¶æ€å‘é‡"""
        state = []
        
        # ä»»åŠ¡ç‰¹å¾ (4ç»´)
        try:
            # ä½¿ç”¨WRENCH APIçš„æ­£ç¡®æ–¹æ³•è·å–ä»»åŠ¡ä¿¡æ¯
            task_flops = float(getattr(task, 'get_flops', lambda: 1e9)()) / 1e9  # ä»»åŠ¡è®¡ç®—é‡ (GFLOPS)
            
            # å°è¯•è·å–å†…å­˜éœ€æ±‚ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼
            try:
                task_memory = float(task.get_memory_requirement()) / 1e9  # å†…å­˜éœ€æ±‚ (GB)
            except (AttributeError, TypeError):
                task_memory = 1.0  # é»˜è®¤å€¼
            
            task_cores = float(getattr(task, 'get_min_num_cores', lambda: 1)())  # æœ€å°æ ¸å¿ƒæ•°
            task_children = float(len(getattr(task, 'get_children', lambda: [])()))  # å­ä»»åŠ¡æ•°
            
            state.extend([task_flops, task_memory, task_cores, task_children])
            
        except Exception as e:
            # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
            state.extend([1.0, 1.0, 1.0, 0.0])
        
        # èŠ‚ç‚¹ç‰¹å¾ (æ¯èŠ‚ç‚¹3ç»´ï¼Œæœ€å¤š4ä¸ªèŠ‚ç‚¹ = 12ç»´)
        for i, node in enumerate(available_nodes[:4]):  # é™åˆ¶æœ€å¤š4ä¸ªèŠ‚ç‚¹
            node_capacity = node_capacities.get(node, 1.0)
            node_load = node_loads.get(node, 0.0)
            
            state.extend([
                float(node_capacity),  # èŠ‚ç‚¹å®¹é‡
                float(node_load),  # èŠ‚ç‚¹è´Ÿè½½
                float(node_load / max(node_capacity, 1e-6))  # è´Ÿè½½ç‡
            ])
        
        # å¡«å……ä¸è¶³çš„èŠ‚ç‚¹ç»´åº¦
        while len(state) < 16:  # 4(ä»»åŠ¡) + 4*3(èŠ‚ç‚¹) = 16ç»´
            state.append(0.0)
        
        # å…¨å±€ç‰¹å¾ (1ç»´)
        avg_load = sum(node_loads.values()) / max(len(node_loads), 1)
        state.append(float(avg_load))
        
        # ç¡®ä¿çŠ¶æ€ç»´åº¦ä¸º17ç»´
        if len(state) > 17:
            state = state[:17]
        elif len(state) < 17:
            state.extend([0.0] * (17 - len(state)))
        
        return np.array(state, dtype=np.float32)
    
    def schedule_task(self, task, available_nodes, node_capacities, node_loads, compute_service):
        if self.model is None:
            # æ¨¡å‹æœªåŠ è½½ï¼ŒæŠ›å‡ºå¼‚å¸¸è€Œä¸æ˜¯å›é€€
            raise RuntimeError("DRLæ¨¡å‹æœªæ­£ç¡®åŠ è½½ï¼Œæ— æ³•è¿›è¡Œè°ƒåº¦")
        
        try:
            state = self._get_state(task, available_nodes, node_capacities, node_loads)
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                q_values = self.model(state_tensor)
                action = q_values.argmax().item()
            
            # æ˜ å°„åŠ¨ä½œåˆ°èŠ‚ç‚¹
            if action < len(self.compute_nodes):
                chosen_node = self.compute_nodes[action]
                if chosen_node in available_nodes:
                    return chosen_node
            
            # å¦‚æœé€‰æ‹©çš„èŠ‚ç‚¹ä¸å¯ç”¨ï¼ŒæŠ›å‡ºå¼‚å¸¸
            raise RuntimeError("DRLæ¨¡å‹é€‰æ‹©çš„èŠ‚ç‚¹ä¸å¯ç”¨")
            
        except Exception as e:
            print(f"âš ï¸ DRLè°ƒåº¦å¤±è´¥: {e}")
            raise RuntimeError(f"DRLè°ƒåº¦å¤±è´¥: {e}")
    
    def _heuristic_fallback(self, task, available_nodes, node_capacities, node_loads):
        """å¯å‘å¼å›é€€è°ƒåº¦ç­–ç•¥"""
        try:
            # è·å–ä»»åŠ¡ç‰¹å¾
            task_flops = float(getattr(task, 'get_flops', lambda: 1e9)())
            
            # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„å¾—åˆ†ï¼ˆè€ƒè™‘å®¹é‡å’Œè´Ÿè½½ï¼‰
            best_node = None
            best_score = -float('inf')
            
            for node in available_nodes:
                capacity = node_capacities.get(node, 1.0)
                load = node_loads.get(node, 0.0)
                
                # è®¡ç®—å¾—åˆ†ï¼šå®¹é‡è¶Šé«˜è¶Šå¥½ï¼Œè´Ÿè½½è¶Šä½è¶Šå¥½
                score = capacity - load * 2.0  # è´Ÿè½½æƒé‡æ›´é«˜
                
                if score > best_score:
                    best_score = score
                    best_node = node
            
            return best_node if best_node else available_nodes[0]
            
        except Exception as e:
            print(f"âš ï¸ å¯å‘å¼å›é€€ä¹Ÿå¤±è´¥: {e}ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨èŠ‚ç‚¹")
            return available_nodes[0]

class WASSRAGScheduler(WRENCHScheduler):
    """åŸºäºRAGçŸ¥è¯†åº“å¢å¼ºçš„è°ƒåº¦å™¨"""
    
    def __init__(self, model_path: str, rag_path: str):
        super().__init__("WASS-RAG")
        self.drl_scheduler = WASSDRLScheduler(model_path)
        self.knowledge_base = None
        self._load_rag_knowledge(rag_path)
    
    def _load_rag_knowledge(self, rag_path: str):
        """åŠ è½½å¢å¼ºçš„RAGçŸ¥è¯†åº“"""
        self.knowledge_base = []
        
        # ä¼˜å…ˆä½¿ç”¨å¢å¼ºçŸ¥è¯†åº“
        enhanced_path = "data/enhanced_rag_knowledge.json"
        if os.path.exists(enhanced_path):
            try:
                with open(enhanced_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                cases = data.get('cases', [])
                self.knowledge_base.extend(cases)
                print(f"âœ… å¢å¼ºRAGçŸ¥è¯†åº“å·²åŠ è½½: {len(cases)} ä¸ªä¼˜åŒ–æ¡ˆä¾‹")
                return
            except Exception as e:
                print(f"å¢å¼ºçŸ¥è¯†åº“åŠ è½½å¤±è´¥: {e}")
        
        # å›é€€åˆ°åŸå§‹æ–¹æ³•
        # æ–¹æ³•1: ä½¿ç”¨æ‰©å±•çš„JSONçŸ¥è¯†åº“
        extended_json_path = "data/extended_rag_knowledge.json"
        if os.path.exists(extended_json_path):
            try:
                with open(extended_json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                cases = []
                if isinstance(data, dict):
                    if 'cases' in data:
                        cases = data['cases']
                    elif 'sample_cases' in data:
                        cases = data['sample_cases']
                    else:
                        cases = list(data.values()) if isinstance(data, dict) else data
                elif isinstance(data, list):
                    cases = data
                
                for case in cases:
                    if isinstance(case, dict):
                        simple_case = {
                            'task_flops': float(case.get('task_flops', case.get('task_execution_time', 1.0) * 2e9)),
                            'chosen_node': str(case.get('chosen_node', 'ComputeHost1')),
                            'scheduler_type': str(case.get('scheduler_type', 'unknown')),
                            'task_execution_time': float(case.get('task_execution_time', 0.0)),
                            'workflow_makespan': float(case.get('workflow_makespan', 0.0)),
                            'node_capacity': float(case.get('node_capacity', 2.0)),
                            'performance_ratio': float(case.get('performance_ratio', 1.0)),
                            'total_workflow_flops': float(case.get('total_workflow_flops', case.get('task_flops', 1e9))),
                            'workflow_size': int(case.get('workflow_size', 5))
                        }
                        self.knowledge_base.append(simple_case)
                
                if self.knowledge_base:
                    print(f"âœ… RAGçŸ¥è¯†åº“å·²ä»æ‰©å±•JSONåŠ è½½: {len(self.knowledge_base)} ä¸ªæ¡ˆä¾‹")
                    return
                    
            except Exception as e:
                print(f"æ‰©å±•JSONåŠ è½½å¤±è´¥: {e}")
        
        # æ–¹æ³•2: ä½¿ç”¨PKLæ–‡ä»¶ï¼ˆå›é€€æ–¹æ¡ˆï¼‰
        try:
            import pickle
            with open(rag_path, 'rb') as f:
                data = pickle.load(f)
            
            # å¤„ç†ä¸åŒæ ¼å¼çš„pickleæ•°æ®
            cases = []
            if isinstance(data, dict):
                cases = data.get('cases', data.get('sample_cases', []))
            elif isinstance(data, list):
                cases = data
            else:
                # å°è¯•ç›´æ¥è¿­ä»£
                try:
                    cases = list(data)
                except:
                    cases = [data]
            
            for case in cases:
                try:
                    if hasattr(case, '__dict__'):
                        # å¤„ç†å¯¹è±¡ç±»å‹
                        case_dict = case.__dict__
                    elif isinstance(case, dict):
                        case_dict = case
                    else:
                        continue
                    
                    simple_case = {
                        'task_flops': float(case_dict.get('task_flops', case_dict.get('task_execution_time', 1.0) * 2e9)),
                        'chosen_node': str(case_dict.get('chosen_node', 'ComputeHost1')),
                        'scheduler_type': str(case_dict.get('scheduler_type', 'unknown')),
                        'task_execution_time': float(case_dict.get('task_execution_time', 0.0))
                    }
                    self.knowledge_base.append(simple_case)
                    
                except Exception as e:
                    continue
            
            if self.knowledge_base:
                print(f"âœ… RAGçŸ¥è¯†åº“å·²ä»PKLåŠ è½½: {len(self.knowledge_base)} ä¸ªæ¡ˆä¾‹")
                return
                
        except Exception as e:
            print(f"PKLåŠ è½½å¤±è´¥: {e}")
        
        # æ–¹æ³•3: ä»æ‰©å±•JSONç›´æ¥è¯»å–ï¼ˆæœ€ç»ˆå›é€€ï¼‰
        try:
            with open("data/extended_rag_knowledge.json", 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ç®€åŒ–å¤„ç†ï¼šä»JSONä¸­æå–ä»»ä½•å¯ç”¨çš„æ¡ˆä¾‹æ•°æ®
            if isinstance(data, list):
                for case in data[:100]:  # é™åˆ¶æ•°é‡é¿å…å†…å­˜é—®é¢˜
                    if isinstance(case, dict):
                        simple_case = {
                            'task_flops': float(case.get('task_flops', 1e9)),
                            'chosen_node': str(case.get('chosen_node', 'ComputeHost1')),
                            'scheduler_type': str(case.get('scheduler_type', 'FIFO'))
                        }
                        self.knowledge_base.append(simple_case)
            
            if self.knowledge_base:
                print(f"âœ… RAGçŸ¥è¯†åº“å·²ä»JSONåŠ è½½(ç®€åŒ–æ¨¡å¼): {len(self.knowledge_base)} ä¸ªæ¡ˆä¾‹")
                return
                
        except Exception as e:
            print(f"æœ€ç»ˆåŠ è½½å¤±è´¥: {e}")
        
        # æ–¹æ³•4: åˆ›å»ºé»˜è®¤çŸ¥è¯†åº“ï¼ˆå¦‚æœæ‰€æœ‰åŠ è½½æ–¹æ³•éƒ½å¤±è´¥ï¼‰
        print("âš ï¸ æ— æ³•åŠ è½½ä»»ä½•RAGçŸ¥è¯†åº“ï¼Œåˆ›å»ºé»˜è®¤çŸ¥è¯†åº“...")
        self._create_default_knowledge_base()
    
    def _create_default_knowledge_base(self):
        """åˆ›å»ºé»˜è®¤çš„RAGçŸ¥è¯†åº“"""
        # åŸºäºèŠ‚ç‚¹æ€§èƒ½å’Œä»»åŠ¡ç‰¹å¾çš„ç®€å•å¯å‘å¼è§„åˆ™
        default_cases = [
            # å°ä»»åŠ¡ä¼˜å…ˆåˆ†é…åˆ°é«˜å®¹é‡èŠ‚ç‚¹
            {'task_flops': 1e9, 'chosen_node': 'ComputeHost4', 'scheduler_type': 'heuristic', 
             'task_execution_time': 0.25, 'workflow_makespan': 5.0, 'node_capacity': 4.0,
             'performance_ratio': 0.8, 'total_workflow_flops': 5e9, 'workflow_size': 5},
            
            # ä¸­ç­‰ä»»åŠ¡åˆ†é…åˆ°ä¸­ç­‰å®¹é‡èŠ‚ç‚¹
            {'task_flops': 5e9, 'chosen_node': 'ComputeHost2', 'scheduler_type': 'heuristic',
             'task_execution_time': 1.67, 'workflow_makespan': 10.0, 'node_capacity': 3.0,
             'performance_ratio': 0.9, 'total_workflow_flops': 20e9, 'workflow_size': 10},
            
            # å¤§ä»»åŠ¡åˆ†é…åˆ°é«˜å®¹é‡èŠ‚ç‚¹
            {'task_flops': 10e9, 'chosen_node': 'ComputeHost4', 'scheduler_type': 'heuristic',
             'task_execution_time': 2.5, 'workflow_makespan': 15.0, 'node_capacity': 4.0,
             'performance_ratio': 0.85, 'total_workflow_flops': 50e9, 'workflow_size': 15},
            
            # è€ƒè™‘è´Ÿè½½å‡è¡¡çš„æ¡ˆä¾‹
            {'task_flops': 3e9, 'chosen_node': 'ComputeHost1', 'scheduler_type': 'heuristic',
             'task_execution_time': 1.5, 'workflow_makespan': 8.0, 'node_capacity': 2.0,
             'performance_ratio': 0.75, 'total_workflow_flops': 15e9, 'workflow_size': 8},
            
            # æ›´å¤šå¤šæ ·åŒ–æ¡ˆä¾‹
            {'task_flops': 7e9, 'chosen_node': 'ComputeHost3', 'scheduler_type': 'heuristic',
             'task_execution_time': 2.8, 'workflow_makespan': 12.0, 'node_capacity': 2.5,
             'performance_ratio': 0.82, 'total_workflow_flops': 30e9, 'workflow_size': 12}
        ]
        
        # æ·»åŠ ä¸€äº›éšæœºå˜åŒ–ä»¥å¢åŠ å¤šæ ·æ€§
        import random
        random.seed(42)  # ç¡®ä¿å¯é‡ç°
        
        for _ in range(20):  # ç”Ÿæˆ20ä¸ªé¢å¤–æ¡ˆä¾‹
            base_case = random.choice(default_cases)
            variation = base_case.copy()
            
            # æ·»åŠ ä¸€äº›éšæœºå˜åŒ–
            variation['task_flops'] *= random.uniform(0.8, 1.2)
            variation['total_workflow_flops'] *= random.uniform(0.9, 1.1)
            variation['workflow_size'] = max(3, int(variation['workflow_size'] * random.uniform(0.8, 1.2)))
            
            # æ ¹æ®ä»»åŠ¡å¤§å°é€‰æ‹©åˆé€‚çš„èŠ‚ç‚¹
            if variation['task_flops'] < 3e9:
                variation['chosen_node'] = random.choice(['ComputeHost1', 'ComputeHost2'])
            elif variation['task_flops'] < 7e9:
                variation['chosen_node'] = random.choice(['ComputeHost2', 'ComputeHost3'])
            else:
                variation['chosen_node'] = random.choice(['ComputeHost3', 'ComputeHost4'])
            
            self.knowledge_base.append(variation)
        
        # æ·»åŠ åŸå§‹é»˜è®¤æ¡ˆä¾‹
        self.knowledge_base.extend(default_cases)
        
        print(f"âœ… é»˜è®¤RAGçŸ¥è¯†åº“å·²åˆ›å»º: {len(self.knowledge_base)} ä¸ªæ¡ˆä¾‹")
    
    def schedule_task(self, task, available_nodes, node_capacities, node_loads, compute_service):
        """åŸºäºRAGçŸ¥è¯†åº“å¢å¼ºçš„è°ƒåº¦å†³ç­– - ä¼˜åŒ–ç‰ˆæœ¬"""
        try:
            # é¦–å…ˆä½¿ç”¨DRLè¿›è¡ŒåŸºç¡€è°ƒåº¦å†³ç­–
            drl_node = self.drl_scheduler.schedule_task(
                task, available_nodes, node_capacities, node_loads, compute_service
            )
            
            # å¦‚æœæ²¡æœ‰çŸ¥è¯†åº“ï¼ŒæŠ›å‡ºå¼‚å¸¸è€Œä¸æ˜¯å›é€€
            if not self.knowledge_base or len(self.knowledge_base) == 0:
                raise RuntimeError("RAGçŸ¥è¯†åº“ä¸ºç©ºï¼Œæ— æ³•è¿›è¡ŒRAGå¢å¼ºè°ƒåº¦")
            
            # è·å–æ›´ä¸°å¯Œçš„ä»»åŠ¡ç‰¹å¾ç”¨äºRAGåŒ¹é…
            try:
                task_flops = float(getattr(task, 'get_flops', lambda: 1e9)())
                task_memory = float(getattr(task, 'get_memory_requirement', lambda: 1024)())
                
                # è®¡ç®—å·¥ä½œæµç‰¹å¾
                total_workflow_flops = sum(
                    float(getattr(t, 'get_flops', lambda: 1e9)()) 
                    for t in [task]  # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥è·å–æ•´ä¸ªå·¥ä½œæµ
                )
                
                # è®¡ç®—å½“å‰èŠ‚ç‚¹è´Ÿè½½ç‰¹å¾
                avg_load = np.mean([node_loads.get(node, 0) for node in available_nodes])
                max_load = max([node_loads.get(node, 0) for node in available_nodes])
                
            except Exception as e:
                task_flops = 1e9
                task_memory = 1024
                total_workflow_flops = task_flops
                avg_load = 0
                max_load = 0
            
            # å¢å¼ºçš„ç›¸ä¼¼åº¦åŒ¹é… - é™ä½é˜ˆå€¼ä»¥è·å–æ›´å¤šåŒ¹é…
            best_matches = []
            min_similarity_threshold = 0.5  # ä»0.7é™ä½åˆ°0.5
            
            for case in self.knowledge_base:
                # å¤šç»´ç‰¹å¾ç›¸ä¼¼åº¦è®¡ç®—
                case_flops = float(case.get('task_flops', 1e9))
                case_workflow_flops = float(case.get('total_workflow_flops', case_flops))
                
                # è®¡ç®—å½’ä¸€åŒ–è·ç¦»
                flops_distance = abs(task_flops - case_flops) / max(task_flops, case_flops, 1e-6)
                workflow_distance = abs(total_workflow_flops - case_workflow_flops) / max(total_workflow_flops, case_workflow_flops, 1e-6)
                
                # ç»¼åˆç›¸ä¼¼åº¦
                similarity = 1.0 - (flops_distance * 0.6 + workflow_distance * 0.4)
                
                if similarity >= min_similarity_threshold:
                    best_matches.append({
                        'case': case,
                        'similarity': similarity,
                        'suggested_node': str(case.get('chosen_node', drl_node))
                    })
            
            # å¦‚æœæœ‰é«˜è´¨é‡åŒ¹é…ï¼Œä½¿ç”¨åŠ æƒæŠ•ç¥¨æœºåˆ¶
            if best_matches:
                # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œå–å‰5ä¸ªæœ€ä½³åŒ¹é…
                best_matches.sort(key=lambda x: x['similarity'], reverse=True)
                top_matches = best_matches[:5]
                
                # åŠ æƒæŠ•ç¥¨é€‰æ‹©èŠ‚ç‚¹
                node_votes = {}
                for match in top_matches:
                    node = match['suggested_node']
                    weight = match['similarity']
                    
                    if node not in node_votes:
                        node_votes[node] = 0
                    node_votes[node] += weight
                
                # é€‰æ‹©å¾—ç¥¨æœ€é«˜çš„èŠ‚ç‚¹
                if node_votes:
                    rag_suggested_node = max(node_votes.keys(), key=lambda x: node_votes[x])
                    
                    # å¢å¼ºRAGå†³ç­–æƒé‡ï¼šå¦‚æœRAGæœ‰å¼ºå»ºè®®ä¸”èŠ‚ç‚¹å¯ç”¨ï¼Œä¼˜å…ˆä½¿ç”¨
                    total_weight = sum(node_votes.values())
                    max_weight = max(node_votes.values())
                    confidence = max_weight / total_weight
                    
                    # é™ä½ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œä»0.4é™åˆ°0.3ï¼Œæ›´å€¾å‘äºä½¿ç”¨RAGå»ºè®®
                    if confidence > 0.3 and rag_suggested_node in available_nodes:
                        print(f"ğŸ¯ RAGå†³ç­–: é€‰æ‹©{rag_suggested_node} (ç½®ä¿¡åº¦: {confidence:.2f})")
                        return rag_suggested_node
            
            # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„åŒ¹é…ï¼ŒæŠ›å‡ºå¼‚å¸¸
            raise RuntimeError("RAGçŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿçš„åŒ¹é…æ¡ˆä¾‹")
            
        except Exception as e:
            print(f"âš ï¸ RAGè°ƒåº¦å¤±è´¥: {e}")
            raise RuntimeError(f"RAGè°ƒåº¦å¤±è´¥: {e}")

class WRENCHExperimentRunner:
    """åŸºäºçœŸå®WRENCHçš„å®éªŒè¿è¡Œå™¨"""
    
    def __init__(self, config_path: str = "configs/experiment.yaml"):
        self.config = load_config(config_path)
        
        # WRENCHå¹³å°é…ç½®
        self.platform_file = self.config['platform']['platform_file']
        self.controller_host = "ControllerHost"
        
        # èŠ‚ç‚¹é…ç½®
        self.compute_nodes = ["ComputeHost1", "ComputeHost2", "ComputeHost3", "ComputeHost4"]
        self.node_capacities = {
            "ComputeHost1": 2.0,
            "ComputeHost2": 3.0,
            "ComputeHost3": 2.5,
            "ComputeHost4": 4.0
        }
        
        # è°ƒåº¦å™¨é…ç½®
        self.schedulers = self._initialize_schedulers()
        
        # å®éªŒå‚æ•°
        self.workflow_sizes = [5, 10, 15, 20]
        self.repetitions = 3
        
        # ç»“æœå­˜å‚¨
        self.results = []
        
        print(f"ğŸš€ WRENCHå®éªŒè¿è¡Œå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_schedulers(self):
        """åˆå§‹åŒ–æ‰€æœ‰è°ƒåº¦å™¨"""
        schedulers = {
            "FIFO": FIFOScheduler(),
            "HEFT": HEFTScheduler(),
            "WASS-Heuristic": WASSHeuristicScheduler(),  # æ–°å¢WASSå¯å‘å¼è°ƒåº¦å™¨
        }
        
        # æ¨¡å‹æ–‡ä»¶ä¼˜å…ˆçº§ï¼šå…¼å®¹æ¨¡å‹ > åŸå§‹ä¼˜åŒ–æ¨¡å‹ > åŸºç¡€æ¨¡å‹
        model_candidates = [
            "models/wass_optimized_models_compatible.pth",
            "models/wass_optimized_models.pth",
            "models/wass_models.pth"
        ]
        
        model_path = None
        for candidate in model_candidates:
            if os.path.exists(candidate):
                model_path = candidate
                break
        
        rag_path = "data/wrench_rag_knowledge_base.pkl"
        
        if model_path:
            print(f"ğŸ“ ä½¿ç”¨æ¨¡å‹æ–‡ä»¶: {model_path}")
            
            # å¼ºåˆ¶å¯ç”¨WASS-DRLè°ƒåº¦å™¨
            try:
                drl_scheduler = WASSDRLScheduler(model_path)
                schedulers["WASS-DRL"] = drl_scheduler
                print("âœ… WASS-DRLè°ƒåº¦å™¨å·²å¼ºåˆ¶å¯ç”¨")
                
                # å¼ºåˆ¶å¯ç”¨WASS-RAGè°ƒåº¦å™¨
                rag_candidates = [
                    rag_path,
                    "data/wrench_rag_knowledge_base.json",
                    "data/extended_rag_knowledge.json"
                ]
                
                rag_available = False
                for rag_candidate in rag_candidates:
                    if os.path.exists(rag_candidate):
                        try:
                            rag_scheduler = WASSRAGScheduler(model_path, rag_candidate)
                            schedulers["WASS-RAG"] = rag_scheduler
                            print(f"âœ… WASS-RAGè°ƒåº¦å™¨å·²å¯ç”¨ (çŸ¥è¯†åº“: {rag_candidate})")
                            rag_available = True
                            break
                        except Exception as e:
                            print(f"âš ï¸  WASS-RAGä»{rag_candidate}åŠ è½½å¤±è´¥: {e}")
                            continue
                
                if not rag_available:
                    # å³ä½¿æ²¡æœ‰çŸ¥è¯†åº“ï¼Œä¹Ÿåˆ›å»ºç©ºçš„RAGè°ƒåº¦å™¨
                    rag_scheduler = WASSRAGScheduler(model_path, rag_path)
                    schedulers["WASS-RAG"] = rag_scheduler
                    print("âš ï¸  WASS-RAGè°ƒåº¦å™¨å·²åˆ›å»º (çŸ¥è¯†åº“ä¸ºç©º)")
                    
            except Exception as e:
                print(f"âŒ DRL/RAGè°ƒåº¦å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        else:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ¨¡å‹æ–‡ä»¶ï¼Œä»…ä½¿ç”¨åŸºç¡€è°ƒåº¦å™¨")
        
        print(f"ğŸ”§ å·²å¯ç”¨è°ƒåº¦å™¨: {list(schedulers.keys())}")
        return schedulers
    
    def run_single_experiment_with_workflow(self, scheduler_name: str, workflow, workflow_size: int, experiment_id: int) -> WRENCHExperimentResult:
        """ä½¿ç”¨é¢„ç”Ÿæˆçš„å·¥ä½œæµè¿è¡Œå•ä¸ªå®éªŒ"""
        print(f"    ğŸ”¬ è¿è¡Œå®éªŒ: {scheduler_name} (å·¥ä½œæµå¤§å°: {workflow_size})")
        
        start_time = time.time()
        
        try:
            # è·å–è°ƒåº¦å™¨
            scheduler = self.schedulers[scheduler_name]
            
            # æ¨¡æ‹ŸWRENCHå®éªŒæ‰§è¡Œ
            # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œåº”è¯¥è°ƒç”¨çœŸå®çš„WRENCH API
            simulation_result = self._simulate_wrench_execution(scheduler, workflow, workflow_size)
            
            # åˆ›å»ºå®éªŒç»“æœ
            result = WRENCHExperimentResult(
                scheduler_name=scheduler_name,
                workflow_id=f"workflow_{workflow_size}_{experiment_id}",
                task_count=workflow_size,
                dependency_count=int(workflow_size * 0.8),  # å‡è®¾80%çš„ä»»åŠ¡æœ‰ä¾èµ–
                makespan=simulation_result['makespan'],
                cpu_utilization=simulation_result['cpu_utilization'],
                task_execution_times=simulation_result['task_times'],
                scheduling_decisions=simulation_result['decisions'],
                experiment_metadata={
                    'experiment_id': experiment_id,
                    'workflow_size': workflow_size,
                    'execution_time': time.time() - start_time,
                    'timestamp': datetime.now().isoformat()
                }
            )
            
            return result
            
        except Exception as e:
            # è¿”å›å¤±è´¥ç»“æœ
            return WRENCHExperimentResult(
                scheduler_name=scheduler_name,
                workflow_id=f"workflow_{workflow_size}_{experiment_id}",
                task_count=workflow_size,
                dependency_count=0,
                makespan=float('inf'),
                cpu_utilization={},
                task_execution_times={},
                scheduling_decisions=[],
                experiment_metadata={
                    'experiment_id': experiment_id,
                    'workflow_size': workflow_size,
                    'execution_time': time.time() - start_time,
                    'timestamp': datetime.now().isoformat(),
                    'error': str(e)
                }
            )
    
    def _generate_workflow(self, workflow_size: int, repetition: int) -> Dict:
        """ç”Ÿæˆå›ºå®šçš„å·¥ä½œæµï¼ˆåŸºäºéšæœºç§å­ç¡®ä¿å¯é‡ç°ï¼‰"""
        # è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ç›¸åŒå‚æ•°ç”Ÿæˆç›¸åŒå·¥ä½œæµ
        seed = 42 + workflow_size * 100 + repetition
        random.seed(seed)
        np.random.seed(seed)
        
        # ç”Ÿæˆå·¥ä½œæµç»“æ„
        workflow = {
            'tasks': [],
            'dependencies': [],
            'seed': seed
        }
        
        # ç”Ÿæˆä»»åŠ¡
        for i in range(workflow_size):
            task = {
                'id': f"task_{i}",
                'flops': random.uniform(1e9, 10e9),  # 1-10 GFLOPS
                'memory': random.uniform(1, 8),       # 1-8 GB
                'cores': random.randint(1, 4)         # 1-4 cores
            }
            workflow['tasks'].append(task)
        
        # ç”Ÿæˆä¾èµ–å…³ç³»ï¼ˆDAGç»“æ„ï¼‰
        # ç®€å•å®ç°ï¼šæ¯ä¸ªä»»åŠ¡ä¾èµ–äºä¹‹å‰çš„1-3ä¸ªä»»åŠ¡
        for i in range(1, workflow_size):
            num_deps = min(random.randint(1, 3), i)  # æœ€å¤šä¾èµ–å‰é¢çš„3ä¸ªä»»åŠ¡
            deps = random.sample(range(i), num_deps)
            
            for dep in deps:
                workflow['dependencies'].append({
                    'from': f"task_{dep}",
                    'to': f"task_{i}"
                })
        
        return workflow
    
    def _simulate_wrench_execution(self, scheduler, workflow: Dict, workflow_size: int) -> Dict:
        """æ¨¡æ‹ŸWRENCHæ‰§è¡Œï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # æ¨¡æ‹ŸèŠ‚ç‚¹è´Ÿè½½
        node_loads = {node: 0.0 for node in self.compute_nodes}
        
        # æ¨¡æ‹Ÿä»»åŠ¡æ‰§è¡Œ
        task_times = {}
        decisions = []
        
        # æŒ‰æ‹“æ‰‘é¡ºåºæ‰§è¡Œä»»åŠ¡ï¼ˆç®€åŒ–å¤„ç†ï¼‰
        task_order = list(range(workflow_size))
        
        # éšæœºæ‰“ä¹±ä»»åŠ¡é¡ºåºï¼ˆæ¨¡æ‹ŸçœŸå®è°ƒåº¦ï¼‰
        random.shuffle(task_order)
        
        total_makespan = 0.0
        
        for task_id in task_order:
            task = workflow['tasks'][task_id]
            
            # è·å–å¯ç”¨èŠ‚ç‚¹
            available_nodes = list(self.compute_nodes)
            
            # ä½¿ç”¨è°ƒåº¦å™¨é€‰æ‹©èŠ‚ç‚¹
            try:
                # åˆ›å»ºæ¨¡æ‹Ÿä»»åŠ¡å¯¹è±¡
                class MockTask:
                    def __init__(self, flops, memory, cores):
                        self._flops = flops
                        self._memory = memory
                        self._cores = cores
                    
                    def get_flops(self):
                        return self._flops
                    
                    def get_memory_requirement(self):
                        return self._memory * 1024 * 1024 * 1024  # è½¬æ¢ä¸ºå­—èŠ‚
                    
                    def get_min_num_cores(self):
                        return self._cores
                    
                    def get_input_files(self):
                        return []  # ç®€åŒ–å¤„ç†
                    
                    def get_output_files(self):
                        return []  # ç®€åŒ–å¤„ç†
                
                mock_task = MockTask(task['flops'], task['memory'], task['cores'])
                
                # è°ƒç”¨è°ƒåº¦å™¨
                chosen_node = scheduler.schedule_task(
                    mock_task, available_nodes, self.node_capacities, node_loads, None
                )
                
                # è®¡ç®—æ‰§è¡Œæ—¶é—´
                capacity = self.node_capacities[chosen_node]
                exec_time = task['flops'] / (capacity * 1e9)
                
                # æ›´æ–°èŠ‚ç‚¹è´Ÿè½½
                node_loads[chosen_node] += exec_time
                
                # è®°å½•ä»»åŠ¡æ‰§è¡Œæ—¶é—´
                task_times[f"task_{task_id}"] = exec_time
                
                # è®°å½•è°ƒåº¦å†³ç­–
                decisions.append({
                    'task_id': f"task_{task_id}",
                    'chosen_node': chosen_node,
                    'execution_time': exec_time,
                    'start_time': node_loads[chosen_node] - exec_time,
                    'end_time': node_loads[chosen_node]
                })
                
                # æ›´æ–°æ€»makespan
                total_makespan = max(total_makespan, node_loads[chosen_node])
                
            except Exception as e:
                print(f"      âš ï¸ ä»»åŠ¡è°ƒåº¦å¤±è´¥: {e}")
                # ä½¿ç”¨é»˜è®¤èŠ‚ç‚¹
                chosen_node = self.compute_nodes[0]
                exec_time = task['flops'] / (self.node_capacities[chosen_node] * 1e9)
                node_loads[chosen_node] += exec_time
                task_times[f"task_{task_id}"] = exec_time
                total_makespan = max(total_makespan, node_loads[chosen_node])
        
        # è®¡ç®—CPUåˆ©ç”¨ç‡
        cpu_utilization = {}
        for node in self.compute_nodes:
            if total_makespan > 0:
                utilization = node_loads[node] / total_makespan
                cpu_utilization[node] = min(utilization, 1.0)
            else:
                cpu_utilization[node] = 0.0
        
        return {
            'makespan': total_makespan,
            'cpu_utilization': cpu_utilization,
            'task_times': task_times,
            'decisions': decisions
        }
    
    def run_single_experiment(self, scheduler_name: str, workflow_size: int, experiment_id: int) -> WRENCHExperimentResult:
        """è¿è¡Œå•æ¬¡WRENCHå®éªŒï¼ˆä½¿ç”¨é¢„ç”Ÿæˆçš„å·¥ä½œæµï¼‰"""
        print(f"  è¿è¡Œå®éªŒ: {scheduler_name}, {workflow_size}ä»»åŠ¡, å®éªŒ#{experiment_id}")
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰é¢„ç”Ÿæˆçš„å·¥ä½œæµ
        workflow_key = f"{workflow_size}_{experiment_id}"
        if workflow_key in self.workflow_cache:
            workflow = self.workflow_cache[workflow_key]
            print(f"    ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„å·¥ä½œæµ: {workflow_key}")
        else:
            # ç”Ÿæˆæ–°çš„å·¥ä½œæµå¹¶ç¼“å­˜
            workflow = self._generate_workflow(workflow_size, experiment_id)
            self.workflow_cache[workflow_key] = workflow
            print(f"    ğŸ“‹ ç”Ÿæˆå¹¶ç¼“å­˜æ–°å·¥ä½œæµ: {workflow_key}")
        
        # ä½¿ç”¨é¢„ç”Ÿæˆçš„å·¥ä½œæµè¿è¡Œå®éªŒ
        return self.run_single_experiment_with_workflow(scheduler_name, workflow, workflow_size, experiment_id)
        
    def run_all_experiments(self):
        """è¿è¡Œæ‰€æœ‰å®éªŒé…ç½®ï¼ˆå…¬å¹³å®éªŒè®¾è®¡ï¼‰"""
        print(f"ğŸ”¬ å¼€å§‹å®Œæ•´WRENCHå®éªŒ...")
        print(f"è°ƒåº¦å™¨: {list(self.schedulers.keys())}")
        print(f"å·¥ä½œæµè§„æ¨¡: {self.workflow_sizes}")
        print(f"é‡å¤æ¬¡æ•°: {self.repetitions}")
        
        total_experiments = len(self.schedulers) * len(self.workflow_sizes) * self.repetitions
        print(f"æ€»å®éªŒæ•°: {total_experiments} = {len(self.schedulers)}è°ƒåº¦å™¨ Ã— {len(self.workflow_sizes)}ä»»åŠ¡è§„æ¨¡ Ã— {self.repetitions}æ¬¡é‡å¤")
        
        # å…¬å¹³å®éªŒè®¾è®¡ï¼šé¢„ç”Ÿæˆå·¥ä½œæµï¼Œç¡®ä¿æ‰€æœ‰è°ƒåº¦å™¨åœ¨ç›¸åŒå·¥ä½œæµä¸Šæµ‹è¯•
        print("\nğŸ“ é¢„ç”Ÿæˆå·¥ä½œæµï¼ˆç¡®ä¿å…¬å¹³æ¯”è¾ƒï¼‰...")
        workflow_cache = {}
        
        for workflow_size in self.workflow_sizes:
            for rep in range(self.repetitions):
                # ä¸ºæ¯ä¸ªå·¥ä½œæµå¤§å°å’Œé‡å¤æ¬¡æ•°ç”Ÿæˆå›ºå®šçš„å·¥ä½œæµ
                workflow_key = (workflow_size, rep)
                print(f"   ç”Ÿæˆå·¥ä½œæµ: {workflow_size}ä¸ªä»»åŠ¡, é‡å¤{rep+1}")
                
                try:
                    # ç”Ÿæˆå·¥ä½œæµå¹¶ç¼“å­˜
                    workflow = self._generate_workflow(workflow_size, rep)
                    workflow_cache[workflow_key] = workflow
                    print(f"   âœ… å·¥ä½œæµç”ŸæˆæˆåŠŸ")
                except Exception as e:
                    print(f"   âŒ å·¥ä½œæµç”Ÿæˆå¤±è´¥: {e}")
                    workflow_cache[workflow_key] = None
        
        print("\nğŸš€ å¼€å§‹è¿è¡Œå®éªŒ...")
        current_exp = 0
        
        # æŒ‰å·¥ä½œæµå¤§å°å’Œé‡å¤æ¬¡æ•°åˆ†ç»„ï¼Œç¡®ä¿å…¬å¹³æ€§
        for workflow_size in self.workflow_sizes:
            for rep in range(self.repetitions):
                # è·å–é¢„ç”Ÿæˆçš„å·¥ä½œæµ
                workflow_key = (workflow_size, rep)
                workflow = workflow_cache[workflow_key]
                
                if workflow is None:
                    print(f"   âš ï¸ è·³è¿‡æ— æ•ˆå·¥ä½œæµ: {workflow_key}")
                    continue
                
                # å¯¹åŒä¸€å·¥ä½œæµæµ‹è¯•æ‰€æœ‰è°ƒåº¦å™¨
                for scheduler_name in self.schedulers.keys():
                    current_exp += 1
                    print(f"\nè¿›åº¦: {current_exp}/{total_experiments}")
                    print(f"   å·¥ä½œæµ: {workflow_size}ä¸ªä»»åŠ¡, é‡å¤{rep+1}")
                    print(f"   è°ƒåº¦å™¨: {scheduler_name}")
                    
                    try:
                        # ä½¿ç”¨é¢„ç”Ÿæˆçš„å·¥ä½œæµè¿è¡Œå®éªŒ
                        result = self.run_single_experiment_with_workflow(
                            scheduler_name, workflow, workflow_size, current_exp
                        )
                        self.results.append(result)
                        print(f"   âœ… å®Œæˆ: {result.makespan:.2f}s")
                    except Exception as e:
                        print(f"   âŒ å®éªŒå¤±è´¥: {e}")
        
        # ä¿å­˜ç»“æœ
        self._save_results()
        self._analyze_results()
        
        print(f"\nğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆï¼å…±è¿è¡Œ {len(self.results)} æ¬¡å®éªŒ")
        return self.results
    
    def _save_results(self):
        """ä¿å­˜å®éªŒç»“æœ"""
        results_dir = Path("results/wrench_experiments")
        results_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_data = {
            "experiment_config": {
                "schedulers": list(self.schedulers.keys()),
                "workflow_sizes": self.workflow_sizes,
                "repetitions": self.repetitions,
                "total_experiments": len(self.results)
            },
            "results": [asdict(result) for result in self.results]
        }
        
        with open(results_dir / "detailed_results.json", 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š å®éªŒç»“æœå·²ä¿å­˜åˆ° {results_dir}")
    
    def _analyze_results(self):
        """åˆ†æå®éªŒç»“æœ"""
        if not self.results:
            print("âŒ æ²¡æœ‰å®éªŒç»“æœå¯åˆ†æ")
            return
        
        print(f"\nğŸ“ˆ å®éªŒç»“æœåˆ†æ:")
        print("=" * 60)
        
        # æŒ‰è°ƒåº¦å™¨åˆ†ç»„ç»Ÿè®¡
        scheduler_stats = {}
        for result in self.results:
            name = result.scheduler_name
            if name not in scheduler_stats:
                scheduler_stats[name] = []
            scheduler_stats[name].append(result.makespan)
        
        # æ˜¾ç¤ºç»Ÿè®¡ç»“æœ
        print(f"{'è°ƒåº¦å™¨':<15} {'å¹³å‡Makespan':<15} {'æ ‡å‡†å·®':<10} {'æœ€ä½³':<10} {'å®éªŒæ¬¡æ•°':<8}")
        print("-" * 60)
        
        for scheduler_name, makespans in scheduler_stats.items():
            avg_makespan = np.mean(makespans)
            std_makespan = np.std(makespans)
            best_makespan = min(makespans)
            count = len(makespans)
            
            print(f"{scheduler_name:<15} {avg_makespan:<15.2f} {std_makespan:<10.2f} {best_makespan:<10.2f} {count:<8}")
        
        # æ‰¾å‡ºæœ€ä½³è°ƒåº¦å™¨
        best_scheduler = min(scheduler_stats.keys(), 
                           key=lambda x: np.mean(scheduler_stats[x]))
        best_avg = np.mean(scheduler_stats[best_scheduler])
        
        print(f"\nğŸ† æœ€ä½³è°ƒåº¦å™¨: {best_scheduler} (å¹³å‡Makespan: {best_avg:.2f}s)")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹åŸºäºWRENCHçš„çœŸå®WASS-RAGå®éªŒ...")
    
    runner = WRENCHExperimentRunner()
    runner.run_all_experiments()
    
    print("\nğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆ!")

if __name__ == "__main__":
    main()
