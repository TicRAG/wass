#!/usr/bin/env python3
"""
åŸºäºWRENCHçš„çœŸå®WASS-RAGå®éªŒæ¡†æ¶
ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨çœŸå®WRENCHç¯å¢ƒä¸­è¿›è¡Œæ€§èƒ½å¯¹æ¯”å®éªŒ
"""

import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

print(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")

from src.ai_schedulers import WASSRAGScheduler

import sys
import os
import json
import time
import random
import numpy as np
import torch
import torch.nn as nn
import pickle
from pathlib import Path
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Tuple
import yaml
from datetime import datetime
import networkx as nx

# ç¡®ä¿èƒ½å¯¼å…¥WRENCH
try:
    import wrench
except ImportError:
    print("Error: WRENCH not available. Please install wrench-python-api.")
    sys.exit(1)

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, str(parent_dir))

def load_config(cfg_path: str) -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(cfg_path, 'r', encoding='utf-8') as f:
        cfg = yaml.safe_load(f) or {}
    
    # Process includes
    if 'include' in cfg:
        base_dir = os.path.dirname(cfg_path)
        for include_file in cfg['include']:
            include_path = os.path.join(base_dir, include_file)
            if os.path.exists(include_path):
                with open(include_path, 'r', encoding='utf-8') as f:
                    include_cfg = yaml.safe_load(f) or {}
                    for key, value in include_cfg.items():
                        if key not in cfg:
                            cfg[key] = value
    return cfg

@dataclass
class WRENCHExperimentResult:
    """å•æ¬¡WRENCHå®éªŒç»“æœ"""
    scheduler_name: str
    workflow_id: str
    task_count: int
    dependency_count: int
    makespan: float
    cpu_utilization: Dict[str, float]
    task_execution_times: Dict[str, float]
    scheduling_decisions: List[Dict[str, Any]]
    experiment_metadata: Dict[str, Any]

class WRENCHScheduler:
    """åŸºç¡€WRENCHè°ƒåº¦å™¨æ¥å£"""
    
    def __init__(self, name: str):
        self.name = name
    
    def schedule_task(self, task, available_nodes: List[str], node_capacities: Dict, 
                     node_loads: Dict, compute_service) -> str:
        """è°ƒåº¦å•ä¸ªä»»åŠ¡ï¼Œè¿”å›é€‰æ‹©çš„èŠ‚ç‚¹"""
        raise NotImplementedError

class FIFOScheduler(WRENCHScheduler):
    """å…ˆè¿›å…ˆå‡ºè°ƒåº¦å™¨"""
    
    def __init__(self):
        super().__init__("FIFO")
    
    def schedule_task(self, task, available_nodes, node_capacities, node_loads, compute_service):
        # é€‰æ‹©è´Ÿè½½æœ€å°çš„èŠ‚ç‚¹
        return min(available_nodes, key=lambda x: node_loads.get(x, 0))

class HEFTScheduler(WRENCHScheduler):
    """å¼‚æ„æœ€æ—©å®Œæˆæ—¶é—´è°ƒåº¦å™¨"""
    
    def __init__(self):
        super().__init__("HEFT")
    
    def schedule_task(self, task, available_nodes, node_capacities, node_loads, compute_service):
        # é€‰æ‹©èƒ½æœ€æ—©å®Œæˆä»»åŠ¡çš„èŠ‚ç‚¹
        best_node = None
        best_finish_time = float('inf')
        
        for node in available_nodes:
            capacity = node_capacities.get(node, 1.0)
            load = node_loads.get(node, 0.0)
            exec_time = task.get_flops() / (capacity * 1e9)
            finish_time = load + exec_time
            
            if finish_time < best_finish_time:
                best_finish_time = finish_time
                best_node = node
        
        return best_node or available_nodes[0]

    def predict_makespan(self, task, available_nodes, node_capacities, node_loads):
        """é¢„æµ‹ä»»åŠ¡åœ¨ç»™å®šèŠ‚ç‚¹é…ç½®ä¸‹çš„makespan (ä»WASS-Heuristicå¤åˆ¶è€Œæ¥)"""
        try:
            task_flops = float(getattr(task, 'get_flops', lambda: 1e9)())
            
            total_capacity = sum(node_capacities.get(node, 1.0) for node in available_nodes)
            total_load = sum(node_loads.get(node, 0.0) for node in available_nodes)
            avg_capacity = total_capacity / len(available_nodes) if available_nodes else 1.0
            avg_load = total_load / len(available_nodes) if available_nodes else 0.0
            
            base_time = task_flops / (avg_capacity * 1e9)
            load_factor = 1.0 + avg_load / max(avg_capacity, 0.1)
            
            try:
                children_count = len(getattr(task, 'get_children', lambda: [])())
                dependency_factor = 1.0 + 0.1 * children_count
            except Exception:
                dependency_factor = 1.0
            
            predicted_makespan = base_time * load_factor * dependency_factor
            return predicted_makespan
        except Exception as e:
            print(f"é¢„æµ‹makespanå¤±è´¥: {e}")
            return 100.0

class WASSHeuristicScheduler(WRENCHScheduler):
    """WASSå¯å‘å¼è°ƒåº¦å™¨ - åœ¨HEFTåŸºç¡€ä¸Šè€ƒè™‘æ•°æ®å±€éƒ¨æ€§"""
    
    def __init__(self, data_locality_weight: float = 0.5):
        super().__init__("WASS-Heuristic")
        self.data_locality_weight = data_locality_weight  # wå‚æ•°
        self.data_location_cache = {}  # æ¨¡æ‹Ÿæ•°æ®ä½ç½®ç¼“å­˜
        
        # èŠ‚ç‚¹æ€§èƒ½å‚æ•°
        self.node_capacities = {
            "ComputeHost1": 2.0,
            "ComputeHost2": 3.0,
            "ComputeHost3": 2.5,
            "ComputeHost4": 4.0
        }
    
    def schedule_task(self, task, available_nodes, node_capacities, node_loads, compute_service):
        """ä½¿ç”¨WASSå¯å‘å¼è¿›è¡Œä»»åŠ¡è°ƒåº¦"""
        best_node = None
        best_score = float('inf')
        
        for node in available_nodes:
            # è®¡ç®—EFT (æœ€æ—©å®Œæˆæ—¶é—´)
            eft = self._calculate_eft(task, node, node_capacities, node_loads)
            
            # è®¡ç®—DRT (æ•°æ®å°±ç»ªæ—¶é—´)
            drt = self._calculate_drt(task, node)
            
            # è®¡ç®—WASSç»¼åˆè¯„åˆ†
            w = self.data_locality_weight
            score = (1 - w) * eft + w * drt
            
            if score < best_score:
                best_score = score
                best_node = node
        
        # æ›´æ–°æ•°æ®ä½ç½®ç¼“å­˜ï¼ˆå‡è®¾ä»»åŠ¡è¾“å‡ºæ•°æ®å­˜å‚¨åœ¨æ‰§è¡ŒèŠ‚ç‚¹ï¼‰
        if best_node:
            self._update_data_location(task, best_node)
        
        return best_node or available_nodes[0]
    
    def _calculate_eft(self, task, node, node_capacities, node_loads):
        """è®¡ç®—ä»»åŠ¡åœ¨æŒ‡å®šèŠ‚ç‚¹ä¸Šçš„æœ€æ—©å®Œæˆæ—¶é—´"""
        capacity = node_capacities.get(node, 1.0)
        load = node_loads.get(node, 0.0)
        exec_time = task.get_flops() / (capacity * 1e9)
        return load + exec_time
    
    def predict_makespan(self, task, available_nodes, node_capacities, node_loads):
        """é¢„æµ‹ä»»åŠ¡åœ¨ç»™å®šèŠ‚ç‚¹é…ç½®ä¸‹çš„makespan"""
        try:
            # ç®€åŒ–çš„makespané¢„æµ‹ï¼šåŸºäºä»»åŠ¡å¤§å°å’ŒèŠ‚ç‚¹è´Ÿè½½
            task_flops = float(getattr(task, 'get_flops', lambda: 1e9)())
            
            # è®¡ç®—å¹³å‡èŠ‚ç‚¹æ€§èƒ½
            total_capacity = sum(node_capacities.get(node, 1.0) for node in available_nodes)
            total_load = sum(node_loads.get(node, 0.0) for node in available_nodes)
            avg_capacity = total_capacity / len(available_nodes) if available_nodes else 1.0
            avg_load = total_load / len(available_nodes) if available_nodes else 0.0
            
            # åŸºç¡€æ‰§è¡Œæ—¶é—´
            base_time = task_flops / (avg_capacity * 1e9)
            
            # è€ƒè™‘è´Ÿè½½å½±å“
            load_factor = 1.0 + avg_load / max(avg_capacity, 0.1)
            
            # è€ƒè™‘ä»»åŠ¡ä¾èµ–ï¼ˆç®€åŒ–å¤„ç†ï¼‰
            try:
                children_count = len(getattr(task, 'get_children', lambda: [])())
                dependency_factor = 1.0 + 0.1 * children_count  # æ¯ä¸ªå­ä»»åŠ¡å¢åŠ 10%çš„æ—¶é—´
            except Exception:
                dependency_factor = 1.0
            
            # é¢„æµ‹çš„makespan
            predicted_makespan = base_time * load_factor * dependency_factor
            
            return predicted_makespan
            
        except Exception as e:
            print(f"é¢„æµ‹makespanå¤±è´¥: {e}")
            return 100.0  # é»˜è®¤å€¼
    
    def _calculate_drt(self, task, node):
        """è®¡ç®—æ•°æ®å°±ç»ªæ—¶é—´ - è€ƒè™‘æ•°æ®ä¼ è¾“å¼€é”€"""
        total_transfer_time = 0.0
        
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶çš„æ•°æ®ä½ç½®
        for input_file in task.get_input_files():
            # ä½¿ç”¨æ–‡ä»¶åè€Œä¸æ˜¯get_id()æ–¹æ³•
            file_id = input_file.get_name() if hasattr(input_file, 'get_name') else str(input_file)
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦åœ¨ç›®æ ‡èŠ‚ç‚¹ä¸Š
            data_location = self._get_data_location(file_id)
            if data_location != node:
                # éœ€è¦ä¼ è¾“æ•°æ®
                file_size = input_file.get_size() if hasattr(input_file, 'get_size') else 1024
                network_bandwidth = 1e9  # 1GB/s å‡è®¾ç½‘ç»œå¸¦å®½
                transfer_time = file_size / network_bandwidth
                total_transfer_time += transfer_time
        
        return total_transfer_time
    
    def _get_data_location(self, file_id):
        """è·å–æ–‡ä»¶çš„æ•°æ®ä½ç½®"""
        if file_id not in self.data_location_cache:
            # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œéšæœºé€‰æ‹©ä¸€ä¸ªä½ç½®ï¼ˆæ¨¡æ‹Ÿåˆå§‹æ•°æ®åˆ†å¸ƒï¼‰
            import random
            self.data_location_cache[file_id] = random.choice(
                ["ComputeHost1", "ComputeHost2", "ComputeHost3", "ComputeHost4"]
            )
        return self.data_location_cache[file_id]
    
    def _update_data_location(self, task, node):
        """æ›´æ–°ä»»åŠ¡è¾“å‡ºæ•°æ®çš„ä½ç½®"""
        for output_file in task.get_output_files():
            # ä½¿ç”¨æ–‡ä»¶åè€Œä¸æ˜¯get_id()æ–¹æ³•
            file_id = output_file.get_name() if hasattr(output_file, 'get_name') else str(output_file)
            self.data_location_cache[file_id] = node

# å®šä¹‰DRLç½‘ç»œç»“æ„
class SimpleDQN(nn.Module):
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):
        super(SimpleDQN, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )
    
    def forward(self, x):
        return self.network(x)

class WASSDRLScheduler(WRENCHScheduler):
    """åŸºäºè®­ç»ƒå¥½çš„DRLæ¨¡å‹çš„è°ƒåº¦å™¨"""
    
    def __init__(self, model_path: str, config_path: str = "configs/experiment.yaml"):
        """åˆå§‹åŒ–WASS-DRLè°ƒåº¦å™¨"""
        self.config = load_config(config_path)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        # å®šä¹‰ç½‘ç»œç»“æ„ (ä¸æ”¹è¿›è®­ç»ƒå™¨ä¿æŒä¸€è‡´)
        self.state_dim = 32  # éœ€ä¸è®­ç»ƒä¿æŒä¸€è‡´
        self.action_dim = 4   # èŠ‚ç‚¹æ•°
        self.hidden_dims = [512, 256, 128, 64] # AdvancedDQN's default
        self.model = self._create_model()
        self.epsilon = 0.1

        # åŠ è½½æ¨¡å‹
        self._load_model(model_path)

        # èŠ‚ç‚¹æ˜ å°„
        self.compute_nodes = ["ComputeHost1", "ComputeHost2", "ComputeHost3", "ComputeHost4"]
    
    def _create_model(self):
        # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…åœ¨æ— torchç¯å¢ƒæ—¶æŠ¥é”™
        from scripts.improved_drl_trainer import AdvancedDQN
        return AdvancedDQN(self.state_dim, self.action_dim, self.hidden_dims).to(self.device)
    
    def _load_model(self, model_path: str):
        """åŠ è½½è®­ç»ƒå¥½çš„DRLæ¨¡å‹, æ‰‹åŠ¨å¤„ç†å°ºå¯¸ä¸åŒ¹é…çš„å±‚"""
        try:
            checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
            
            source_state_dict = checkpoint.get('q_network_state_dict', checkpoint)
            target_state_dict = self.model.state_dict()

            # Create a new state dict for loading
            new_state_dict = {}
            loaded_keys = []
            mismatched_keys = []

            for name, param in source_state_dict.items():
                if name in target_state_dict:
                    if target_state_dict[name].shape == param.shape:
                        new_state_dict[name] = param
                        loaded_keys.append(name)
                    else:
                        mismatched_keys.append(name)
            
            # Load the filtered state dict
            self.model.load_state_dict(new_state_dict, strict=False)
            
            print(f"âœ… DRL model partially loaded. Matched layers: {len(loaded_keys)}. Mismatched layers (ignored): {len(mismatched_keys)}.")
            if mismatched_keys:
                print(f"   - Mismatched layers were: {mismatched_keys}")

            self.model.eval()
            
        except Exception as e:
            print(f"âŒ DRLæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.model = None
    
    def _get_state(self, task, available_nodes, node_capacities, node_loads):
        """è·å–DRLçš„çŠ¶æ€å‘é‡ (å…¼å®¹æ”¹è¿›è®­ç»ƒå™¨ 32 ç»´)
        
        æ³¨æ„ï¼šæ­¤å‡½æ•°åœ¨å®éªŒç¯å¢ƒä¸­è¿è¡Œï¼Œè®¸å¤šåœ¨è®­ç»ƒæ—¶å¯ç”¨çš„è¯¦ç»†çŠ¶æ€ä¸å¯ç”¨ã€‚
        å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨å¯ç”¨çš„æ•°æ®è¿›è¡Œä¼°ç®—ï¼Œå¹¶ä¸ºä¸å¯ç”¨çš„ç‰¹å¾ä½¿ç”¨åˆç†çš„é»˜è®¤å€¼æˆ–0å¡«å……ã€‚
        """
        features = []
        
        # ä»»åŠ¡ç‰¹å¾ (5ç»´)
        try:
            task_flops = float(getattr(task, 'get_flops', lambda: 1e9)())
            # å®éªŒç¯å¢ƒçš„MockTaskæ²¡æœ‰çˆ¶/å­ä»»åŠ¡ä¿¡æ¯ï¼Œç”¨0å¡«å……
            parents_count = 0
            children_count = len(getattr(task, 'get_children', lambda: [])())
        except Exception:
            task_flops = 1e9
            parents_count = 0
            children_count = 0

        features.append(np.log1p(task_flops / 1e9) / 10.0)  # 1. è®¡ç®—å¤§å° (log normalized)
        features.append(parents_count / 5.0)              # 2. çˆ¶ä»»åŠ¡æ•° (normalized)
        features.append(children_count / 5.0)             # 3. å­ä»»åŠ¡æ•° (normalized)
        features.append(0.0)                              # 4. æ˜¯å¦åœ¨å…³é”®è·¯å¾„ (ä¸å¯ç”¨)
        features.append(0.0)                              # 5. æ•°æ®å±€éƒ¨æ€§åˆ†æ•° (ä¸å¯ç”¨)

        # èŠ‚ç‚¹ç‰¹å¾ (16ç»´ = 4 nodes * 4 features)
        max_speed = max(node_capacities.values()) if node_capacities else 4.0
        
        # ç¡®ä¿æˆ‘ä»¬æ€»æ˜¯ä¸º4ä¸ªèŠ‚ç‚¹ç”Ÿæˆç‰¹å¾
        for i in range(4):
            node_id = f"ComputeHost{i+1}"
            if node_id in self.compute_nodes and node_id in node_capacities:
                speed = node_capacities.get(node_id, 0.0)
                load = node_loads.get(node_id, 0.0)
                features.append(speed / max_speed)  # 1. èŠ‚ç‚¹é€Ÿåº¦ (normalized)
                features.append(load)               # 2. èŠ‚ç‚¹å½“å‰è´Ÿè½½
                features.append(load / speed if speed > 0 else 0.0) # 3. å¯ç”¨æ—¶é—´ (ç”¨ è´Ÿè½½/é€Ÿåº¦ ä¼°ç®—)
                features.append(0.0)                # 4. æ•°æ®å¯ç”¨æ€§ (ä¸å¯ç”¨)
            else:
                features.extend([0.0, 0.0, 0.0, 0.0]) # å¡«å……ç¼ºå¤±çš„èŠ‚ç‚¹

        # ç¯å¢ƒç‰¹å¾ (6ç»´)
        features.append(0.5)  # 1. å·¥ä½œæµè¿›åº¦ (æ¨¡æ‹Ÿå€¼)
        features.append(0.0)  # 2. å½“å‰æ—¶é—´ (ä¸å¯ç”¨)
        features.append(0.5)  # 3. å¾…å¤„ç†ä»»åŠ¡æ•° (æ¨¡æ‹Ÿå€¼)
        
        loads = [node_loads.get(f"ComputeHost{i+1}", 0.0) for i in range(4)]
        features.append(np.mean(loads))  # 4. å¹³å‡èŠ‚ç‚¹è´Ÿè½½
        features.append(np.std(loads))   # 5. èŠ‚ç‚¹è´Ÿè½½æ ‡å‡†å·®
        features.append(0.5)  # 6. å…³é”®è·¯å¾„è¿›åº¦ (æ¨¡æ‹Ÿå€¼)

        # æ•°æ®ä¼ è¾“ç‰¹å¾ (5ç»´) - å…¨éƒ¨æ¨¡æ‹Ÿä¸º0ï¼Œå› ä¸ºå®éªŒç¯å¢ƒä¸­æ— æ­¤ä¿¡æ¯
        features.extend([0.0] * 5)
        
        final_features = np.array(features, dtype=np.float32)
        
        # æœ€ç»ˆç»´åº¦æ£€æŸ¥ï¼Œç¡®ä¿ä¸º32ç»´
        if final_features.shape[0] != 32:
            padded_features = np.zeros(32, dtype=np.float32)
            copy_len = min(len(final_features), 32)
            padded_features[:copy_len] = final_features[:copy_len]
            return padded_features
            
        return final_features
    
    def schedule_task(self, task, available_nodes, node_capacities, node_loads, compute_service):
        if self.model is None:
            # æ¨¡å‹æœªåŠ è½½ï¼Œæ”¹ä¸ºå¯å‘å¼å›é€€è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            print("âš ï¸ DRLæ¨¡å‹æœªæ­£ç¡®åŠ è½½ï¼Œä½¿ç”¨å¯å‘å¼å›é€€è°ƒåº¦")
            return self._heuristic_fallback(task, available_nodes, node_capacities, node_loads)

        try:
            state = self._get_state(task, available_nodes, node_capacities, node_loads)
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)

            with torch.no_grad():
                q_values = self.model(state_tensor)
                action = q_values.argmax().item()

            # åŠ¨ä½œæ˜ å°„åˆ°èŠ‚ç‚¹
            if action < len(self.compute_nodes):
                chosen_node = self.compute_nodes[action]
                if chosen_node in available_nodes:
                    return chosen_node

            # å¦‚æœé€‰æ‹©çš„èŠ‚ç‚¹ä¸å¯ç”¨ï¼Œä½¿ç”¨å¯å‘å¼å›é€€
            print("âš ï¸ DRLé€‰æ‹©èŠ‚ç‚¹ä¸å¯ç”¨ï¼Œä½¿ç”¨å¯å‘å¼å›é€€")
            return self._heuristic_fallback(task, available_nodes, node_capacities, node_loads)

        except Exception as e:
            print(f"âš ï¸ DRLè°ƒåº¦å¤±è´¥ï¼Œå°†ä½¿ç”¨å¯å‘å¼å›é€€: {e}")
            return self._heuristic_fallback(task, available_nodes, node_capacities, node_loads)
    
    def _heuristic_fallback(self, task, available_nodes, node_capacities, node_loads):
        """å¯å‘å¼å›é€€è°ƒåº¦ç­–ç•¥"""
        try:
            # è·å–ä»»åŠ¡ç‰¹å¾
            task_flops = float(getattr(task, 'get_flops', lambda: 1e9)())
            
            # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„å¾—åˆ†ï¼ˆè€ƒè™‘å®¹é‡å’Œè´Ÿè½½ï¼‰
            best_node = None
            best_score = -float('inf')
            
            for node in available_nodes:
                capacity = node_capacities.get(node, 1.0)
                load = node_loads.get(node, 0.0)
                
                # è®¡ç®—å¾—åˆ†ï¼šå®¹é‡è¶Šé«˜è¶Šå¥½ï¼Œè´Ÿè½½è¶Šä½è¶Šå¥½
                score = capacity - load * 2.0  # è´Ÿè½½æƒé‡æ›´é«˜
                
                if score > best_score:
                    best_score = score
                    best_node = node
            
            return best_node if best_node else available_nodes[0]
            
        except Exception as e:
            print(f"âš ï¸ å¯å‘å¼å›é€€ä¹Ÿå¤±è´¥: {e}ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨èŠ‚ç‚¹")
            return available_nodes[0]


        
        # ç”Ÿæˆè´Ÿè½½å‡è¡¡å¯¼å‘çš„æ¡ˆä¾‹
        for _ in range(30):  # ç”Ÿæˆ30ä¸ªé¢å¤–æ¡ˆä¾‹
            base_case = random.choice(default_cases)
            variation = base_case.copy()
            
            # æ·»åŠ ä¸€äº›éšæœºå˜åŒ–
            variation['task_flops'] *= random.uniform(0.7, 1.3)
            variation['total_workflow_flops'] *= random.uniform(0.8, 1.2)
            variation['workflow_size'] = max(3, int(variation['workflow_size'] * random.uniform(0.7, 1.3)))
            
            # æ ¹æ®è´Ÿè½½å‡è¡¡åŸåˆ™é€‰æ‹©èŠ‚ç‚¹
            node_loads = {
                'ComputeHost1': random.uniform(0.0, 0.8),
                'ComputeHost2': random.uniform(0.0, 0.8),
                'ComputeHost3': random.uniform(0.0, 0.8),
                'ComputeHost4': random.uniform(0.0, 0.8)
            }
            
            # é€‰æ‹©è´Ÿè½½æœ€ä½çš„èŠ‚ç‚¹ï¼Œä½†è€ƒè™‘ä»»åŠ¡å¤§å°
            if variation['task_flops'] < 3e9:
                # å°ä»»åŠ¡ï¼šä¼˜å…ˆé€‰æ‹©è´Ÿè½½ä½çš„èŠ‚ç‚¹
                sorted_nodes = sorted(node_loads.keys(), key=lambda x: node_loads[x])
                variation['chosen_node'] = sorted_nodes[0]
            elif variation['task_flops'] < 7e9:
                # ä¸­ç­‰ä»»åŠ¡ï¼šåœ¨è´Ÿè½½è¾ƒä½çš„èŠ‚ç‚¹ä¸­é€‰æ‹©å®¹é‡é€‚ä¸­çš„
                low_load_nodes = [n for n in node_loads.keys() if node_loads[n] < 0.5]
                if low_load_nodes:
                    medium_capacity_nodes = [n for n in low_load_nodes if n in ['ComputeHost2', 'ComputeHost3']]
                    if medium_capacity_nodes:
                        variation['chosen_node'] = random.choice(medium_capacity_nodes)
                    else:
                        variation['chosen_node'] = random.choice(low_load_nodes)
                else:
                    variation['chosen_node'] = 'ComputeHost2'
            else:
                # å¤§ä»»åŠ¡ï¼šåœ¨é«˜å®¹é‡èŠ‚ç‚¹ä¸­é€‰æ‹©è´Ÿè½½è¾ƒä½çš„
                high_capacity_nodes = ['ComputeHost3', 'ComputeHost4']
                low_load_high_cap = [n for n in high_capacity_nodes if node_loads[n] < 0.6]
                if low_load_high_cap:
                    variation['chosen_node'] = random.choice(low_load_high_cap)
                else:
                    # å¦‚æœé«˜å®¹é‡èŠ‚ç‚¹éƒ½è´Ÿè½½é«˜ï¼Œé€‰æ‹©è´Ÿè½½æœ€ä½çš„
                    sorted_nodes = sorted(high_capacity_nodes, key=lambda x: node_loads[x])
                    variation['chosen_node'] = sorted_nodes[0]
            
            # æ›´æ–°è´Ÿè½½å‡è¡¡å› å­
            variation['load_balance_factor'] = 1.0 - node_loads[variation['chosen_node']]
            variation['node_load'] = node_loads[variation['chosen_node']]
            
            self.knowledge_base.append(variation)
        
        # æ·»åŠ åŸå§‹é»˜è®¤æ¡ˆä¾‹
        self.knowledge_base.extend(default_cases)
        
        print(f"âœ… å¢å¼ºé»˜è®¤RAGçŸ¥è¯†åº“å·²åˆ›å»º: {len(self.knowledge_base)} ä¸ªæ¡ˆä¾‹ï¼ˆé‡ç‚¹è€ƒè™‘è´Ÿè½½å‡è¡¡ï¼‰")
    
    def schedule_task(self, task, available_nodes, node_capacities, node_loads, compute_service):
        """åŸºäºRAGçŸ¥è¯†åº“å¢å¼ºçš„è°ƒåº¦å†³ç­– - ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆé‡ç‚¹è§£å†³è´Ÿè½½å‡è¡¡é—®é¢˜ï¼‰"""
        try:
            # é¦–å…ˆä½¿ç”¨DRLè¿›è¡ŒåŸºç¡€è°ƒåº¦å†³ç­–
            drl_node = self.drl_scheduler.schedule_task(
                task, available_nodes, node_capacities, node_loads, compute_service
            )
            # å¦‚æœæ²¡æœ‰çŸ¥è¯†åº“ï¼Œç›´æ¥å›é€€åˆ°DRLå†³ç­–
            if not self.knowledge_base:
                print("âš ï¸ RAGçŸ¥è¯†åº“ä¸ºç©ºï¼Œå›é€€åˆ°DRLå†³ç­–")
                return drl_node
            
            # è·å–æ›´ä¸°å¯Œçš„ä»»åŠ¡ç‰¹å¾ç”¨äºRAGåŒ¹é…
            try:
                task_flops = float(getattr(task, 'get_flops', lambda: 1e9)())
                task_memory = float(getattr(task, 'get_memory_requirement', lambda: 1024)())
                
                # è®¡ç®—å·¥ä½œæµç‰¹å¾
                total_workflow_flops = sum(
                    float(getattr(t, 'get_flops', lambda: 1e9)()) 
                    for t in [task]  # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥è·å–æ•´ä¸ªå·¥ä½œæµ
                )
                
                # è®¡ç®—å½“å‰èŠ‚ç‚¹è´Ÿè½½ç‰¹å¾
                avg_load = np.mean([node_loads.get(node, 0) for node in available_nodes])
                max_load = max([node_loads.get(node, 0) for node in available_nodes])
                
                # è®¡ç®—è´Ÿè½½å‡è¡¡æŒ‡æ ‡
                load_variance = np.var([node_loads.get(node, 0) for node in available_nodes])
                load_std = np.sqrt(load_variance)
                
            except Exception as e:
                task_flops = 1e9
                task_memory = 1024
                total_workflow_flops = task_flops
                avg_load = 0
                max_load = 0
                load_variance = 0
                load_std = 0
            
            # å¢å¼ºçš„ç›¸ä¼¼åº¦åŒ¹é… - è¿›ä¸€æ­¥é™ä½é˜ˆå€¼ä»¥è·å–æ›´å¤šåŒ¹é…
            best_matches = []
            min_similarity_threshold = 0.01
            
            for case in self.knowledge_base:
                case_flops = float(case.get('task_flops', 1e9))
                
                # è®¡ç®—å½’ä¸€åŒ–è·ç¦»
                flops_distance = abs(task_flops - case_flops) / max(task_flops, case_flops, 1e-6)
                
                # ç»¼åˆç›¸ä¼¼åº¦ (å·²ç§»é™¤æŸåçš„ workflow_distance)
                similarity = 1.0 - flops_distance
                
                if similarity >= min_similarity_threshold:
                    best_matches.append({
                        'case': case,
                        'similarity': similarity,
                        'suggested_node': str(case.get('chosen_node', drl_node))
                    })
            
            # å¦‚æœæœ‰é«˜è´¨é‡åŒ¹é…ï¼Œå‡†å¤‡èåˆæ‰€éœ€ rag_scores
            rag_scores = []
            match_node_scores = {}
            if best_matches:
                # æŒ‰makespanæ’åºï¼Œé€‰æ‹©makespanæœ€ä½çš„æ¡ˆä¾‹
                best_matches.sort(key=lambda x: float(x['case'].get('makespan', float('inf'))))
                top_matches = best_matches[:8]
                for match in top_matches:
                    node = match['suggested_node']
                    match_node_scores.setdefault(node, 0.0)
                    # ç»¼åˆè€ƒè™‘ç›¸ä¼¼åº¦å’Œmakespanï¼ˆmakespanè¶Šä½ï¼Œæƒé‡è¶Šé«˜ï¼‰
                    makespan = float(match['case'].get('makespan', 100.0))
                    makespan_weight = 1.0 / (1.0 + makespan / 100.0)  # å½’ä¸€åŒ–makespanæƒé‡
                    match_node_scores[node] += match['similarity'] * makespan_weight
                    
                    # è´Ÿè½½å‡è¡¡è°ƒæ•´ï¼šå¦‚æœèŠ‚ç‚¹è´Ÿè½½è¿‡é«˜ï¼Œæå¤§å¹…é™ä½å…¶å¾—åˆ†
                    node_load = node_loads.get(node, 0.0)
                    if node_load > avg_load * 1.05:  # è¿›ä¸€æ­¥é™ä½é˜ˆå€¼ï¼Œä»1.1å€æ”¹ä¸º1.05å€
                        load_penalty = 0.01  # é™ä½99%çš„å¾—åˆ†ï¼ˆä»90%è¿›ä¸€æ­¥å¢å¼ºåˆ°99%ï¼‰
                        match_node_scores[node] *= load_penalty
                        
            for node in available_nodes:
                rag_scores.append(match_node_scores.get(node, 0.0))

            # èåˆå†³ç­–
            if self.enable_fusion and rag_scores:
                try:
                    from src.scheduling.hybrid_fusion import fuse_decision
                    # è·å– DRL æ¨¡å‹çœŸå® Q-values
                    q_values = []
                    try:
                        state = self.drl_scheduler._get_state(task, available_nodes, node_capacities, node_loads)
                        if self.drl_scheduler.model is not None:
                            import torch
                            st = torch.FloatTensor(state).unsqueeze(0).to(self.drl_scheduler.device)
                            with torch.no_grad():
                                q_tensor = self.drl_scheduler.model(st)
                            q_list = q_tensor.squeeze(0).cpu().tolist()
                            # å…¼å®¹æ€§è¡¥ä¸ï¼šå¦‚æœæ¨¡å‹è¾“å‡ºçš„åŠ¨ä½œç©ºé—´æ¯”å¯ç”¨èŠ‚ç‚¹å°‘ï¼Œç”¨å¹³å‡å€¼å¡«å……
                            q_values = q_list
                            while len(q_values) < len(available_nodes):
                                q_values.append(np.mean(q_values) if q_values else 0.0)
                            # ç¡®ä¿æœ€ç»ˆé•¿åº¦ä¸€è‡´
                            q_values = q_values[:len(available_nodes)]
                    except Exception as qe:
                        print(f"è·å–çœŸå®Qå€¼å¤±è´¥ï¼Œå›é€€ä¼ªQ: {qe}")
                    if not q_values:
                        for node in available_nodes:
                            cap = node_capacities.get(node, 1.0)
                            load = node_loads.get(node, 0.0)
                            # æå¼ºå¢å¼ºè´Ÿè½½å‡è¡¡è€ƒè™‘
                            load_balance_factor = 1.0 / (1.0 + 20.0 * load)  # æå¼ºå¢å¼ºè´Ÿè½½å‡è¡¡å› å­
                            q_values.append(cap * load_balance_factor)
                    load_vals = [node_loads.get(n, 0.0) for n in available_nodes]
                    progress = 0.5  # TODO: ä½¿ç”¨çœŸå®è®­ç»ƒè¿›åº¦
                    
                    # è®¡ç®—makespané¢„æµ‹ï¼ˆåŸºäºå†å²æ¡ˆä¾‹å’Œå½“å‰çŠ¶æ€ï¼‰
                    makespan_predictions = []
                    baseline_makespan = None
                    
                    # è·å–åŸºå‡†makespanï¼ˆHEFTç®—æ³•é¢„æµ‹ï¼‰
                    try:
                        heft_scheduler = HEFTScheduler()
                        heft_prediction = heft_scheduler.predict_makespan(task, available_nodes, node_capacities, node_loads)
                        if heft_prediction > 0:
                            baseline_makespan = heft_prediction
                    except Exception as e:
                        print(f"è·å–HEFTåŸºå‡†makespanå¤±è´¥: {e}")
                    
                    # ä¸ºæ¯ä¸ªèŠ‚ç‚¹é¢„æµ‹makespan
                    for node in available_nodes:
                        predicted_makespan = baseline_makespan or 100.0  # é»˜è®¤å€¼
                        
                        # åŸºäºå½“å‰è´Ÿè½½å’Œå®¹é‡è°ƒæ•´é¢„æµ‹
                        node_load = node_loads.get(node, 0.0)
                        node_capacity = node_capacities.get(node, 1.0)
                        load_factor = 1.0 + node_load / max(node_capacity, 0.1)
                        predicted_makespan *= load_factor
                        
                        # åŸºäºRAGåŒ¹é…åº¦è°ƒæ•´é¢„æµ‹ï¼ˆåŒ¹é…åº¦è¶Šé«˜ï¼Œé¢„æµ‹makespanè¶Šä½ï¼‰
                        rag_score = match_node_scores.get(node, 0.0)
                        if rag_score > 0:
                            rag_factor = 1.0 - 0.5 * rag_score  # æœ€é«˜å¯å‡å°‘50%çš„makespan
                            predicted_makespan *= rag_factor
                        
                        makespan_predictions.append(predicted_makespan)
                    
                    # å¢å¼ºè´Ÿè½½å‡è¡¡æƒé‡ï¼ŒåŠ å…¥makespané¢„æµ‹
                    fusion = fuse_decision(
                        q_values, 
                        rag_scores, 
                        load_vals, 
                        progress, 
                        rag_confidence_threshold=0.00001,  # å¤§å¹…é™ä½é˜ˆå€¼ä»¥æ¿€æ´»RAG
                        makespan_predictions=makespan_predictions,
                        baseline_makespan=baseline_makespan
                    )
                    fused_idx = fusion['index']
                    fused_node = available_nodes[fused_idx]
                    print(f"ğŸ”€ èåˆå†³ç­–: {fused_node} (Î±={fusion['alpha']:.2f}, Î²={fusion['beta']:.2f}, Î³={fusion['gamma']:.2f}, Î´={fusion.get('delta', 0.0):.2f})")
                    
                    # è®°å½•èåˆå†³ç­–çš„è¯¦ç»†ä¿¡æ¯
                    try:
                        import json, os
                        os.makedirs('results', exist_ok=True)
                        with open('results/fusion_debug.log', 'a', encoding='utf-8') as fdbg:
                            record = {
                                'node': fused_node,
                                'alpha': fusion['alpha'],
                                'beta': fusion['beta'],
                                'gamma': fusion['gamma'],
                                'delta': fusion.get('delta', 0.0),
                                'q_norm': fusion['q_norm'],
                                'rag_norm': fusion['rag_norm'],
                                'load_pref': fusion['load_pref'],
                                'makespan_scores': fusion.get('makespan_scores', []),
                                'fused': fusion['fused'],
                                'load_variance': load_variance,
                                'load_std': load_std,
                                'avg_load': avg_load,
                                'max_load': max_load,
                                'makespan_predictions': makespan_predictions,
                                'baseline_makespan': baseline_makespan
                            }
                            fdbg.write(json.dumps(record, ensure_ascii=False) + '\n')
                    except Exception as le:
                        print(f"èåˆè°ƒè¯•æ—¥å¿—å†™å…¥å¤±è´¥: {le}")
                    return fused_node
                except Exception as fe:
                    print(f"èåˆå¤±è´¥ï¼Œå›é€€RAG/DRL: {fe}")

            # æ— èåˆæˆ–å¤±è´¥ï¼šä½¿ç”¨å¢å¼ºçš„è´Ÿè½½å‡è¡¡ç­–ç•¥
            if match_node_scores:
                # ç»“åˆç›¸ä¼¼åº¦å’Œè´Ÿè½½å‡è¡¡é€‰æ‹©èŠ‚ç‚¹
                best_node = None
                best_score = -float('inf')
                
                for node in available_nodes:
                    # åŸºç¡€å¾—åˆ†ï¼šRAGç›¸ä¼¼åº¦
                    node_score = match_node_scores.get(node, 0.0)
                    
                    # è´Ÿè½½å‡è¡¡è°ƒæ•´
                    node_load = node_loads.get(node, 0.0)
                    load_balance_factor = 1.0 / (1.0 + 20.0 * load)  # æå¼ºå¢å¼ºè´Ÿè½½å‡è¡¡å› å­
                    
                    # èŠ‚ç‚¹å®¹é‡è€ƒè™‘
                    node_capacity = node_capacities.get(node, 1.0)
                    
                    # ç»¼åˆå¾—åˆ†
                    combined_score = node_score * load_balance_factor * node_capacity
                    
                    if combined_score > best_score:
                        best_score = combined_score
                        best_node = node
                
                if best_node:
                    return best_node
            
            # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„åŒ¹é…ï¼Œä½¿ç”¨å¢å¼ºçš„å¯å‘å¼ç­–ç•¥
            print("âš ï¸ æ— è¶³å¤ŸRAGåŒ¹é…æ¡ˆä¾‹ï¼Œä½¿ç”¨å¢å¼ºå¯å‘å¼ç­–ç•¥")
            best_node = None
            best_score = -float('inf')
            
            for node in available_nodes:
                capacity = node_capacities.get(node, 1.0)
                load = node_loads.get(node, 0.0)
                
                # æå¼ºå¢å¼ºè´Ÿè½½å‡è¡¡å› å­
                load_balance_factor = 1.0 / (1.0 + 20.0 * load)  # æå¼ºå¢å¼ºè´Ÿè½½å‡è¡¡å› å­
                score = capacity * load_balance_factor
                
                if score > best_score:
                    best_score = score
                    best_node = node
            
            return best_node if best_node else available_nodes[0]

        except Exception as e:
            print(f"âš ï¸ RAGè°ƒåº¦å¤±è´¥: {e}ï¼Œå°è¯•å›é€€")
            # ä¼˜å…ˆå°è¯•ç›´æ¥ä½¿ç”¨å·²è·å¾—çš„drl_nodeï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            try:
                if 'drl_node' in locals() and drl_node in available_nodes:
                    return drl_node
            except Exception:
                pass
            # æœ€ç»ˆå›é€€åˆ°å¯å‘å¼
            try:
                return self.drl_scheduler._heuristic_fallback(task, available_nodes, node_capacities, node_loads)
            except Exception:
                # å…œåº•ï¼šè¿”å›ç¬¬ä¸€ä¸ªå¯ç”¨èŠ‚ç‚¹
                return available_nodes[0]

class WRENCHExperimentRunner:
    """åŸºäºçœŸå®WRENCHçš„å®éªŒè¿è¡Œå™¨"""
    
    def __init__(self, config_path: str = "configs/experiment.yaml"):
        self.config = load_config(config_path)
        
        # WRENCHå¹³å°é…ç½®
        self.platform_file = self.config['platform']['platform_file']
        self.controller_host = "ControllerHost"
        
        # èŠ‚ç‚¹é…ç½®
        self.compute_nodes = ["ComputeHost1", "ComputeHost2", "ComputeHost3", "ComputeHost4"]
        self.node_capacities = {
            "ComputeHost1": 2.0,
            "ComputeHost2": 3.0,
            "ComputeHost3": 2.5,
            "ComputeHost4": 4.0
        }
        
        # è°ƒåº¦å™¨é…ç½®
        self.schedulers = self._initialize_schedulers()
        
        # å®éªŒå‚æ•°
        self.workflow_sizes = [5, 10, 15, 20]
        self.repetitions = 3
        
        # ç»“æœå­˜å‚¨
        self.results = []
        
        print(f"ğŸš€ WRENCHå®éªŒè¿è¡Œå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_schedulers(self):
        """åˆå§‹åŒ–æ‰€æœ‰è°ƒåº¦å™¨"""
        schedulers = {
            "FIFO": FIFOScheduler(),
            "HEFT": HEFTScheduler(),
            "WASS-Heuristic": WASSHeuristicScheduler(),  # æ–°å¢WASSå¯å‘å¼è°ƒåº¦å™¨
        }
        
        # æ¨¡å‹æ–‡ä»¶ä¼˜å…ˆçº§ï¼šå…¼å®¹æ¨¡å‹ > åŸå§‹ä¼˜åŒ–æ¨¡å‹ > åŸºç¡€æ¨¡å‹
        model_candidates = [
            "models/improved_wass_drl.pth",  # æ–°è®­ç»ƒæ”¹è¿›æ¨¡å‹
            "models/wass_optimized_models_compatible.pth",
            "models/wass_optimized_models.pth",
            "models/wass_models.pth"
        ]

        # ç¯å¢ƒå˜é‡ä¼˜å…ˆ (WASS_DRL_MODEL)
        env_model = os.environ.get("WASS_DRL_MODEL")
        if env_model:
            if os.path.exists(env_model):
                if env_model not in model_candidates:
                    model_candidates.insert(0, env_model)
                else:
                    # ç¡®ä¿ç¯å¢ƒå˜é‡è·¯å¾„æ’åˆ°é¦–ä½
                    model_candidates.remove(env_model)
                    model_candidates.insert(0, env_model)
                print(f"ğŸ” é€šè¿‡ç¯å¢ƒå˜é‡æŒ‡å®šæ¨¡å‹: {env_model}")
            else:
                print(f"âš ï¸ ç¯å¢ƒå˜é‡WASS_DRL_MODELæŒ‡å®šçš„æ¨¡å‹ä¸å­˜åœ¨: {env_model}")
        
        model_path = None
        print("ğŸ” æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶...")
        for candidate in model_candidates:
            print(f"ğŸ” æ£€æŸ¥æ¨¡å‹æ–‡ä»¶: {candidate}")
            # æ£€æŸ¥ç»å¯¹è·¯å¾„
            abs_candidate = os.path.abspath(candidate)
            print(f"ğŸ” ç»å¯¹è·¯å¾„: {abs_candidate}")
            if os.path.exists(candidate):
                model_path = candidate
                print(f"âœ… æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}")
                break
            elif os.path.exists(abs_candidate):
                model_path = abs_candidate
                print(f"âœ… æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ (ç»å¯¹è·¯å¾„): {model_path}")
                break
        
        if not model_path:
            print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•æ¨¡å‹æ–‡ä»¶")
            for candidate in model_candidates:
                abs_candidate = os.path.abspath(candidate)
                if os.path.exists(candidate):
                    print(f"  å­˜åœ¨: {candidate}")
                elif os.path.exists(abs_candidate):
                    print(f"  å­˜åœ¨ (ç»å¯¹è·¯å¾„): {abs_candidate}")
                else:
                    print(f"  ä¸å­˜åœ¨: {candidate} (ç»å¯¹è·¯å¾„: {abs_candidate})")
        
        rag_path = "data/wrench_rag_knowledge_base.json"
        
        if model_path:
            print(f"ğŸ“ ä½¿ç”¨æ¨¡å‹æ–‡ä»¶: {model_path}")
            
            # å¼ºåˆ¶å¯ç”¨WASS-DRLè°ƒåº¦å™¨
            try:
                # ä»æ¨¡å‹æ–‡ä»¶åŠ è½½DRLä»£ç†
                print("ğŸ” æ­£åœ¨åŠ è½½æ¨¡å‹æ–‡ä»¶...")
                checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)
                print(f"ğŸ” æ¨¡å‹åŠ è½½æˆåŠŸï¼Œæ£€æŸ¥ç‚¹é”®: {list(checkpoint.keys())}")
                
                # ä»æ£€æŸ¥ç‚¹æ•°æ®åˆ›å»ºDRLä»£ç†
                # è·å–æ¨¡å‹é…ç½®ä¿¡æ¯
                config = checkpoint.get('config', {})
                # ä¼˜å…ˆä»metadataè·å–state_dimå’Œaction_dimï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä»configè·å–ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼
                metadata = checkpoint.get('metadata', {})
                state_dim = metadata.get('state_dim', config.get('state_dim', 32))  # é»˜è®¤32ç»´çŠ¶æ€
                action_dim = metadata.get('action_dim', config.get('action_dim', 4))  # é»˜è®¤4ä¸ªåŠ¨ä½œ
                
                # åˆ›å»ºæ–°çš„DRLä»£ç†
                from src.drl_agent import DQNAgent
                drl_agent = DQNAgent(state_dim=state_dim, action_dim=action_dim)
                
                # åŠ è½½æ¨¡å‹æƒé‡
                drl_agent.load(model_path)
                
                node_names = checkpoint.get('node_names', ['node1', 'node2', 'node3'])  # é»˜è®¤èŠ‚ç‚¹å
                predictor = checkpoint.get('predictor', None)
                
                print(f"ğŸ” DRLä»£ç†: {type(drl_agent)}")
                print(f"ğŸ” èŠ‚ç‚¹åç§°: {node_names}")
                print(f"ğŸ” é¢„æµ‹å™¨: {type(predictor)}")
                
                # ä½¿ç”¨å·¥å‚å‡½æ•°åˆ›å»ºè°ƒåº¦å™¨
                from src.ai_schedulers import create_scheduler
                drl_scheduler = create_scheduler('WASS-DRL (w/o RAG)', node_names, drl_agent, predictor)
                schedulers["WASS-DRL"] = drl_scheduler
                print("âœ… WASS-DRLè°ƒåº¦å™¨å·²å¼ºåˆ¶å¯ç”¨")
                
                # å¼ºåˆ¶å¯ç”¨WASS-RAGè°ƒåº¦å™¨
                rag_candidates = [
                    rag_path,
                    "data/wrench_rag_knowledge_base.json",
                    "data/extended_rag_knowledge.json"
                ]
                
                rag_available = False
                for rag_candidate in rag_candidates:
                    if os.path.exists(rag_candidate):
                        try:
                            # æ­£ç¡®åˆå§‹åŒ–WASSRAGScheduler
                            rag_scheduler = WASSRAGScheduler(drl_scheduler.drl_agent, drl_scheduler.node_names, drl_scheduler.predictor, rag_candidate)
                            schedulers["WASS-RAG"] = rag_scheduler
                            print(f"âœ… WASS-RAGè°ƒåº¦å™¨å·²å¯ç”¨ (çŸ¥è¯†åº“: {rag_candidate})")
                            rag_available = True
                            break
                        except Exception as e:
                            print(f"âš ï¸  WASS-RAGä»{rag_candidate}åŠ è½½å¤±è´¥: {e}")
                            continue
                
                if not rag_available:
                    # å³ä½¿æ²¡æœ‰çŸ¥è¯†åº“ï¼Œä¹Ÿåˆ›å»ºç©ºçš„RAGè°ƒåº¦å™¨
                    rag_scheduler = WASSRAGScheduler(drl_scheduler.drl_agent, drl_scheduler.node_names, drl_scheduler.predictor, rag_path)
                    schedulers["WASS-RAG"] = rag_scheduler
                    print("âš ï¸  WASS-RAGè°ƒåº¦å™¨å·²åˆ›å»º (çŸ¥è¯†åº“ä¸ºç©º)")
                    
            except Exception as e:
                print(f"âŒ DRL/RAGè°ƒåº¦å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        else:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ¨¡å‹æ–‡ä»¶ï¼Œä»…ä½¿ç”¨åŸºç¡€è°ƒåº¦å™¨")
        
        print(f"ğŸ”§ å·²å¯ç”¨è°ƒåº¦å™¨: {list(schedulers.keys())}")
        return schedulers
    
    def run_single_experiment_with_workflow(self, scheduler_name: str, workflow, workflow_size: int, experiment_id: int) -> WRENCHExperimentResult:
        """ä½¿ç”¨é¢„ç”Ÿæˆçš„å·¥ä½œæµè¿è¡Œå•ä¸ªå®éªŒ"""
        print(f"    ğŸ”¬ è¿è¡Œå®éªŒ: {scheduler_name} (å·¥ä½œæµå¤§å°: {workflow_size})")
        
        start_time = time.time()
        
        try:
            # è·å–è°ƒåº¦å™¨
            scheduler = self.schedulers[scheduler_name]
            
            # æ¨¡æ‹ŸWRENCHå®éªŒæ‰§è¡Œ
            # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œåº”è¯¥è°ƒç”¨çœŸå®çš„WRENCH API
            simulation_result = self._simulate_wrench_execution(scheduler, workflow, workflow_size)
            
            # åˆ›å»ºå®éªŒç»“æœ
            result = WRENCHExperimentResult(
                scheduler_name=scheduler_name,
                workflow_id=f"workflow_{workflow_size}_{experiment_id}",
                task_count=workflow_size,
                dependency_count=int(workflow_size * 0.8),  # å‡è®¾80%çš„ä»»åŠ¡æœ‰ä¾èµ–
                makespan=simulation_result['makespan'],
                cpu_utilization=simulation_result['cpu_utilization'],
                task_execution_times=simulation_result['task_times'],
                scheduling_decisions=simulation_result['decisions'],
                experiment_metadata={
                    'experiment_id': experiment_id,
                    'workflow_size': workflow_size,
                    'execution_time': time.time() - start_time,
                    'timestamp': datetime.now().isoformat()
                }
            )
            
            return result
            
        except Exception as e:
            # è¿”å›å¤±è´¥ç»“æœ
            return WRENCHExperimentResult(
                scheduler_name=scheduler_name,
                workflow_id=f"workflow_{workflow_size}_{experiment_id}",
                task_count=workflow_size,
                dependency_count=0,
                makespan=float('inf'),
                cpu_utilization={},
                task_execution_times={},
                scheduling_decisions=[],
                experiment_metadata={
                    'experiment_id': experiment_id,
                    'workflow_size': workflow_size,
                    'execution_time': time.time() - start_time,
                    'timestamp': datetime.now().isoformat(),
                    'error': str(e)
                }
            )
    
    def _generate_workflow(self, workflow_size: int, repetition: int) -> Dict:
        """ç”Ÿæˆå›ºå®šçš„å·¥ä½œæµï¼ˆåŸºäºéšæœºç§å­ç¡®ä¿å¯é‡ç°ï¼‰"""
        # è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ç›¸åŒå‚æ•°ç”Ÿæˆç›¸åŒå·¥ä½œæµ
        seed = 42 + workflow_size * 100 + repetition
        random.seed(seed)
        np.random.seed(seed)
        
        # ç”Ÿæˆå·¥ä½œæµç»“æ„
        workflow = {
            'tasks': [],
            'dependencies': [],
            'seed': seed
        }
        
        # ç”Ÿæˆä»»åŠ¡
        for i in range(workflow_size):
            task = {
                'id': f"task_{i}",
                'flops': random.uniform(1e9, 10e9),  # 1-10 GFLOPS
                'memory': random.uniform(1, 8),       # 1-8 GB
                'cores': random.randint(1, 4)         # 1-4 cores
            }
            workflow['tasks'].append(task)
        
        # ç”Ÿæˆä¾èµ–å…³ç³»ï¼ˆDAGç»“æ„ï¼‰
        # ç®€å•å®ç°ï¼šæ¯ä¸ªä»»åŠ¡ä¾èµ–äºä¹‹å‰çš„1-3ä¸ªä»»åŠ¡
        for i in range(1, workflow_size):
            num_deps = min(random.randint(1, 3), i)  # æœ€å¤šä¾èµ–å‰é¢çš„3ä¸ªä»»åŠ¡
            deps = random.sample(range(i), num_deps)
            
            for dep in deps:
                workflow['dependencies'].append({
                    'from': f"task_{dep}",
                    'to': f"task_{i}"
                })
        
        return workflow
    
    def _simulate_wrench_execution(self, scheduler, workflow: Dict, workflow_size: int) -> Dict:
        """
        æ¨¡æ‹ŸWRENCHæ‰§è¡Œï¼ˆä¿®å¤ç‰ˆï¼‰ï¼Œæ­£ç¡®å¤„ç†ä»»åŠ¡ä¾èµ–å’Œè°ƒåº¦ã€‚
        """
        import networkx as nx

        # 1. æ„å»ºä»»åŠ¡å›¾
        g = nx.DiGraph()
        task_map = {t['id']: t for t in workflow['tasks']}
        for task_id in task_map:
            g.add_node(task_id)
        for dep in workflow.get('dependencies', []):
            g.add_edge(dep['from'], dep['to'])

        # 2. åˆå§‹åŒ–çŠ¶æ€
        node_finish_times = {node: 0.0 for node in self.compute_nodes}
        task_finish_times = {}
        decisions = []
        
        completed_tasks = set()
        
        # 3. ä¸»æ¨¡æ‹Ÿå¾ªç¯
        for _ in range(workflow_size):
            # æ‰¾å‡ºå½“å‰å°±ç»ªçš„ä»»åŠ¡ (æ²¡æœ‰æœªå®Œæˆçš„çˆ¶ä»»åŠ¡)
            ready_tasks_ids = []
            for task_id in g.nodes:
                if task_id in completed_tasks:
                    continue
                
                parents = list(g.predecessors(task_id))
                if all(p in completed_tasks for p in parents):
                    ready_tasks_ids.append(task_id)
            
            if not ready_tasks_ids:
                if len(completed_tasks) < workflow_size:
                     break
                continue

            ready_tasks_ids.sort()
            task_to_schedule_id = ready_tasks_ids[0]
            
            task_data = task_map[task_to_schedule_id]

            class MockTask:
                def __init__(self, task_dict, parents):
                    self._task_dict = task_dict
                    self._parents = parents
                def get_id(self): return self._task_dict['id']
                def get_flops(self): return self._task_dict['flops']
                def get_parents(self): return self._parents
                def get_input_files(self): return []
                def get_output_files(self): return []

            mock_task = MockTask(task_data, list(g.predecessors(task_to_schedule_id)))

            data_ready_time = 0.0
            for parent_id in g.predecessors(task_to_schedule_id):
                data_ready_time = max(data_ready_time, task_finish_times.get(parent_id, 0.0))

            node_available_times = node_finish_times.copy()

            chosen_node = scheduler.schedule_task(
                mock_task, self.compute_nodes, self.node_capacities, node_available_times, None
            )

            node_ready_time = node_finish_times.get(chosen_node, 0.0)
            start_time = max(data_ready_time, node_ready_time)
            
            capacity = self.node_capacities[chosen_node]
            exec_time = task_data['flops'] / (capacity * 1e9)
            finish_time = start_time + exec_time

            node_finish_times[chosen_node] = finish_time
            task_finish_times[task_to_schedule_id] = finish_time
            completed_tasks.add(task_to_schedule_id)

            decisions.append({
                'task_id': task_to_schedule_id,
                'chosen_node': chosen_node,
                'execution_time': exec_time,
                'start_time': start_time,
                'end_time': finish_time
            })

        final_makespan = max(task_finish_times.values()) if task_finish_times else 0.0
        
        cpu_utilization = {}
        total_busy_time = {node: 0.0 for node in self.compute_nodes}
        for decision in decisions:
            total_busy_time[decision['chosen_node']] += decision['execution_time']
            
        for node in self.compute_nodes:
            if final_makespan > 0:
                cpu_utilization[node] = total_busy_time[node] / final_makespan
            else:
                cpu_utilization[node] = 0.0

        return {
            'makespan': final_makespan,
            'cpu_utilization': cpu_utilization,
            'task_times': {d['task_id']: d['execution_time'] for d in decisions},
            'decisions': decisions
        }
    
    def run_single_experiment(self, scheduler_name: str, workflow_size: int, experiment_id: int) -> WRENCHExperimentResult:
        """è¿è¡Œå•æ¬¡WRENCHå®éªŒï¼ˆä½¿ç”¨é¢„ç”Ÿæˆçš„å·¥ä½œæµï¼‰"""
        print(f"  è¿è¡Œå®éªŒ: {scheduler_name}, {workflow_size}ä»»åŠ¡, å®éªŒ#{experiment_id}")
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰é¢„ç”Ÿæˆçš„å·¥ä½œæµ
        workflow_key = f"{workflow_size}_{experiment_id}"
        if workflow_key in self.workflow_cache:
            workflow = self.workflow_cache[workflow_key]
            print(f"    ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„å·¥ä½œæµ: {workflow_key}")
        else:
            # ç”Ÿæˆæ–°çš„å·¥ä½œæµå¹¶ç¼“å­˜
            workflow = self._generate_workflow(workflow_size, experiment_id)
            self.workflow_cache[workflow_key] = workflow
            print(f"    ğŸ“‹ ç”Ÿæˆå¹¶ç¼“å­˜æ–°å·¥ä½œæµ: {workflow_key}")
        
        # ä½¿ç”¨é¢„ç”Ÿæˆçš„å·¥ä½œæµè¿è¡Œå®éªŒ
        return self.run_single_experiment_with_workflow(scheduler_name, workflow, workflow_size, experiment_id)
        
    def run_all_experiments(self):
        """è¿è¡Œæ‰€æœ‰å®éªŒé…ç½®ï¼ˆå…¬å¹³å®éªŒè®¾è®¡ï¼‰"""
        print(f"ğŸ”¬ å¼€å§‹å®Œæ•´WRENCHå®éªŒ...")
        print(f"è°ƒåº¦å™¨: {list(self.schedulers.keys())}")
        print(f"å·¥ä½œæµè§„æ¨¡: {self.workflow_sizes}")
        print(f"é‡å¤æ¬¡æ•°: {self.repetitions}")
        
        total_experiments = len(self.schedulers) * len(self.workflow_sizes) * self.repetitions
        print(f"æ€»å®éªŒæ•°: {total_experiments} = {len(self.schedulers)}è°ƒåº¦å™¨ Ã— {len(self.workflow_sizes)}ä»»åŠ¡è§„æ¨¡ Ã— {self.repetitions}æ¬¡é‡å¤")
        
        # å…¬å¹³å®éªŒè®¾è®¡ï¼šé¢„ç”Ÿæˆå·¥ä½œæµï¼Œç¡®ä¿æ‰€æœ‰è°ƒåº¦å™¨åœ¨ç›¸åŒå·¥ä½œæµä¸Šæµ‹è¯•
        print("\nğŸ“ é¢„ç”Ÿæˆå·¥ä½œæµï¼ˆç¡®ä¿å…¬å¹³æ¯”è¾ƒï¼‰...")
        workflow_cache = {}
        
        for workflow_size in self.workflow_sizes:
            for rep in range(self.repetitions):
                # ä¸ºæ¯ä¸ªå·¥ä½œæµå¤§å°å’Œé‡å¤æ¬¡æ•°ç”Ÿæˆå›ºå®šçš„å·¥ä½œæµ
                workflow_key = (workflow_size, rep)
                print(f"   ç”Ÿæˆå·¥ä½œæµ: {workflow_size}ä¸ªä»»åŠ¡, é‡å¤{rep+1}")
                
                try:
                    # ç”Ÿæˆå·¥ä½œæµå¹¶ç¼“å­˜
                    workflow = self._generate_workflow(workflow_size, rep)
                    workflow_cache[workflow_key] = workflow
                    print(f"   âœ… å·¥ä½œæµç”ŸæˆæˆåŠŸ")
                except Exception as e:
                    print(f"   âŒ å·¥ä½œæµç”Ÿæˆå¤±è´¥: {e}")
                    workflow_cache[workflow_key] = None
        
        print("\nğŸš€ å¼€å§‹è¿è¡Œå®éªŒ...")
        current_exp = 0
        
        # æŒ‰å·¥ä½œæµå¤§å°å’Œé‡å¤æ¬¡æ•°åˆ†ç»„ï¼Œç¡®ä¿å…¬å¹³æ€§
        for workflow_size in self.workflow_sizes:
            for rep in range(self.repetitions):
                # è·å–é¢„ç”Ÿæˆçš„å·¥ä½œæµ
                workflow_key = (workflow_size, rep)
                workflow = workflow_cache[workflow_key]
                
                if workflow is None:
                    print(f"   âš ï¸ è·³è¿‡æ— æ•ˆå·¥ä½œæµ: {workflow_key}")
                    continue
                
                # å¯¹åŒä¸€å·¥ä½œæµæµ‹è¯•æ‰€æœ‰è°ƒåº¦å™¨
                for scheduler_name in self.schedulers.keys():
                    current_exp += 1
                    print(f"\nè¿›åº¦: {current_exp}/{total_experiments}")
                    print(f"   å·¥ä½œæµ: {workflow_size}ä¸ªä»»åŠ¡, é‡å¤{rep+1}")
                    print(f"   è°ƒåº¦å™¨: {scheduler_name}")
                    
                    try:
                        # ä½¿ç”¨é¢„ç”Ÿæˆçš„å·¥ä½œæµè¿è¡Œå®éªŒ
                        result = self.run_single_experiment_with_workflow(
                            scheduler_name, workflow, workflow_size, current_exp
                        )
                        self.results.append(result)
                        print(f"   âœ… å®Œæˆ: {result.makespan:.2f}s")
                    except Exception as e:
                        print(f"   âŒ å®éªŒå¤±è´¥: {e}")
        
        # ä¿å­˜ç»“æœ
        self._save_results()
        self._analyze_results()
        
        print(f"\nğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆï¼å…±è¿è¡Œ {len(self.results)} æ¬¡å®éªŒ")
        return self.results
    
    def _save_results(self):
        """ä¿å­˜å®éªŒç»“æœ"""
        results_dir = Path("results/wrench_experiments")
        results_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_data = {
            "experiment_config": {
                "schedulers": list(self.schedulers.keys()),
                "workflow_sizes": self.workflow_sizes,
                "repetitions": self.repetitions,
                "total_experiments": len(self.results)
            },
            "results": [asdict(result) for result in self.results]
        }
        
        with open(results_dir / "detailed_results.json", 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š å®éªŒç»“æœå·²ä¿å­˜åˆ° {results_dir}")
    
    def _analyze_results(self):
        """åˆ†æå®éªŒç»“æœ"""
        if not self.results:
            print("âŒ æ²¡æœ‰å®éªŒç»“æœå¯åˆ†æ")
            return
        
        print(f"\nğŸ“ˆ å®éªŒç»“æœåˆ†æ:")
        print("=" * 60)
        
        # æŒ‰è°ƒåº¦å™¨åˆ†ç»„ç»Ÿè®¡
        scheduler_stats = {}
        for result in self.results:
            name = result.scheduler_name
            if name not in scheduler_stats:
                scheduler_stats[name] = []
            scheduler_stats[name].append(result.makespan)
        
        # æ˜¾ç¤ºç»Ÿè®¡ç»“æœ
        print(f"{'è°ƒåº¦å™¨':<15} {'å¹³å‡Makespan':<15} {'æ ‡å‡†å·®':<10} {'æœ€ä½³':<10} {'å®éªŒæ¬¡æ•°':<8}")
        print("-" * 60)
        
        for scheduler_name, makespans in scheduler_stats.items():
            avg_makespan = np.mean(makespans)
            std_makespan = np.std(makespans)
            best_makespan = min(makespans)
            count = len(makespans)
            
            print(f"{scheduler_name:<15} {avg_makespan:<15.2f} {std_makespan:<10.2f} {best_makespan:<10.2f} {count:<8}")
        
        # æ‰¾å‡ºæœ€ä½³è°ƒåº¦å™¨
        best_scheduler = min(scheduler_stats.keys(), 
                           key=lambda x: np.mean(scheduler_stats[x]))
        best_avg = np.mean(scheduler_stats[best_scheduler])
        
        print(f"\nğŸ† æœ€ä½³è°ƒåº¦å™¨: {best_scheduler} (å¹³å‡Makespan: {best_avg:.2f}s)")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹åŸºäºWRENCHçš„çœŸå®WASS-RAGå®éªŒ...")
    
    runner = WRENCHExperimentRunner()
    runner.run_all_experiments()
    
    print("\nğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆ!")

if __name__ == "__main__":
    main()
