#!/usr/bin/env python3
"""
åŸºäºWRENCHçš„çœŸå®WASS-RAGå®éªŒæ¡†æ¶
ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨çœŸå®WRENCHç¯å¢ƒä¸­è¿›è¡Œæ€§èƒ½å¯¹æ¯”å®éªŒ
"""

import sys
import os
import json
import time
import random
import numpy as np
import torch
import torch.nn as nn
import pickle
from pathlib import Path
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Tuple
import yaml

# ç¡®ä¿èƒ½å¯¼å…¥WRENCH
try:
    import wrench
except ImportError:
    print("Error: WRENCH not available. Please install wrench-python-api.")
    sys.exit(1)

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, str(parent_dir))

def load_config(cfg_path: str) -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(cfg_path, 'r', encoding='utf-8') as f:
        cfg = yaml.safe_load(f) or {}
    
    # Process includes
    if 'include' in cfg:
        base_dir = os.path.dirname(cfg_path)
        for include_file in cfg['include']:
            include_path = os.path.join(base_dir, include_file)
            if os.path.exists(include_path):
                with open(include_path, 'r', encoding='utf-8') as f:
                    include_cfg = yaml.safe_load(f) or {}
                    for key, value in include_cfg.items():
                        if key not in cfg:
                            cfg[key] = value
    return cfg

@dataclass
class WRENCHExperimentResult:
    """å•æ¬¡WRENCHå®éªŒç»“æœ"""
    scheduler_name: str
    workflow_id: str
    task_count: int
    dependency_count: int
    makespan: float
    cpu_utilization: Dict[str, float]
    task_execution_times: Dict[str, float]
    scheduling_decisions: List[Dict[str, Any]]
    experiment_metadata: Dict[str, Any]

class WRENCHScheduler:
    """åŸºç¡€WRENCHè°ƒåº¦å™¨æ¥å£"""
    
    def __init__(self, name: str):
        self.name = name
    
    def schedule_task(self, task, available_nodes: List[str], node_capacities: Dict, 
                     node_loads: Dict, compute_service) -> str:
        """è°ƒåº¦å•ä¸ªä»»åŠ¡ï¼Œè¿”å›é€‰æ‹©çš„èŠ‚ç‚¹"""
        raise NotImplementedError

class FIFOScheduler(WRENCHScheduler):
    """å…ˆè¿›å…ˆå‡ºè°ƒåº¦å™¨"""
    
    def __init__(self):
        super().__init__("FIFO")
    
    def schedule_task(self, task, available_nodes, node_capacities, node_loads, compute_service):
        # é€‰æ‹©è´Ÿè½½æœ€å°çš„èŠ‚ç‚¹
        return min(available_nodes, key=lambda x: node_loads.get(x, 0))

class HEFTScheduler(WRENCHScheduler):
    """å¼‚æ„æœ€æ—©å®Œæˆæ—¶é—´è°ƒåº¦å™¨"""
    
    def __init__(self):
        super().__init__("HEFT")
    
    def schedule_task(self, task, available_nodes, node_capacities, node_loads, compute_service):
        # é€‰æ‹©èƒ½æœ€æ—©å®Œæˆä»»åŠ¡çš„èŠ‚ç‚¹
        best_node = None
        best_finish_time = float('inf')
        
        for node in available_nodes:
            capacity = node_capacities.get(node, 1.0)
            load = node_loads.get(node, 0.0)
            exec_time = task.get_flops() / (capacity * 1e9)
            finish_time = load + exec_time
            
            if finish_time < best_finish_time:
                best_finish_time = finish_time
                best_node = node
        
        return best_node or available_nodes[0]

class WASSHeuristicScheduler(WRENCHScheduler):
    """WASSå¯å‘å¼è°ƒåº¦å™¨ - åœ¨HEFTåŸºç¡€ä¸Šè€ƒè™‘æ•°æ®å±€éƒ¨æ€§"""
    
    def __init__(self, data_locality_weight: float = 0.5):
        super().__init__("WASS-Heuristic")
        self.data_locality_weight = data_locality_weight  # wå‚æ•°
        self.data_location_cache = {}  # æ¨¡æ‹Ÿæ•°æ®ä½ç½®ç¼“å­˜
        
        # èŠ‚ç‚¹æ€§èƒ½å‚æ•°
        self.node_capacities = {
            "ComputeHost1": 2.0,
            "ComputeHost2": 3.0,
            "ComputeHost3": 2.5,
            "ComputeHost4": 4.0
        }
    
    def schedule_task(self, task, available_nodes, node_capacities, node_loads, compute_service):
        """ä½¿ç”¨WASSå¯å‘å¼è¿›è¡Œä»»åŠ¡è°ƒåº¦"""
        best_node = None
        best_score = float('inf')
        
        for node in available_nodes:
            # è®¡ç®—EFT (æœ€æ—©å®Œæˆæ—¶é—´)
            eft = self._calculate_eft(task, node, node_capacities, node_loads)
            
            # è®¡ç®—DRT (æ•°æ®å°±ç»ªæ—¶é—´)
            drt = self._calculate_drt(task, node)
            
            # è®¡ç®—WASSç»¼åˆè¯„åˆ†
            w = self.data_locality_weight
            score = (1 - w) * eft + w * drt
            
            if score < best_score:
                best_score = score
                best_node = node
        
        # æ›´æ–°æ•°æ®ä½ç½®ç¼“å­˜ï¼ˆå‡è®¾ä»»åŠ¡è¾“å‡ºæ•°æ®å­˜å‚¨åœ¨æ‰§è¡ŒèŠ‚ç‚¹ï¼‰
        if best_node:
            self._update_data_location(task, best_node)
        
        return best_node or available_nodes[0]
    
    def _calculate_eft(self, task, node, node_capacities, node_loads):
        """è®¡ç®—ä»»åŠ¡åœ¨æŒ‡å®šèŠ‚ç‚¹ä¸Šçš„æœ€æ—©å®Œæˆæ—¶é—´"""
        capacity = node_capacities.get(node, 1.0)
        load = node_loads.get(node, 0.0)
        exec_time = task.get_flops() / (capacity * 1e9)
        return load + exec_time
    
    def _calculate_drt(self, task, node):
        """è®¡ç®—æ•°æ®å°±ç»ªæ—¶é—´ - è€ƒè™‘æ•°æ®ä¼ è¾“å¼€é”€"""
        total_transfer_time = 0.0
        
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶çš„æ•°æ®ä½ç½®
        for input_file in task.get_input_files():
            # ä½¿ç”¨æ–‡ä»¶åè€Œä¸æ˜¯get_id()æ–¹æ³•
            file_id = input_file.get_name() if hasattr(input_file, 'get_name') else str(input_file)
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦åœ¨ç›®æ ‡èŠ‚ç‚¹ä¸Š
            data_location = self._get_data_location(file_id)
            if data_location != node:
                # éœ€è¦ä¼ è¾“æ•°æ®
                file_size = input_file.get_size() if hasattr(input_file, 'get_size') else 1024
                network_bandwidth = 1e9  # 1GB/s å‡è®¾ç½‘ç»œå¸¦å®½
                transfer_time = file_size / network_bandwidth
                total_transfer_time += transfer_time
        
        return total_transfer_time
    
    def _get_data_location(self, file_id):
        """è·å–æ–‡ä»¶çš„æ•°æ®ä½ç½®"""
        if file_id not in self.data_location_cache:
            # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œéšæœºé€‰æ‹©ä¸€ä¸ªä½ç½®ï¼ˆæ¨¡æ‹Ÿåˆå§‹æ•°æ®åˆ†å¸ƒï¼‰
            import random
            self.data_location_cache[file_id] = random.choice(
                ["ComputeHost1", "ComputeHost2", "ComputeHost3", "ComputeHost4"]
            )
        return self.data_location_cache[file_id]
    
    def _update_data_location(self, task, node):
        """æ›´æ–°ä»»åŠ¡è¾“å‡ºæ•°æ®çš„ä½ç½®"""
        for output_file in task.get_output_files():
            # ä½¿ç”¨æ–‡ä»¶åè€Œä¸æ˜¯get_id()æ–¹æ³•
            file_id = output_file.get_name() if hasattr(output_file, 'get_name') else str(output_file)
            self.data_location_cache[file_id] = node

class WASSDRLScheduler(WRENCHScheduler):
    """åŸºäºè®­ç»ƒå¥½çš„DRLæ¨¡å‹çš„è°ƒåº¦å™¨"""
    
    def __init__(self, model_path: str, config_path: str = "configs/experiment.yaml"):
        """åˆå§‹åŒ–WASS-DRLè°ƒåº¦å™¨"""
        self.config = load_config(config_path)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # å®šä¹‰ç½‘ç»œç»“æ„
        self.state_dim = 17  # ä¸è®­ç»ƒæ—¶ä¸€è‡´
        self.action_dim = 4   # 4ä¸ªèŠ‚ç‚¹
        
        # å…ˆåˆ›å»ºæ¨¡å‹ï¼Œå†åŠ è½½æƒé‡
        self.model = self._create_model()
        self.epsilon = 0.1
        
        # åŠ è½½æ¨¡å‹
        self._load_model(model_path)
        
        # èŠ‚ç‚¹æ˜ å°„
        self.compute_nodes = ["ComputeHost1", "ComputeHost2", "ComputeHost3", "ComputeHost4"]
    
    def _create_model(self):
        """åˆ›å»ºDRLæ¨¡å‹"""
        return SimpleDQN(self.state_dim, self.action_dim).to(self.device)
    
    def _load_model(self, model_path: str):
        """åŠ è½½è®­ç»ƒå¥½çš„DRLæ¨¡å‹"""
        try:
            # ä¿®å¤PyTorch 2.6å…¼å®¹æ€§é—®é¢˜
            checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
            
            # æ£€æŸ¥æ¨¡å‹æ ¼å¼
            if 'drl_agent' in checkpoint:
                agent_state = checkpoint['drl_agent']
                self.model.load_state_dict(agent_state['model_state_dict'])
                self.epsilon = agent_state.get('epsilon', 0.1)
                print(f"âœ… DRLæ¨¡å‹åŠ è½½æˆåŠŸ (è®­ç»ƒè½®æ•°: {agent_state.get('training_episodes', 'unknown')})")
            else:
                # å…¼å®¹æ—§æ ¼å¼
                self.model.load_state_dict(checkpoint)
                print("âœ… DRLæ¨¡å‹åŠ è½½æˆåŠŸ (æ—§æ ¼å¼)")
                
            self.model.eval()
            
        except Exception as e:
            print(f"âŒ DRLæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.model = None
    
    def _get_state(self, task, available_nodes, node_capacities, node_loads):
        """è·å–DRLçš„çŠ¶æ€å‘é‡"""
        state = []
        
        # ä»»åŠ¡ç‰¹å¾ (4ç»´)
        try:
            # ä½¿ç”¨WRENCH APIçš„æ­£ç¡®æ–¹æ³•è·å–ä»»åŠ¡ä¿¡æ¯
            task_flops = float(getattr(task, 'get_flops', lambda: 1e9)()) / 1e9  # ä»»åŠ¡è®¡ç®—é‡ (GFLOPS)
            
            # å°è¯•è·å–å†…å­˜éœ€æ±‚ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼
            try:
                task_memory = float(task.get_memory_requirement()) / 1e9  # å†…å­˜éœ€æ±‚ (GB)
            except (AttributeError, TypeError):
                task_memory = 1.0  # é»˜è®¤å€¼
            
            task_cores = float(getattr(task, 'get_min_num_cores', lambda: 1)())  # æœ€å°æ ¸å¿ƒæ•°
            task_children = float(len(getattr(task, 'get_children', lambda: [])()))  # å­ä»»åŠ¡æ•°
            
            state.extend([task_flops, task_memory, task_cores, task_children])
            
        except Exception as e:
            # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
            state.extend([1.0, 1.0, 1.0, 0.0])
        
        # èŠ‚ç‚¹ç‰¹å¾ (æ¯èŠ‚ç‚¹3ç»´ï¼Œæœ€å¤š4ä¸ªèŠ‚ç‚¹ = 12ç»´)
        for i, node in enumerate(available_nodes[:4]):  # é™åˆ¶æœ€å¤š4ä¸ªèŠ‚ç‚¹
            node_capacity = node_capacities.get(node, 1.0)
            node_load = node_loads.get(node, 0.0)
            
            state.extend([
                float(node_capacity),  # èŠ‚ç‚¹å®¹é‡
                float(node_load),  # èŠ‚ç‚¹è´Ÿè½½
                float(node_load / max(node_capacity, 1e-6))  # è´Ÿè½½ç‡
            ])
        
        # å¡«å……ä¸è¶³çš„èŠ‚ç‚¹ç»´åº¦
        while len(state) < 16:  # 4(ä»»åŠ¡) + 4*3(èŠ‚ç‚¹) = 16ç»´
            state.append(0.0)
        
        # å…¨å±€ç‰¹å¾ (1ç»´)
        avg_load = sum(node_loads.values()) / max(len(node_loads), 1)
        state.append(float(avg_load))
        
        # ç¡®ä¿çŠ¶æ€ç»´åº¦ä¸º17ç»´
        if len(state) > 17:
            state = state[:17]
        elif len(state) < 17:
            state.extend([0.0] * (17 - len(state)))
        
        return np.array(state, dtype=np.float32)
    
    def schedule_task(self, task, available_nodes, node_capacities, node_loads, compute_service):
        if self.model is None:
            # æ¨¡å‹æœªåŠ è½½ï¼ŒæŠ›å‡ºå¼‚å¸¸
            raise RuntimeError("DRLæ¨¡å‹æœªæ­£ç¡®åŠ è½½ï¼Œæ— æ³•è¿›è¡Œè°ƒåº¦")
        
        try:
            state = self._get_state(task, available_nodes, node_capacities, node_loads)
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                q_values = self.model(state_tensor)
                action = q_values.argmax().item()
            
            # æ˜ å°„åŠ¨ä½œåˆ°èŠ‚ç‚¹
            if action < len(self.compute_nodes):
                chosen_node = self.compute_nodes[action]
                if chosen_node in available_nodes:
                    return chosen_node
            
            # å¦‚æœé€‰æ‹©çš„èŠ‚ç‚¹ä¸å¯ç”¨ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨èŠ‚ç‚¹
            return available_nodes[0]
            
        except Exception as e:
            raise RuntimeError(f"DRLè°ƒåº¦å¤±è´¥: {e}")

class WASSRAGScheduler(WRENCHScheduler):
    """åŸºäºRAGçŸ¥è¯†åº“å¢å¼ºçš„è°ƒåº¦å™¨"""
    
    def __init__(self, model_path: str, rag_path: str):
        super().__init__("WASS-RAG")
        self.drl_scheduler = WASSDRLScheduler(model_path)
        self.knowledge_base = None
        self._load_rag_knowledge(rag_path)
    
    def _load_rag_knowledge(self, rag_path: str):
        """åŠ è½½RAGçŸ¥è¯†åº“"""
        self.knowledge_base = []
        
        # æ–¹æ³•1: ä½¿ç”¨æ‰©å±•çš„JSONçŸ¥è¯†åº“ï¼ˆæœ€ä¼˜å…ˆï¼‰
        extended_json_path = "data/extended_rag_knowledge.json"
        if os.path.exists(extended_json_path):
            try:
                with open(extended_json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # å¤„ç†ä¸åŒæ ¼å¼çš„JSONæ•°æ®
                cases = []
                if isinstance(data, dict):
                    if 'cases' in data:
                        cases = data['cases']
                    elif 'sample_cases' in data:
                        cases = data['sample_cases']
                    else:
                        # å‡è®¾æ•´ä¸ªå°±æ˜¯æ¡ˆä¾‹åˆ—è¡¨
                        cases = list(data.values()) if isinstance(data, dict) else data
                elif isinstance(data, list):
                    cases = data
                
                for case in cases:
                    if isinstance(case, dict):
                        simple_case = {
                            'task_flops': float(case.get('task_flops', case.get('task_execution_time', 1.0) * 2e9)),
                            'chosen_node': str(case.get('chosen_node', 'ComputeHost1')),
                            'scheduler_type': str(case.get('scheduler_type', 'unknown')),
                            'task_execution_time': float(case.get('task_execution_time', 0.0)),
                            'workflow_makespan': float(case.get('workflow_makespan', 0.0)),
                            'node_capacity': float(case.get('node_capacity', 2.0)),
                            'performance_ratio': float(case.get('performance_ratio', 1.0))
                        }
                        self.knowledge_base.append(simple_case)
                
                if self.knowledge_base:
                    print(f"âœ… RAGçŸ¥è¯†åº“å·²ä»æ‰©å±•JSONåŠ è½½: {len(self.knowledge_base)} ä¸ªæ¡ˆä¾‹")
                    return
                    
            except Exception as e:
                print(f"æ‰©å±•JSONåŠ è½½å¤±è´¥: {e}")
        
        # æ–¹æ³•2: ä½¿ç”¨PKLæ–‡ä»¶ï¼ˆå›é€€æ–¹æ¡ˆï¼‰
        try:
            import pickle
            with open(rag_path, 'rb') as f:
                data = pickle.load(f)
            
            # å¤„ç†ä¸åŒæ ¼å¼çš„pickleæ•°æ®
            cases = []
            if isinstance(data, dict):
                cases = data.get('cases', data.get('sample_cases', []))
            elif isinstance(data, list):
                cases = data
            else:
                # å°è¯•ç›´æ¥è¿­ä»£
                try:
                    cases = list(data)
                except:
                    cases = [data]
            
            for case in cases:
                try:
                    if hasattr(case, '__dict__'):
                        # å¤„ç†å¯¹è±¡ç±»å‹
                        case_dict = case.__dict__
                    elif isinstance(case, dict):
                        case_dict = case
                    else:
                        continue
                    
                    simple_case = {
                        'task_flops': float(case_dict.get('task_flops', case_dict.get('task_execution_time', 1.0) * 2e9)),
                        'chosen_node': str(case_dict.get('chosen_node', 'ComputeHost1')),
                        'scheduler_type': str(case_dict.get('scheduler_type', 'unknown')),
                        'task_execution_time': float(case_dict.get('task_execution_time', 0.0))
                    }
                    self.knowledge_base.append(simple_case)
                    
                except Exception as e:
                    continue
            
            if self.knowledge_base:
                print(f"âœ… RAGçŸ¥è¯†åº“å·²ä»PKLåŠ è½½: {len(self.knowledge_base)} ä¸ªæ¡ˆä¾‹")
                return
                
        except Exception as e:
            print(f"PKLåŠ è½½å¤±è´¥: {e}")
        
        # æ–¹æ³•3: ä»æ‰©å±•JSONç›´æ¥è¯»å–ï¼ˆæœ€ç»ˆå›é€€ï¼‰
        try:
            with open("data/extended_rag_knowledge.json", 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ç®€åŒ–å¤„ç†ï¼šä»JSONä¸­æå–ä»»ä½•å¯ç”¨çš„æ¡ˆä¾‹æ•°æ®
            if isinstance(data, list):
                for case in data[:100]:  # é™åˆ¶æ•°é‡é¿å…å†…å­˜é—®é¢˜
                    if isinstance(case, dict):
                        simple_case = {
                            'task_flops': float(case.get('task_flops', 1e9)),
                            'chosen_node': str(case.get('chosen_node', 'ComputeHost1')),
                            'scheduler_type': str(case.get('scheduler_type', 'FIFO'))
                        }
                        self.knowledge_base.append(simple_case)
            
            if self.knowledge_base:
                print(f"âœ… RAGçŸ¥è¯†åº“å·²ä»JSONåŠ è½½(ç®€åŒ–æ¨¡å¼): {len(self.knowledge_base)} ä¸ªæ¡ˆä¾‹")
                return
                
        except Exception as e:
            print(f"æœ€ç»ˆåŠ è½½å¤±è´¥: {e}")
        
        print("âŒ æ— æ³•åŠ è½½ä»»ä½•RAGçŸ¥è¯†åº“ï¼ŒWASS-RAGå°†ä¸å¯ç”¨")
    
    def schedule_task(self, task, available_nodes, node_capacities, node_loads, compute_service):
        """åŸºäºRAGçŸ¥è¯†åº“å¢å¼ºçš„è°ƒåº¦å†³ç­–"""
        try:
            # é¦–å…ˆä½¿ç”¨DRLè¿›è¡ŒåŸºç¡€è°ƒåº¦å†³ç­–
            drl_node = self.drl_scheduler.schedule_task(task, available_nodes, node_capacities, node_loads, compute_service)
            
            # å¦‚æœæ²¡æœ‰çŸ¥è¯†åº“ï¼Œç›´æ¥è¿”å›DRLå†³ç­–
            if not self.knowledge_base or len(self.knowledge_base) == 0:
                return drl_node
            
            # è·å–ä»»åŠ¡ç‰¹å¾ç”¨äºRAGåŒ¹é…
            try:
                task_flops = float(getattr(task, 'get_flops', lambda: 1e9)())
            except:
                task_flops = 1e9
            
            # åœ¨çŸ¥è¯†åº“ä¸­æŸ¥æ‰¾ç›¸ä¼¼ä»»åŠ¡
            best_match = None
            min_distance = float('inf')
            
            for case in self.knowledge_base:
                # ç®€å•çš„ç›¸ä¼¼åº¦åŒ¹é…ï¼šåŸºäºè®¡ç®—é‡
                case_flops = float(case.get('task_flops', 1e9))
                distance = abs(task_flops - case_flops) / max(task_flops, case_flops, 1e-6)
                
                if distance < min_distance and distance < 0.2:  # 20%å®¹å·®
                    min_distance = distance
                    best_match = case
            
            # å¦‚æœæ‰¾åˆ°åŒ¹é…çš„æ¡ˆä¾‹ï¼Œä½¿ç”¨RAGå»ºè®®çš„èŠ‚ç‚¹
            if best_match:
                suggested_node = str(best_match.get('chosen_node', drl_node))
                if suggested_node in available_nodes:
                    return suggested_node
            
            # å¦åˆ™ä½¿ç”¨DRLå†³ç­–
            return drl_node
            
        except Exception as e:
            # ä»»ä½•é”™è¯¯éƒ½å›é€€åˆ°DRLå†³ç­–
            try:
                return self.drl_scheduler.schedule_task(task, available_nodes, node_capacities, node_loads, compute_service)
            except:
                # æœ€ç»ˆå›é€€ï¼šé€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨èŠ‚ç‚¹
                return available_nodes[0] if available_nodes else None

class WRENCHExperimentRunner:
    """åŸºäºçœŸå®WRENCHçš„å®éªŒè¿è¡Œå™¨"""
    
    def __init__(self, config_path: str = "configs/experiment.yaml"):
        self.config = load_config(config_path)
        
        # WRENCHå¹³å°é…ç½®
        self.platform_file = self.config['platform']['platform_file']
        self.controller_host = "ControllerHost"
        
        # èŠ‚ç‚¹é…ç½®
        self.compute_nodes = ["ComputeHost1", "ComputeHost2", "ComputeHost3", "ComputeHost4"]
        self.node_capacities = {
            "ComputeHost1": 2.0,
            "ComputeHost2": 3.0,
            "ComputeHost3": 2.5,
            "ComputeHost4": 4.0
        }
        
        # è°ƒåº¦å™¨é…ç½®
        self.schedulers = self._initialize_schedulers()
        
        # å®éªŒå‚æ•°
        self.workflow_sizes = [5, 10, 15, 20]
        self.repetitions = 3
        
        # ç»“æœå­˜å‚¨
        self.results = []
        
        print(f"ğŸš€ WRENCHå®éªŒè¿è¡Œå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_schedulers(self):
        """åˆå§‹åŒ–æ‰€æœ‰è°ƒåº¦å™¨"""
        schedulers = {
            "FIFO": FIFOScheduler(),
            "HEFT": HEFTScheduler(),
            "WASS-Heuristic": WASSHeuristicScheduler(),  # æ–°å¢WASSå¯å‘å¼è°ƒåº¦å™¨
        }
        
        # ä½¿ç”¨å…¼å®¹çš„æ¨¡å‹æ–‡ä»¶
        model_path = "models/wass_optimized_models_compatible.pth"
        original_model_path = "models/wass_optimized_models.pth"
        rag_path = "data/wrench_rag_knowledge_base.pkl"
        
        # å¦‚æœæ²¡æœ‰å…¼å®¹æ¨¡å‹ï¼Œå°è¯•åˆ›å»ºæˆ–ä½¿ç”¨åŸå§‹æ¨¡å‹
        if not os.path.exists(model_path):
            if os.path.exists(original_model_path):
                print("âš ï¸  ä½¿ç”¨åŸå§‹æ¨¡å‹ï¼Œä½†å¯èƒ½å­˜åœ¨å…¼å®¹æ€§é—®é¢˜")
                model_path = original_model_path
        
        if os.path.exists(model_path):
            # å°è¯•åŠ è½½DRLè°ƒåº¦å™¨
            try:
                drl_scheduler = WASSDRLScheduler(model_path)
                if drl_scheduler.model is not None:
                    schedulers["WASS-DRL"] = drl_scheduler
                    print("âœ… WASS-DRLè°ƒåº¦å™¨å·²å¯ç”¨")
                    
                    # åªæœ‰åœ¨DRLæˆåŠŸåŠ è½½åæ‰å°è¯•RAG
                    if os.path.exists(rag_path):
                        try:
                            rag_scheduler = WASSRAGScheduler(model_path, rag_path)
                            if rag_scheduler.knowledge_base:
                                schedulers["WASS-RAG"] = rag_scheduler
                                print("âœ… WASS-RAGè°ƒåº¦å™¨å·²å¯ç”¨")
                            else:
                                print("âš ï¸  RAGçŸ¥è¯†åº“ä¸ºç©ºï¼Œè·³è¿‡WASS-RAG")
                        except Exception as e:
                            print(f"âš ï¸  WASS-RAGåˆå§‹åŒ–å¤±è´¥: {e}")
                    else:
                        print(f"âš ï¸  RAGçŸ¥è¯†åº“æ–‡ä»¶æœªæ‰¾åˆ°: {rag_path}")
                else:
                    print("âš ï¸  DRLæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè·³è¿‡WASS-DRLå’ŒWASS-RAG")
            except Exception as e:
                print(f"âš ï¸  DRLè°ƒåº¦å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        else:
            print(f"âš ï¸  è®­ç»ƒæ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {model_path}")
        
        print(f"ğŸ”§ å·²å¯ç”¨è°ƒåº¦å™¨: {list(schedulers.keys())}")
        return schedulers
    
    def run_single_experiment(self, scheduler_name: str, workflow_size: int, experiment_id: int) -> WRENCHExperimentResult:
        """è¿è¡Œå•æ¬¡WRENCHå®éªŒ"""
        print(f"  è¿è¡Œå®éªŒ: {scheduler_name}, {workflow_size}ä»»åŠ¡, å®éªŒ#{experiment_id}")
        
        with open(self.platform_file, 'r', encoding='utf-8') as f:
            platform_xml = f.read()
        
        # åˆ›å»ºä»¿çœŸ
        sim = wrench.Simulation()
        sim.start(platform_xml, self.controller_host)
        
        try:
            # åˆ›å»ºæœåŠ¡
            storage_service = sim.create_simple_storage_service("StorageHost", ["/storage"])
            
            compute_resources = {}
            for node in self.compute_nodes:
                compute_resources[node] = (4, 8_589_934_592)  # 4æ ¸, 8GBå†…å­˜
            
            compute_service = sim.create_bare_metal_compute_service(
                "ComputeHost1", compute_resources, "/scratch", {}, {}
            )
            
            # åˆ›å»ºå·¥ä½œæµ
            workflow = sim.create_workflow()
            tasks = []
            files = []
            
            # åˆ›å»ºä»»åŠ¡
            for i in range(workflow_size):
                flops = random.uniform(2e9, 10e9)
                task = workflow.add_task(f"task_{experiment_id}_{i}", flops, 1, 1, 0)
                tasks.append(task)
                
                # åˆ›å»ºè¾“å‡ºæ–‡ä»¶
                if i < workflow_size - 1:
                    output_file = sim.add_file(f"output_{experiment_id}_{i}", random.randint(1024, 10240))
                    task.add_output_file(output_file)
                    files.append(output_file)
            
            # åˆ›å»ºä¾èµ–å…³ç³»
            dependency_count = 0
            for i in range(1, min(workflow_size, len(files) + 1)):
                if i > 1 and random.random() < 0.3:  # 30%æ¦‚ç‡æœ‰ä¾èµ–
                    dep_idx = random.randint(0, i-2)
                    if dep_idx < len(files):
                        tasks[i].add_input_file(files[dep_idx])
                        dependency_count += 1
            
            # ä¸ºæ–‡ä»¶åˆ›å»ºå‰¯æœ¬
            for file in files:
                storage_service.create_file_copy(file)
            
            # è·å–è°ƒåº¦å™¨
            scheduler = self.schedulers[scheduler_name]
            
            # æ‰§è¡Œè°ƒåº¦
            node_loads = {node: 0.0 for node in self.compute_nodes}
            task_execution_times = {}
            scheduling_decisions = []
            
            # æ¨¡æ‹Ÿè°ƒåº¦è¿‡ç¨‹
            ready_tasks = workflow.get_ready_tasks()
            while ready_tasks:
                current_task = ready_tasks[0]
                
                # è°ƒåº¦å†³ç­–
                chosen_node = scheduler.schedule_task(
                    current_task, self.compute_nodes, self.node_capacities, node_loads, compute_service
                )
                
                # è®°å½•è°ƒåº¦å†³ç­–
                scheduling_decisions.append({
                    "task": current_task.get_name(),
                    "node": chosen_node,
                    "scheduler": scheduler_name,
                    "task_flops": current_task.get_flops()
                })
                
                # æäº¤ä½œä¸š
                file_locations = {}
                for f in current_task.get_input_files():
                    file_locations[f] = storage_service
                for f in current_task.get_output_files():
                    file_locations[f] = storage_service
                
                job = sim.create_standard_job([current_task], file_locations)
                compute_service.submit_standard_job(job)
                
                # ç­‰å¾…ä½œä¸šå®Œæˆ
                start_time = sim.get_simulated_time()
                while True:
                    event = sim.wait_for_next_event()
                    if event["event_type"] == "standard_job_completion":
                        completed_job = event["standard_job"]
                        if completed_job == job:
                            break
                    elif event["event_type"] == "simulation_termination":
                        break
                
                end_time = sim.get_simulated_time()
                execution_time = end_time - start_time
                
                # è®°å½•æ‰§è¡Œæ—¶é—´
                task_execution_times[current_task.get_name()] = execution_time
                node_loads[chosen_node] += execution_time
                
                # è·å–ä¸‹ä¸€æ‰¹å°±ç»ªä»»åŠ¡
                ready_tasks = workflow.get_ready_tasks()
            
            # è®¡ç®—æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡
            makespan = sim.get_simulated_time()
            
            # è®¡ç®—CPUåˆ©ç”¨ç‡
            total_work = sum(task_execution_times.values())
            cpu_utilization = {}
            for node in self.compute_nodes:
                node_work = sum(execution_time for task_name, execution_time in task_execution_times.items() 
                               if any(d["task"] == task_name and d["node"] == node for d in scheduling_decisions))
                cpu_utilization[node] = node_work / makespan if makespan > 0 else 0.0
            
            return WRENCHExperimentResult(
                scheduler_name=scheduler_name,
                workflow_id=f"workflow_{experiment_id}",
                task_count=workflow_size,
                dependency_count=dependency_count,
                makespan=makespan,
                cpu_utilization=cpu_utilization,
                task_execution_times=task_execution_times,
                scheduling_decisions=scheduling_decisions,
                experiment_metadata={
                    "experiment_id": experiment_id,
                    "platform": self.platform_file,
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
                }
            )
        
        finally:
            sim.terminate()
    
    def run_all_experiments(self):
        """è¿è¡Œæ‰€æœ‰å®éªŒé…ç½®"""
        print(f"ğŸ”¬ å¼€å§‹å®Œæ•´WRENCHå®éªŒ...")
        print(f"è°ƒåº¦å™¨: {list(self.schedulers.keys())}")
        print(f"å·¥ä½œæµè§„æ¨¡: {self.workflow_sizes}")
        print(f"é‡å¤æ¬¡æ•°: {self.repetitions}")
        
        total_experiments = len(self.schedulers) * len(self.workflow_sizes) * self.repetitions
        current_exp = 0
        
        print(f"æ€»å®éªŒæ•°: {total_experiments} = {len(self.schedulers)}è°ƒåº¦å™¨ Ã— {len(self.workflow_sizes)}ä»»åŠ¡è§„æ¨¡ Ã— {self.repetitions}æ¬¡é‡å¤")
        
        for scheduler_name in self.schedulers.keys():
            for workflow_size in self.workflow_sizes:
                for rep in range(self.repetitions):
                    current_exp += 1
                    print(f"\nè¿›åº¦: {current_exp}/{total_experiments}")
                    
                    try:
                        result = self.run_single_experiment(scheduler_name, workflow_size, current_exp)
                        self.results.append(result)
                        print(f"  âœ… å®Œæˆ: {result.makespan:.2f}s (è°ƒåº¦å™¨: {scheduler_name}, ä»»åŠ¡æ•°: {workflow_size}, é‡å¤: {rep+1})")
                    except Exception as e:
                        print(f"  âŒ å®éªŒå¤±è´¥: {e}")
        
        # ä¿å­˜ç»“æœ
        self._save_results()
        self._analyze_results()
    
    def _save_results(self):
        """ä¿å­˜å®éªŒç»“æœ"""
        results_dir = Path("results/wrench_experiments")
        results_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_data = {
            "experiment_config": {
                "schedulers": list(self.schedulers.keys()),
                "workflow_sizes": self.workflow_sizes,
                "repetitions": self.repetitions,
                "total_experiments": len(self.results)
            },
            "results": [asdict(result) for result in self.results]
        }
        
        with open(results_dir / "detailed_results.json", 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š å®éªŒç»“æœå·²ä¿å­˜åˆ° {results_dir}")
    
    def _analyze_results(self):
        """åˆ†æå®éªŒç»“æœ"""
        if not self.results:
            print("âŒ æ²¡æœ‰å®éªŒç»“æœå¯åˆ†æ")
            return
        
        print(f"\nğŸ“ˆ å®éªŒç»“æœåˆ†æ:")
        print("=" * 60)
        
        # æŒ‰è°ƒåº¦å™¨åˆ†ç»„ç»Ÿè®¡
        scheduler_stats = {}
        for result in self.results:
            name = result.scheduler_name
            if name not in scheduler_stats:
                scheduler_stats[name] = []
            scheduler_stats[name].append(result.makespan)
        
        # æ˜¾ç¤ºç»Ÿè®¡ç»“æœ
        print(f"{'è°ƒåº¦å™¨':<15} {'å¹³å‡Makespan':<15} {'æ ‡å‡†å·®':<10} {'æœ€ä½³':<10} {'å®éªŒæ¬¡æ•°':<8}")
        print("-" * 60)
        
        for scheduler_name, makespans in scheduler_stats.items():
            avg_makespan = np.mean(makespans)
            std_makespan = np.std(makespans)
            best_makespan = min(makespans)
            count = len(makespans)
            
            print(f"{scheduler_name:<15} {avg_makespan:<15.2f} {std_makespan:<10.2f} {best_makespan:<10.2f} {count:<8}")
        
        # æ‰¾å‡ºæœ€ä½³è°ƒåº¦å™¨
        best_scheduler = min(scheduler_stats.keys(), 
                           key=lambda x: np.mean(scheduler_stats[x]))
        best_avg = np.mean(scheduler_stats[best_scheduler])
        
        print(f"\nğŸ† æœ€ä½³è°ƒåº¦å™¨: {best_scheduler} (å¹³å‡Makespan: {best_avg:.2f}s)")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹åŸºäºWRENCHçš„çœŸå®WASS-RAGå®éªŒ...")
    
    runner = WRENCHExperimentRunner()
    runner.run_all_experiments()
    
    print("\nğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆ!")

if __name__ == "__main__":
    main()

# å®šä¹‰DRLç½‘ç»œç»“æ„
class SimpleDQN(nn.Module):
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):
        super(SimpleDQN, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )
    
    def forward(self, x):
        return self.network(x)
