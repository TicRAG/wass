#!/usr/bin/env python3
"""
WRENCHæµ‹è¯•è¿è¡Œè„šæœ¬

è¿™ä¸ªè„šæœ¬ç”¨äºåœ¨æœ‰WRENCHç¯å¢ƒçš„æµ‹è¯•æœºå™¨ä¸Šè¿è¡Œæ‰€æœ‰æµ‹è¯•ã€‚

ä½¿ç”¨æ–¹æ³•:
    python run_wrench_tests.py --all
    python run_wrench_tests.py --basic
    python run_wrench_tests.py --integration
    python run_wrench_tests.py --performance

Author: WASS-RAG Team  
Date: 2024-12
"""

import argparse
import sys
import os
import traceback
from pathlib import Path
from datetime import datetime
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def check_wrench_environment():
    """æ£€æŸ¥WRENCHç¯å¢ƒæ˜¯å¦æ­£ç¡®è®¾ç½®"""
    print("ğŸ” æ£€æŸ¥WRENCHç¯å¢ƒ...")
    
    # æ£€æŸ¥WRENCHå¯¼å…¥
    try:
        import wrench
        print(f"âœ… WRENCH {wrench.__version__} å¯ç”¨")
        return True
    except ImportError as e:
        print(f"âŒ WRENCHå¯¼å…¥å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿WRENCHå·²æ­£ç¡®å®‰è£…å¹¶ä¸”Pythonç»‘å®šå¯ç”¨")
        return False

def run_basic_tests():
    """è¿è¡ŒåŸºç¡€æµ‹è¯•"""
    print("\n" + "="*50)
    print("ğŸ§ª è¿è¡ŒåŸºç¡€æµ‹è¯•")
    print("="*50)
    
    success_count = 0
    total_count = 0
    
    # æµ‹è¯•1: WRENCHæ¨¡å—æµ‹è¯•
    total_count += 1
    print(f"\nğŸ“‹ æµ‹è¯• {total_count}: WRENCHæ¨¡å—åŸºç¡€åŠŸèƒ½")
    try:
        from wrench_integration.simulator import test_wrench_integration
        if test_wrench_integration():
            success_count += 1
            print("âœ… WRENCHæ¨¡å—æµ‹è¯•é€šè¿‡")
        else:
            print("âŒ WRENCHæ¨¡å—æµ‹è¯•å¤±è´¥")
    except Exception as e:
        print(f"âŒ WRENCHæ¨¡å—æµ‹è¯•å¼‚å¸¸: {e}")
        traceback.print_exc()
    
    # æµ‹è¯•2: å¹³å°åˆ›å»ºæµ‹è¯•
    total_count += 1
    print(f"\nğŸ“‹ æµ‹è¯• {total_count}: å¹³å°åˆ›å»ºåŠŸèƒ½")
    try:
        from wrench_integration.simulator import WRENCHSimulator
        
        simulator = WRENCHSimulator()
        platform_config = {
            'hosts': [
                {'id': 'test_node', 'speed': '1Gf', 'cores': 2}
            ],
            'links': [],
            'routes': []
        }
        
        platform_file = simulator.create_platform(platform_config)
        if os.path.exists(platform_file):
            success_count += 1
            print(f"âœ… å¹³å°æ–‡ä»¶åˆ›å»ºæˆåŠŸ: {platform_file}")
        else:
            print("âŒ å¹³å°æ–‡ä»¶åˆ›å»ºå¤±è´¥")
            
    except Exception as e:
        print(f"âŒ å¹³å°åˆ›å»ºæµ‹è¯•å¼‚å¸¸: {e}")
        traceback.print_exc()
    
    # æµ‹è¯•3: å·¥ä½œæµåˆ›å»ºæµ‹è¯•
    total_count += 1
    print(f"\nğŸ“‹ æµ‹è¯• {total_count}: å·¥ä½œæµåˆ›å»ºåŠŸèƒ½")
    try:
        workflow_spec = {
            'name': 'test_workflow',
            'tasks': [
                {
                    'id': 'task1',
                    'flops': 1e9,
                    'bytes_read': 1e6,
                    'bytes_written': 1e6,
                    'dependencies': []
                }
            ]
        }
        
        workflow_id = simulator.create_workflow(workflow_spec)
        if workflow_id == 'test_workflow':
            success_count += 1
            print(f"âœ… å·¥ä½œæµåˆ›å»ºæˆåŠŸ: {workflow_id}")
        else:
            print("âŒ å·¥ä½œæµåˆ›å»ºå¤±è´¥")
            
    except Exception as e:
        print(f"âŒ å·¥ä½œæµåˆ›å»ºæµ‹è¯•å¼‚å¸¸: {e}")
        traceback.print_exc()
    
    print(f"\nğŸ“Š åŸºç¡€æµ‹è¯•ç»“æœ: {success_count}/{total_count} é€šè¿‡")
    return success_count, total_count

def run_integration_tests():
    """è¿è¡Œé›†æˆæµ‹è¯•"""
    print("\n" + "="*50)
    print("ğŸ”§ è¿è¡Œé›†æˆæµ‹è¯•")
    print("="*50)
    
    success_count = 0
    total_count = 0
    
    # æµ‹è¯•1: åŸºç¡€ä»¿çœŸå®éªŒ
    total_count += 1
    print(f"\nğŸ“‹ æµ‹è¯• {total_count}: åŸºç¡€ä»¿çœŸå®éªŒ")
    try:
        from experiments.basic_simulation import run_basic_simulation, get_default_config
        
        config = get_default_config()
        # ç®€åŒ–é…ç½®ä»¥åŠ å¿«æµ‹è¯•
        config['workflows'] = config['workflows'][:1]  # åªæµ‹è¯•ç¬¬ä¸€ä¸ªå·¥ä½œæµ
        
        output_dir = f"test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        os.makedirs(output_dir, exist_ok=True)
        
        results = run_basic_simulation(config, output_dir)
        
        if results and 'workflows' in results:
            success_count += 1
            print(f"âœ… åŸºç¡€ä»¿çœŸå®éªŒæˆåŠŸï¼Œç»“æœä¿å­˜åˆ°: {output_dir}")
        else:
            print("âŒ åŸºç¡€ä»¿çœŸå®éªŒå¤±è´¥")
            
    except Exception as e:
        print(f"âŒ åŸºç¡€ä»¿çœŸå®éªŒå¼‚å¸¸: {e}")
        traceback.print_exc()
    
    # æµ‹è¯•2: WRENCHç›´æ¥æ¥å£æµ‹è¯•
    total_count += 1
    print(f"\nğŸ“‹ æµ‹è¯• {total_count}: WRENCHç›´æ¥æ¥å£")
    try:
        import wrench
        
        # åˆ›å»ºä»¿çœŸ
        simulation = wrench.Simulation()
        print("âœ… WRENCHä»¿çœŸå¯¹è±¡åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•å¹³å°åŠ è½½ï¼ˆéœ€è¦å®é™…çš„å¹³å°æ–‡ä»¶ï¼‰
        test_platform = create_test_platform_file()
        simulation.add_platform(test_platform)
        print("âœ… å¹³å°åŠ è½½æˆåŠŸ")
        
        success_count += 1
        
    except Exception as e:
        print(f"âŒ WRENCHç›´æ¥æ¥å£æµ‹è¯•å¼‚å¸¸: {e}")
        traceback.print_exc()
    
    print(f"\nğŸ“Š é›†æˆæµ‹è¯•ç»“æœ: {success_count}/{total_count} é€šè¿‡")
    return success_count, total_count

def create_test_platform_file():
    """åˆ›å»ºæµ‹è¯•ç”¨çš„å¹³å°æ–‡ä»¶"""
    platform_xml = '''<?xml version="1.0"?>
<!DOCTYPE platform SYSTEM "https://simgrid.org/simgrid.dtd">
<platform version="4.1">
  <zone id="AS0" routing="Full">
    <host id="test_host" speed="1Gf" core="1"/>
  </zone>
</platform>'''
    
    platform_file = "test_platform.xml"
    with open(platform_file, 'w') as f:
        f.write(platform_xml)
    
    return platform_file

def run_performance_tests():
    """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
    print("\n" + "="*50)
    print("âš¡ è¿è¡Œæ€§èƒ½æµ‹è¯•")
    print("="*50)
    
    success_count = 0
    total_count = 0
    
    # æµ‹è¯•1: å¤§å·¥ä½œæµä»¿çœŸ
    total_count += 1
    print(f"\nğŸ“‹ æµ‹è¯• {total_count}: å¤§è§„æ¨¡å·¥ä½œæµä»¿çœŸ")
    try:
        from wrench_integration.simulator import WRENCHSimulator
        
        # åˆ›å»ºå¤§å·¥ä½œæµ
        large_workflow = {
            'name': 'large_test_workflow',
            'tasks': []
        }
        
        # ç”Ÿæˆ100ä¸ªä»»åŠ¡çš„å·¥ä½œæµ
        for i in range(100):
            task = {
                'id': f'task_{i}',
                'flops': 1e8,  # 100 MFlops
                'bytes_read': 1e5,  # 100 KB
                'bytes_written': 1e5,  # 100 KB
                'dependencies': [f'task_{i-1}'] if i > 0 else []
            }
            large_workflow['tasks'].append(task)
        
        simulator = WRENCHSimulator()
        workflow_id = simulator.create_workflow(large_workflow)
        
        print(f"âœ… å¤§å·¥ä½œæµåˆ›å»ºæˆåŠŸ: {len(large_workflow['tasks'])} ä¸ªä»»åŠ¡")
        success_count += 1
        
    except Exception as e:
        print(f"âŒ å¤§å·¥ä½œæµæµ‹è¯•å¼‚å¸¸: {e}")
        traceback.print_exc()
    
    print(f"\nğŸ“Š æ€§èƒ½æµ‹è¯•ç»“æœ: {success_count}/{total_count} é€šè¿‡")
    return success_count, total_count

def generate_test_report(basic_results, integration_results, performance_results):
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    report = {
        'timestamp': datetime.now().isoformat(),
        'environment': {
            'python_version': sys.version,
            'platform': sys.platform,
        },
        'test_results': {
            'basic_tests': {
                'passed': basic_results[0],
                'total': basic_results[1],
                'success_rate': basic_results[0] / basic_results[1] if basic_results[1] > 0 else 0
            },
            'integration_tests': {
                'passed': integration_results[0],
                'total': integration_results[1],
                'success_rate': integration_results[0] / integration_results[1] if integration_results[1] > 0 else 0
            },
            'performance_tests': {
                'passed': performance_results[0],
                'total': performance_results[1],
                'success_rate': performance_results[0] / performance_results[1] if performance_results[1] > 0 else 0
            }
        }
    }
    
    # æ·»åŠ WRENCHç‰ˆæœ¬ä¿¡æ¯
    try:
        import wrench
        report['environment']['wrench_version'] = wrench.__version__
    except:
        report['environment']['wrench_version'] = 'Unknown'
    
    total_passed = basic_results[0] + integration_results[0] + performance_results[0]
    total_tests = basic_results[1] + integration_results[1] + performance_results[1]
    report['overall'] = {
        'passed': total_passed,
        'total': total_tests,
        'success_rate': total_passed / total_tests if total_tests > 0 else 0
    }
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = f"test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"\nğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    return report

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="WRENCHæµ‹è¯•è¿è¡Œå™¨")
    parser.add_argument('--all', action='store_true', help='è¿è¡Œæ‰€æœ‰æµ‹è¯•')
    parser.add_argument('--basic', action='store_true', help='è¿è¡ŒåŸºç¡€æµ‹è¯•')
    parser.add_argument('--integration', action='store_true', help='è¿è¡Œé›†æˆæµ‹è¯•')
    parser.add_argument('--performance', action='store_true', help='è¿è¡Œæ€§èƒ½æµ‹è¯•')
    
    args = parser.parse_args()
    
    if not any([args.all, args.basic, args.integration, args.performance]):
        print("è¯·æŒ‡å®šè¦è¿è¡Œçš„æµ‹è¯•ç±»å‹")
        parser.print_help()
        return 1
    
    print("ğŸš€ WASS-RAG WRENCHæµ‹è¯•å¼€å§‹")
    print("="*60)
    
    # æ£€æŸ¥ç¯å¢ƒ
    if not check_wrench_environment():
        print("âŒ WRENCHç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
        return 1
    
    basic_results = (0, 0)
    integration_results = (0, 0)
    performance_results = (0, 0)
    
    # è¿è¡Œæµ‹è¯•
    if args.all or args.basic:
        basic_results = run_basic_tests()
    
    if args.all or args.integration:
        integration_results = run_integration_tests()
    
    if args.all or args.performance:
        performance_results = run_performance_tests()
    
    # ç”ŸæˆæŠ¥å‘Š
    report = generate_test_report(basic_results, integration_results, performance_results)
    
    # æ‰“å°æ€»ç»“
    print("\n" + "="*60)
    print("ğŸ“‹ æµ‹è¯•æ€»ç»“")
    print("="*60)
    
    total_passed = report['overall']['passed']
    total_tests = report['overall']['total']
    success_rate = report['overall']['success_rate']
    
    print(f"æ€»ä½“ç»“æœ: {total_passed}/{total_tests} é€šè¿‡ ({success_rate:.1%})")
    
    if args.all or args.basic:
        basic = report['test_results']['basic_tests']
        print(f"åŸºç¡€æµ‹è¯•: {basic['passed']}/{basic['total']} é€šè¿‡ ({basic['success_rate']:.1%})")
    
    if args.all or args.integration:
        integration = report['test_results']['integration_tests']
        print(f"é›†æˆæµ‹è¯•: {integration['passed']}/{integration['total']} é€šè¿‡ ({integration['success_rate']:.1%})")
    
    if args.all or args.performance:
        performance = report['test_results']['performance_tests']
        print(f"æ€§èƒ½æµ‹è¯•: {performance['passed']}/{performance['total']} é€šè¿‡ ({performance['success_rate']:.1%})")
    
    if success_rate >= 0.8:
        print("\nğŸ‰ æµ‹è¯•ç»“æœè‰¯å¥½ï¼")
        return 0
    elif success_rate >= 0.5:
        print("\nâš ï¸  æµ‹è¯•ç»“æœä¸€èˆ¬ï¼Œéœ€è¦ä¿®å¤ä¸€äº›é—®é¢˜")
        return 1
    else:
        print("\nâŒ æµ‹è¯•ç»“æœè¾ƒå·®ï¼Œéœ€è¦é‡ç‚¹ä¿®å¤")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
