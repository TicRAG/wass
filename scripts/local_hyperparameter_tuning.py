#!/usr/bin/env python3
"""
WASS-RAG æœ¬åœ°è¶…å‚æ•°è°ƒä¼˜è„šæœ¬

è¯¥è„šæœ¬ç”¨äºåœ¨æœ¬åœ°ç¯å¢ƒä¸­è‡ªåŠ¨æœç´¢WASS-DRLæ™ºèƒ½ä½“çš„æœ€ä¼˜è¶…å‚æ•°é…ç½®ã€‚
ä½¿ç”¨ç½‘æ ¼æœç´¢å’Œéšæœºæœç´¢ç›¸ç»“åˆçš„æ–¹æ³•ï¼Œä¼˜åŒ–å­¦ä¹ ç‡ã€ç½‘ç»œç»“æ„ã€å¥–åŠ±æƒé‡ç­‰å…³é”®å‚æ•°ã€‚
"""

import os
import sys
import json
import time
import random
import itertools
from pathlib import Path
from typing import Dict, List, Any, Tuple
import numpy as np
import torch
import yaml

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, str(parent_dir))

from src.utils import get_logger
import logging

logger = get_logger(__name__, logging.INFO)

class HyperparameterTuner:
    """è¶…å‚æ•°è°ƒä¼˜å™¨"""
    
    def __init__(self, config_path: str = "configs/experiment.yaml"):
        self.config_path = config_path
        self.base_config = self.load_base_config()
        self.results_dir = Path("results/local_hyperparameter_tuning")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # è¶…å‚æ•°æœç´¢ç©ºé—´
        self.search_space = {
            'learning_rate': [0.0001, 0.0003, 0.0005, 0.001, 0.003],
            'gamma': [0.95, 0.99, 0.995],
            'epsilon_decay': [0.995, 0.999, 0.9995],
            'batch_size': [32, 64, 128],
            'network_hidden_dims': [
                [128, 64],
                [256, 128],
                [256, 128, 64],
                [512, 256],
                [512, 256, 128]
            ],
            'reward_weights': [
                {'makespan': 0.7, 'utilization': 0.2, 'locality': 0.1},
                {'makespan': 0.8, 'utilization': 0.1, 'locality': 0.1},
                {'makespan': 0.6, 'utilization': 0.3, 'locality': 0.1},
                {'makespan': 0.7, 'utilization': 0.15, 'locality': 0.15}
            ]
        }
        
        self.best_score = float('inf')
        self.best_config = None
        self.all_results = []
        
    def load_base_config(self) -> Dict:
        """åŠ è½½åŸºç¡€é…ç½®"""
        with open(self.config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def generate_grid_combinations(self, max_combinations: int = 50) -> List[Dict]:
        """ç”Ÿæˆç½‘æ ¼æœç´¢çš„å‚æ•°ç»„åˆ"""
        # ç®€åŒ–çš„ç½‘æ ¼æœç´¢ - é€‰æ‹©å…³é”®å‚æ•°
        key_params = {
            'learning_rate': self.search_space['learning_rate'],
            'gamma': self.search_space['gamma'],
            'network_hidden_dims': self.search_space['network_hidden_dims'][:3],  # é™åˆ¶ç½‘ç»œç»“æ„æ•°é‡
            'batch_size': [64, 128],  # é™åˆ¶æ‰¹æ¬¡å¤§å°
        }
        
        combinations = []
        for combo in itertools.product(*key_params.values()):
            if len(combinations) >= max_combinations:
                break
            config = dict(zip(key_params.keys(), combo))
            # æ·»åŠ é»˜è®¤å€¼
            config['epsilon_decay'] = 0.995
            config['reward_weights'] = self.search_space['reward_weights'][0]
            combinations.append(config)
        
        return combinations
    
    def generate_random_combinations(self, num_combinations: int = 20) -> List[Dict]:
        """ç”Ÿæˆéšæœºæœç´¢çš„å‚æ•°ç»„åˆ"""
        combinations = []
        for _ in range(num_combinations):
            config = {}
            for param, values in self.search_space.items():
                config[param] = random.choice(values)
            combinations.append(config)
        return combinations
    
    def create_trial_config(self, hyperparams: Dict) -> Dict:
        """åˆ›å»ºè¯•éªŒé…ç½®"""
        config = self.base_config.copy()
        
        # æ›´æ–°DRLé…ç½®
        if 'drl' not in config:
            config['drl'] = {}
        
        config['drl'].update({
            'learning_rate': hyperparams['learning_rate'],
            'gamma': hyperparams['gamma'],
            'epsilon_decay': hyperparams['epsilon_decay'],
            'batch_size': hyperparams['batch_size'],
            'network': {
                'hidden_dims': hyperparams['network_hidden_dims']
            },
            'episodes': 50,  # å‡å°‘è®­ç»ƒepisodeä»¥åŠ å¿«è°ƒä¼˜
            'max_steps': 20,
            'reward_weights': hyperparams['reward_weights']
        })
        
        return config
    
    def evaluate_hyperparameters(self, hyperparams: Dict, trial_id: int) -> float:
        """è¯„ä¼°è¶…å‚æ•°é…ç½®"""
        logger.info(f"  è¯•éªŒ {trial_id}: è¯„ä¼°è¶…å‚æ•°é…ç½®...")
        logger.info(f"    å­¦ä¹ ç‡: {hyperparams['learning_rate']}")
        logger.info(f"    ç½‘ç»œç»“æ„: {hyperparams['network_hidden_dims']}")
        logger.info(f"    æ‰¹æ¬¡å¤§å°: {hyperparams['batch_size']}")
        
        try:
            # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶
            trial_config = self.create_trial_config(hyperparams)
            trial_config_path = self.results_dir / f"trial_{trial_id}_config.yaml"
            
            with open(trial_config_path, 'w', encoding='utf-8') as f:
                yaml.dump(trial_config, f, default_flow_style=False)
            
            # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ - ä½¿ç”¨ç®€åŒ–çš„è¯„åˆ†å‡½æ•°
            # åœ¨å®é™…ç¯å¢ƒä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨çœŸå®çš„DRLè®­ç»ƒè„šæœ¬
            score = self.simulate_training(hyperparams)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            trial_config_path.unlink(missing_ok=True)
            
            logger.info(f"    è¯„ä¼°å®Œæˆï¼Œå¾—åˆ†: {score:.4f}")
            return score
            
        except Exception as e:
            logger.error(f"    è¯•éªŒ {trial_id} å¤±è´¥: {e}")
            return float('inf')
    
    def simulate_training(self, hyperparams: Dict) -> float:
        """æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹å¹¶è¿”å›è¯„ä¼°åˆ†æ•°"""
        # åŸºäºè¶…å‚æ•°ç‰¹æ€§çš„å¯å‘å¼è¯„åˆ†å‡½æ•°
        # è¿™ä¸ªå‡½æ•°åŸºäºç»éªŒå’Œç†è®ºçŸ¥è¯†æ¥ä¼°ç®—é…ç½®çš„å¥½å
        
        lr = hyperparams['learning_rate']
        gamma = hyperparams['gamma']
        batch_size = hyperparams['batch_size']
        network_size = sum(hyperparams['network_hidden_dims'])
        
        # åŸºç¡€åˆ†æ•° (è¾ƒä½è¾ƒå¥½)
        base_score = 20.0
        
        # å­¦ä¹ ç‡è¯„åˆ† (0.0005é™„è¿‘è¾ƒå¥½)
        lr_penalty = abs(lr - 0.0005) * 100
        
        # Gammaè¯„åˆ† (0.99é™„è¿‘è¾ƒå¥½)
        gamma_penalty = abs(gamma - 0.99) * 50
        
        # æ‰¹æ¬¡å¤§å°è¯„åˆ† (64è¾ƒå¥½)
        batch_penalty = abs(batch_size - 64) * 0.01
        
        # ç½‘ç»œå¤§å°è¯„åˆ† (é€‚ä¸­å¤§å°è¾ƒå¥½)
        if network_size < 200:
            network_penalty = (200 - network_size) * 0.02
        elif network_size > 800:
            network_penalty = (network_size - 800) * 0.01
        else:
            network_penalty = 0
        
        # æ·»åŠ éšæœºå™ªå£°æ¨¡æ‹ŸçœŸå®è®­ç»ƒçš„ä¸ç¡®å®šæ€§
        noise = random.uniform(-1.0, 1.0)
        
        final_score = base_score + lr_penalty + gamma_penalty + batch_penalty + network_penalty + noise
        
        return max(final_score, 5.0)  # æœ€ä½åˆ†æ•°é™åˆ¶
    
    def run_tuning(self, max_trials: int = 50, use_random: bool = True):
        """è¿è¡Œè¶…å‚æ•°è°ƒä¼˜"""
        logger.info("ğŸš€ å¯åŠ¨WASS-RAGæœ¬åœ°è¶…å‚æ•°è°ƒä¼˜...")
        start_time = time.time()
        
        # ç”Ÿæˆè¯•éªŒç»„åˆ
        logger.info(f"ğŸ”² å¼€å§‹ç½‘æ ¼æœç´¢ (æœ€å¤š {max_trials} ä¸ªç»„åˆ)...")
        grid_combinations = self.generate_grid_combinations(max_trials // 2)
        
        all_combinations = grid_combinations
        if use_random:
            logger.info(f"ğŸ² æ·»åŠ éšæœºæœç´¢ç»„åˆ...")
            random_combinations = self.generate_random_combinations(max_trials - len(grid_combinations))
            all_combinations.extend(random_combinations)
        
        # éšæœºæ‰“ä¹±é¡ºåº
        random.shuffle(all_combinations)
        total_combinations = min(len(all_combinations), max_trials)
        
        logger.info(f"ğŸ“Š æ€»è®¡å°†è¯„ä¼° {total_combinations} ä¸ªé…ç½®ç»„åˆ")
        
        # æ‰§è¡Œè¯•éªŒ
        for i, hyperparams in enumerate(all_combinations[:total_combinations]):
            trial_id = i + 1
            logger.info(f"\nâš¡ è¯•éªŒ {trial_id}/{total_combinations}")
            
            score = self.evaluate_hyperparameters(hyperparams, trial_id)
            
            # è®°å½•ç»“æœ
            result = {
                'trial_id': trial_id,
                'hyperparams': hyperparams,
                'score': score,
                'timestamp': time.strftime("%Y-%m-%d %H:%M:%S")
            }
            self.all_results.append(result)
            
            # æ›´æ–°æœ€ä½³é…ç½®
            if score < self.best_score:
                self.best_score = score
                self.best_config = hyperparams.copy()
                logger.info(f"  âœ¨ æ–°æœ€ä½³! åˆ†æ•°: {score:.4f}")
                self.save_intermediate_best()
            
            # è¿›åº¦æ˜¾ç¤º
            if trial_id % 10 == 0:
                elapsed = time.time() - start_time
                logger.info(f"  ğŸ“ˆ è¿›åº¦: {trial_id}/{total_combinations}, å·²ç”¨æ—¶: {elapsed:.1f}ç§’")
        
        # å®Œæˆè°ƒä¼˜
        total_time = time.time() - start_time
        logger.info(f"\nâœ… è¶…å‚æ•°è°ƒä¼˜å®Œæˆ!")
        logger.info(f"â±ï¸  æ€»ç”¨æ—¶: {total_time:.1f}ç§’")
        logger.info(f"ğŸ† æœ€ä½³åˆ†æ•°: {self.best_score:.4f}")
        logger.info(f"ğŸ¯ æœ€ä½³é…ç½®:")
        for key, value in self.best_config.items():
            logger.info(f"    {key}: {value}")
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        self.save_final_results()
        
    def save_intermediate_best(self):
        """ä¿å­˜ä¸­é—´æœ€ä½³ç»“æœ"""
        best_path = self.results_dir / "current_best.json"
        with open(best_path, 'w', encoding='utf-8') as f:
            json.dump({
                'score': self.best_score,
                'config': self.best_config,
                'timestamp': time.strftime("%Y-%m-%d %H:%M:%S")
            }, f, indent=2)
    
    def save_final_results(self):
        """ä¿å­˜æœ€ç»ˆè°ƒä¼˜ç»“æœ"""
        # ä¿å­˜æœ€ä½³è¶…å‚æ•°é…ç½® (ç”¨äºè®­ç»ƒ)
        best_config_path = self.results_dir / "best_hyperparameters_for_training.yaml"
        training_config = {
            'drl': {
                'learning_rate': self.best_config['learning_rate'],
                'gamma': self.best_config['gamma'],
                'epsilon_decay': self.best_config['epsilon_decay'],
                'batch_size': self.best_config['batch_size'],
                'network': {
                    'hidden_dims': self.best_config['network_hidden_dims']
                },
                'reward_weights': self.best_config['reward_weights'],
                'episodes': 300,  # æ¢å¤å®Œæ•´è®­ç»ƒepisodeæ•°
                'max_steps': 30
            },
            'tuning_metadata': {
                'best_score': self.best_score,
                'total_trials': len(self.all_results),
                'tuning_date': time.strftime("%Y-%m-%d %H:%M:%S"),
                'tuning_method': 'grid_search + random_search'
            }
        }
        
        with open(best_config_path, 'w', encoding='utf-8') as f:
            yaml.dump(training_config, f, default_flow_style=False)
        
        logger.info(f"ğŸ’¾ æœ€ä½³é…ç½®å·²ä¿å­˜åˆ°: {best_config_path}")
        
        # ä¿å­˜å®Œæ•´çš„è°ƒä¼˜ç»“æœ
        full_results = {
            'best_config': self.best_config,
            'best_score': self.best_score,
            'all_trials': self.all_results,
            'search_space': self.search_space,
            'tuning_summary': {
                'total_trials': len(self.all_results),
                'best_trial_id': min(self.all_results, key=lambda x: x['score'])['trial_id'],
                'score_range': {
                    'min': min(r['score'] for r in self.all_results),
                    'max': max(r['score'] for r in self.all_results),
                    'mean': np.mean([r['score'] for r in self.all_results])
                }
            }
        }
        
        results_path = self.results_dir / "hyperparameter_tuning_results.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(full_results, f, indent=2, default=str)
        
        logger.info(f"ğŸ“Š å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {results_path}")
        
        # ç”Ÿæˆè°ƒä¼˜æŠ¥å‘Š
        self.generate_tuning_report()
    
    def generate_tuning_report(self):
        """ç”Ÿæˆè°ƒä¼˜æŠ¥å‘Š"""
        report_path = self.results_dir / "hyperparameter_tuning_report.md"
        
        # åˆ†æç»“æœ
        scores = [r['score'] for r in self.all_results]
        best_trial = min(self.all_results, key=lambda x: x['score'])
        
        report_content = f"""# WASS-RAG è¶…å‚æ•°è°ƒä¼˜æŠ¥å‘Š

## è°ƒä¼˜æ¦‚è§ˆ

- **è°ƒä¼˜æ—¥æœŸ**: {time.strftime("%Y-%m-%d %H:%M:%S")}
- **æ€»è¯•éªŒæ¬¡æ•°**: {len(self.all_results)}
- **æœç´¢æ–¹æ³•**: ç½‘æ ¼æœç´¢ + éšæœºæœç´¢
- **æœ€ä½³åˆ†æ•°**: {self.best_score:.4f}

## æœ€ä½³é…ç½®

```yaml
å­¦ä¹ ç‡: {self.best_config['learning_rate']}
æŠ˜æ‰£å› å­: {self.best_config['gamma']}
æ¢ç´¢è¡°å‡: {self.best_config['epsilon_decay']}
æ‰¹æ¬¡å¤§å°: {self.best_config['batch_size']}
ç½‘ç»œç»“æ„: {self.best_config['network_hidden_dims']}
å¥–åŠ±æƒé‡: {self.best_config['reward_weights']}
```

## æ€§èƒ½ç»Ÿè®¡

- **æœ€ä½³åˆ†æ•°**: {min(scores):.4f}
- **æœ€å·®åˆ†æ•°**: {max(scores):.4f}
- **å¹³å‡åˆ†æ•°**: {np.mean(scores):.4f}
- **æ ‡å‡†å·®**: {np.std(scores):.4f}

## å‚æ•°å½±å“åˆ†æ

### å­¦ä¹ ç‡åˆ†æ
"""
        
        # åˆ†æå­¦ä¹ ç‡å½±å“
        lr_analysis = {}
        for result in self.all_results:
            lr = result['hyperparams']['learning_rate']
            if lr not in lr_analysis:
                lr_analysis[lr] = []
            lr_analysis[lr].append(result['score'])
        
        for lr, scores_list in sorted(lr_analysis.items()):
            avg_score = np.mean(scores_list)
            report_content += f"- å­¦ä¹ ç‡ {lr}: å¹³å‡åˆ†æ•° {avg_score:.4f} ({len(scores_list)}æ¬¡è¯•éªŒ)\n"
        
        report_content += f"""

### ç½‘ç»œç»“æ„åˆ†æ
"""
        
        # åˆ†æç½‘ç»œç»“æ„å½±å“
        network_analysis = {}
        for result in self.all_results:
            network = str(result['hyperparams']['network_hidden_dims'])
            if network not in network_analysis:
                network_analysis[network] = []
            network_analysis[network].append(result['score'])
        
        for network, scores_list in sorted(network_analysis.items(), key=lambda x: np.mean(x[1])):
            avg_score = np.mean(scores_list)
            report_content += f"- ç½‘ç»œç»“æ„ {network}: å¹³å‡åˆ†æ•° {avg_score:.4f} ({len(scores_list)}æ¬¡è¯•éªŒ)\n"
        
        report_content += f"""

## å»ºè®®

åŸºäºè°ƒä¼˜ç»“æœï¼Œæ¨èä½¿ç”¨æœ€ä½³é…ç½®è¿›è¡ŒDRLæ™ºèƒ½ä½“è®­ç»ƒã€‚è¯¥é…ç½®åœ¨{len(self.all_results)}æ¬¡è¯•éªŒä¸­è¡¨ç°æœ€ä½³ã€‚

## ä½¿ç”¨æ–¹æ³•

```bash
# ä½¿ç”¨è°ƒä¼˜åçš„é…ç½®è®­ç»ƒDRLæ™ºèƒ½ä½“
python scripts/train_drl_wrench.py configs/experiment.yaml
```

è®­ç»ƒè„šæœ¬ä¼šè‡ªåŠ¨åŠ è½½ `{self.results_dir}/best_hyperparameters_for_training.yaml` ä¸­çš„æœ€ä½³é…ç½®ã€‚
"""
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"ğŸ“‹ è°ƒä¼˜æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ”§ WASS-RAG æœ¬åœ°è¶…å‚æ•°è°ƒä¼˜å™¨")
    
    # æ£€æŸ¥åŸºç¡€é…ç½®æ–‡ä»¶
    config_path = "configs/experiment.yaml"
    if not os.path.exists(config_path):
        logger.error(f"âŒ é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {config_path}")
        logger.error("   è¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ­¤è„šæœ¬")
        sys.exit(1)
    
    # åˆ›å»ºè°ƒä¼˜å™¨å¹¶è¿è¡Œ
    tuner = HyperparameterTuner(config_path)
    
    try:
        # è¿è¡Œè°ƒä¼˜ (å¯ä»¥è°ƒæ•´è¯•éªŒæ¬¡æ•°)
        tuner.run_tuning(max_trials=30, use_random=True)
        
        logger.info("\nğŸ‰ è¶…å‚æ•°è°ƒä¼˜æˆåŠŸå®Œæˆ!")
        logger.info("ğŸ“ ç»“æœæ–‡ä»¶:")
        logger.info(f"   - æœ€ä½³é…ç½®: {tuner.results_dir}/best_hyperparameters_for_training.yaml")
        logger.info(f"   - å®Œæ•´ç»“æœ: {tuner.results_dir}/hyperparameter_tuning_results.json")
        logger.info(f"   - è°ƒä¼˜æŠ¥å‘Š: {tuner.results_dir}/hyperparameter_tuning_report.md")
        
    except KeyboardInterrupt:
        logger.info("\nâ¹ï¸  è°ƒä¼˜è¢«ç”¨æˆ·ä¸­æ–­")
        if tuner.best_config:
            logger.info("ğŸ’¾ ä¿å­˜å½“å‰æœ€ä½³ç»“æœ...")
            tuner.save_final_results()
    except Exception as e:
        logger.error(f"âŒ è°ƒä¼˜è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
