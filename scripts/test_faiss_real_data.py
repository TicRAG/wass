#!/usr/bin/env python3
"""
ç‹¬ç«‹æµ‹è¯•FAISSæ“ä½œï¼Œæ¨¡æ‹Ÿinitialize_ai_models.pyçš„å…·ä½“æƒ…å†µ
"""

import sys
import os
import numpy as np
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

def test_faiss_with_real_data():
    """ä½¿ç”¨çœŸå®æ•°æ®æµ‹è¯•FAISSæ“ä½œ"""
    
    try:
        from src.ai_schedulers import RAGKnowledgeBase
        
        print("=== FAISSå®é™…æ•°æ®æµ‹è¯• ===")
        
        # 1. æ¨¡æ‹Ÿä»JSONè¯»å–çš„æ•°æ®ï¼ˆè¿™æ˜¯é—®é¢˜çš„æ ¹æºï¼‰
        print("1. æ¨¡æ‹Ÿä»JSONè¯»å–embeddingæ•°æ®...")
        
        # åˆ›å»ºæ¨¡æ‹Ÿçš„state_embeddingï¼ˆå°±åƒinitialize_ai_models.pyä¸­çš„æ•°æ®ï¼‰
        original_numpy = np.array([
            0.05,  # task_count / 100.0
            0.3,   # avg_flops / 10e9
            0.25,  # avg_memory / 4e9
            0.4,   # dependency_ratio
            0.6,   # data_intensity
            0.5,   # node_count / 16.0
            0.7,   # avg_load
        ] + [np.random.randn() for _ in range(25)])  # å¡«å……åˆ°32ç»´
        
        # æ¨¡æ‹ŸJSONåºåˆ—åŒ–/ååºåˆ—åŒ–è¿‡ç¨‹
        json_data = original_numpy.tolist()  # è¿™æ˜¯ä¿å­˜åˆ°JSONçš„è¿‡ç¨‹
        print(f"   JSONæ•°æ®ç±»å‹: {type(json_data)}")
        print(f"   JSONæ•°æ®é•¿åº¦: {len(json_data)}")
        
        # æ¨¡æ‹Ÿä»JSONè¯»å–å›æ¥çš„è¿‡ç¨‹
        loaded_embedding = np.array(json_data, dtype=np.float32)  # è¿™æ˜¯ä»JSONè¯»å–çš„è¿‡ç¨‹
        print(f"   åŠ è½½åç±»å‹: {type(loaded_embedding)}")
        print(f"   åŠ è½½åå½¢çŠ¶: {loaded_embedding.shape}")
        print(f"   åŠ è½½ådtype: {loaded_embedding.dtype}")
        
        # 2. åˆ›å»ºçŸ¥è¯†åº“å¹¶æµ‹è¯•æ·»åŠ 
        print("\n2. åˆ›å»ºçŸ¥è¯†åº“...")
        kb = RAGKnowledgeBase(embedding_dim=32)
        
        print("3. æµ‹è¯•æ·»åŠ æ¡ˆä¾‹...")
        
        # è¿™å°±æ˜¯initialize_ai_models.pyä¸­è°ƒç”¨çš„æ–¹å¼
        kb.add_case(
            embedding=loaded_embedding,  # ä»JSONåŠ è½½çš„æ•°æ®
            workflow_info={"task_count": 5, "complexity": "medium", "type": "synthetic"},
            actions=["node_0", "node_1", "node_2"],
            makespan=10.5
        )
        
        print("   âœ“ æˆåŠŸæ·»åŠ ç¬¬ä¸€ä¸ªæ¡ˆä¾‹!")
        
        # æ·»åŠ æ›´å¤šæ¡ˆä¾‹
        for i in range(5):
            test_data = [0.1 * (i+1)] * 7 + [np.random.randn() for _ in range(25)]
            json_data = test_data  # æ¨¡æ‹ŸJSONæ•°æ®
            embedding = np.array(json_data, dtype=np.float32)
            
            kb.add_case(
                embedding=embedding,
                workflow_info={"task_count": i+1, "complexity": "test", "type": "synthetic"},
                actions=[f"node_{j}" for j in range(i+1)],
                makespan=5.0 + i
            )
        
        print(f"   âœ“ æˆåŠŸæ·»åŠ äº† {len(kb.cases)} ä¸ªæ¡ˆä¾‹!")
        
        # 4. æµ‹è¯•æŸ¥è¯¢
        print("\n4. æµ‹è¯•æŸ¥è¯¢...")
        query_embedding = np.array([0.05] * 7 + [0.1] * 25, dtype=np.float32)
        results = kb.retrieve_similar_cases(query_embedding, top_k=3)
        
        print(f"   âœ“ æŸ¥è¯¢æˆåŠŸ! æ‰¾åˆ° {len(results['similar_cases'])} ä¸ªç›¸ä¼¼æ¡ˆä¾‹")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_faiss_with_real_data()
    
    if success:
        print("\nğŸ‰ FAISSå®é™…æ•°æ®æµ‹è¯•æˆåŠŸ!")
        print("é—®é¢˜å·²è§£å†³ï¼Œå¯ä»¥è¿è¡Œå®Œæ•´çš„åˆå§‹åŒ–è„šæœ¬")
    else:
        print("\nâŒ ä»æœ‰é—®é¢˜éœ€è¦è§£å†³")
