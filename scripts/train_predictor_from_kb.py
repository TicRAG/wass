#!/usr/bin/env python3
"""
WASS-RAG é˜¶æ®µäºŒï¼šæ€§èƒ½é¢„æµ‹å™¨è®­ç»ƒè„šæœ¬ (æœ€ç»ˆAPIä¿®æ­£ç‰ˆ)

è¯¥è„šæœ¬åŠ è½½ç”± `generate_kb_dataset.py` ç”Ÿæˆçš„é«˜è´¨é‡æ•°æ®é›†ï¼Œ
å¹¶ä½¿ç”¨è¿™äº›æ•°æ®æ¥è®­ç»ƒ Performance Predictor æ¨¡å‹ã€‚
è¿™è§£å†³äº†â€œè®­ç»ƒ-ä»¿çœŸåå·®â€é—®é¢˜ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®é¢„æµ‹çœŸå®ä»¿çœŸç¯å¢ƒä¸‹çš„æ€§èƒ½ã€‚
"""

import sys
import os
import json
import time
from pathlib import Path
from dataclasses import dataclass, asdict
from typing import List, Dict, Any
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, str(parent_dir))
sys.path.insert(0, os.path.join(parent_dir, 'src'))

# å¯¼å…¥ AI è°ƒåº¦å™¨ä¸­å®šä¹‰çš„æ¨¡å‹ç»“æ„
from src.performance_predictor import PerformancePredictor, SimplePerformancePredictor

def load_training_dataset() -> List[Dict[str, Any]]:
    """åŠ è½½çŸ¥è¯†åº“æ’­ç§é˜¶æ®µç”Ÿæˆçš„æ•°æ®é›†"""
    # Align with JSONL knowledge base generation output name if needed
    dataset_path = Path("data/kb_training_dataset.json")
    if not dataset_path.exists():
        print(f"âŒ é”™è¯¯ï¼šæ•°æ®é›†æ–‡ä»¶æœªæ‰¾åˆ°äº {dataset_path}")
        print("   è¯·å…ˆè¿è¡Œ `scripts/generate_kb_dataset.py` æ¥ç”Ÿæˆæ•°æ®ã€‚")
        sys.exit(1)
        
    print(f"ğŸ“š Loading dataset from {dataset_path}...")
    with open(dataset_path, 'r') as f:
        dataset = json.load(f)
    print(f"   Successfully loaded {len(dataset)} samples.")
    return dataset

def train_predictor(training_data: List[Dict[str, Any]], epochs: int = 100, batch_size: int = 512, learning_rate: float = 0.001):
    """
    ä½¿ç”¨åŠ è½½çš„æ•°æ®é›†è®­ç»ƒ PerformancePredictor æ¨¡å‹ã€‚
    ä¿®å¤ï¼šæ·»åŠ æ•°æ®å»é‡å’Œæ­£ç¡®çš„è®­ç»ƒ/éªŒè¯é›†åˆ†å‰²
    """
    print(f"\nğŸš€ Starting Performance Predictor training...")
    print(f"   Epochs: {epochs}, Batch Size: {batch_size}, Learning Rate: {learning_rate}")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"   Using device: {device}")
    
    # 1. æ•°æ®å»é‡å¤„ç†
    print(f"   Original samples: {len(training_data)}")
    
    # ä½¿ç”¨ç‰¹å¾ç»„åˆä½œä¸ºé”®æ¥å»é™¤é‡å¤æ ·æœ¬
    unique_data = {}
    for sample in training_data:
        key = (tuple(sample['state_features']), tuple(sample['action_features']), tuple(sample['context_features']))
        if key not in unique_data:
            unique_data[key] = sample
    
    unique_samples = list(unique_data.values())
    print(f"   Unique samples after deduplication: {len(unique_samples)}")
    
    # 2. å‡†å¤‡æ•°æ®
    X = np.array([
        s['state_features'] + s['action_features'] + s['context_features'] 
        for s in unique_samples
    ])
    y = np.array([s['achieved_finish_time'] for s in unique_samples])
    
    y_mean, y_std = np.mean(y), np.std(y)
    if y_std < 1e-8: y_std = 1.0
    
    print(f"ğŸ“ˆ Target (achieved_finish_time) stats: mean={y_mean:.2f}, std={y_std:.2f}")
    
    # 3. è®­ç»ƒ/éªŒè¯é›†åˆ†å‰² (80/20)
    from sklearn.model_selection import train_test_split
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.2, random_state=42, shuffle=True
    )
    
    print(f"   Train samples: {len(X_train)}, Validation samples: {len(X_val)}")
    
    # æ ‡å‡†åŒ–æ•°æ®
    y_train_normalized = (y_train - y_mean) / y_std
    y_val_normalized = (y_val - y_mean) / y_std
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    X_train_tensor = torch.FloatTensor(X_train).to(device)
    y_train_tensor = torch.FloatTensor(y_train_normalized).view(-1, 1).to(device)
    X_val_tensor = torch.FloatTensor(X_val).to(device)
    y_val_tensor = torch.FloatTensor(y_val_normalized).view(-1, 1).to(device)
    
    # åˆ›å»ºè®­ç»ƒæ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    
    # 4. åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    model = SimplePerformancePredictor(input_dim=X.shape[1], hidden_dim=128).to(device)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)
    
    # 5. è®­ç»ƒå¾ªç¯
    best_val_loss = float('inf')
    patience_counter = 0
    max_patience = 15
    
    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        for batch_X, batch_y in train_dataloader:
            optimizer.zero_grad()
            predictions = model(batch_X)
            loss = criterion(predictions, batch_y)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        
        avg_train_loss = train_loss / len(train_dataloader)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        with torch.no_grad():
            val_predictions = model(X_val_tensor)
            val_loss = criterion(val_predictions, y_val_tensor).item()
        
        scheduler.step(val_loss)
        
        # æ—©åœæ£€æŸ¥å’Œæ¨¡å‹ä¿å­˜
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            torch.save(model.state_dict(), "temp_best_predictor.pth")
        else:
            patience_counter += 1
        
        if (epoch + 1) % 10 == 0:
            print(f"   Epoch {epoch+1}/{epochs}: Train Loss = {avg_train_loss:.6f}, Val Loss = {val_loss:.6f}, LR = {optimizer.param_groups[0]['lr']:.6f}")
        
        # æ—©åœ
        if patience_counter >= max_patience:
            print(f"   Early stopping at epoch {epoch+1} due to no improvement")
            break
    
    # 6. åŠ è½½æœ€ä½³æ¨¡å‹å¹¶æœ€ç»ˆè¯„ä¼°
    print("\nâœ… Training complete. Evaluating on validation set...")
    model.load_state_dict(torch.load("temp_best_predictor.pth"))
    os.remove("temp_best_predictor.pth")

    model.eval()
    with torch.no_grad():
        val_predictions_norm = model(X_val_tensor).cpu().numpy().flatten()
        val_predictions = val_predictions_norm * y_std + y_mean
        
        mse = np.mean((val_predictions - y_val) ** 2)
        mae = np.mean(np.abs(val_predictions - y_val))
        r2 = 1 - (np.sum((y_val - val_predictions) ** 2) / np.sum((y_val - np.mean(y_val)) ** 2))
        
        print(f"   Validation Results: MSE = {mse:.4f}, MAE = {mae:.4f}, RÂ² = {r2:.4f}")
        
        # å¥å…¨æ€§æ£€æŸ¥å’Œè­¦å‘Š
        if r2 > 0.98:
            print(f"   âš ï¸  è­¦å‘Š: RÂ²å€¼è¿‡é«˜ ({r2:.4f})ï¼Œå¯èƒ½å­˜åœ¨è½»å¾®è¿‡æ‹Ÿåˆï¼Œä½†ä»å¯æ¥å—")
        elif r2 < 0.3:
            print(f"   âš ï¸  è­¦å‘Š: RÂ²å€¼è¾ƒä½ ({r2:.4f})ï¼Œæ¨¡å‹æ€§èƒ½å¯èƒ½ä¸ä½³")
        else:
            print(f"   âœ… RÂ²å€¼æ­£å¸¸ ({r2:.4f})ï¼Œæ¨¡å‹è®­ç»ƒæˆåŠŸ")
        
    return model, y_mean, y_std, {"r2": r2, "mse": mse, "mae": mae}

def save_model(model: PerformancePredictor, y_mean: float, y_std: float, metrics: Dict):
    """å°†è®­ç»ƒå¥½çš„æ¨¡å‹å’Œå…ƒæ•°æ®ä¿å­˜åˆ° WASS æ¨¡å‹æ–‡ä»¶ä¸­"""
    model_path = Path("models/wass_models.pth")
    model_path.parent.mkdir(exist_ok=True)
    
    print(f"\nğŸ’¾ Saving trained model and metadata to {model_path}...")
    
    try:
        # --- API ä¿®æ­£å¤„ ---
        # æ·»åŠ  weights_only=False ä»¥å…è®¸åŠ è½½åŒ…å«å…ƒæ•°æ®çš„å®Œæ•´ checkpoint æ–‡ä»¶
        checkpoint = torch.load(model_path, map_location="cpu", weights_only=False)
        # --- ä¿®æ­£ç»“æŸ ---
        print("   Found existing model file. Updating Performance Predictor weights.")
    except (FileNotFoundError, EOFError): # Also handle empty/corrupt files
        checkpoint = {}
        print("   No existing model file found or file is invalid. Creating a new checkpoint.")

    checkpoint["performance_predictor"] = model.state_dict()
    
    checkpoint["metadata"] = checkpoint.get("metadata", {})
    checkpoint["metadata"]["performance_predictor"] = {
        "y_mean": float(y_mean),
        "y_std": float(y_std),
        "training_samples": metrics['total_samples'],
        "unique_samples": metrics['unique_samples'],
        "retrained_at": time.strftime("%Y-%m-%d %H:%M:%S"),
        "training_source": "kb_training_dataset.json",
        "validation_results": {
            "r2": metrics["r2"],
            "mse": metrics["mse"],
            "mae": metrics.get("mae", 0.0)
        },
        "training_improvements": "Added data deduplication and proper train/val split"
    }
    
    torch.save(checkpoint, model_path)
    print(f"âœ… Model saved successfully.")

def main():
    """ä¸»å‡½æ•°"""
    dataset = load_training_dataset()
    
    model, y_mean, y_std, metrics = train_predictor(dataset)
    
    # æ·»åŠ é¢å¤–çš„ç»Ÿè®¡ä¿¡æ¯
    metrics['total_samples'] = len(dataset)
    
    # è®¡ç®—å»é‡åçš„æ ·æœ¬æ•°é‡
    unique_data = {}
    for sample in dataset:
        key = (tuple(sample['state_features']), tuple(sample['action_features']), tuple(sample['context_features']))
        if key not in unique_data:
            unique_data[key] = sample
    metrics['unique_samples'] = len(unique_data)
    
    save_model(model, y_mean, y_std, metrics)

if __name__ == "__main__":
    main()