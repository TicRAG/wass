# scripts/seed_knowledge_base.py
import os
import sys
import json
import numpy as np
import torch
import joblib
from pathlib import Path
from sklearn.preprocessing import StandardScaler

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from scripts.workflow_manager import WorkflowManager
from src.drl.gnn_encoder import GNNEncoder
from src.drl.knowledge_teacher import KnowledgeBase
from src.drl.utils import workflow_json_to_pyg_data
# --- æ ¸å¿ƒä¿®æ”¹ï¼šå¯¼å…¥ HEFTScheduler å’Œæˆ‘ä»¬æ–°å¢çš„ RandomScheduler ---
from src.wrench_schedulers import HEFTScheduler, RandomScheduler
from src.utils import WrenchExperimentRunner

# --- é…ç½®ä¿æŒä¸å˜ ---
GNN_IN_CHANNELS = 4
GNN_HIDDEN_CHANNELS = 64
GNN_OUT_CHANNELS = 32
KB_DIMENSION = GNN_OUT_CHANNELS
PLATFORM_FILE = "configs/test_platform.xml"
WORKFLOW_CONFIG_FILE = "configs/workflow_config.yaml"
FEATURE_SCALER_PATH = "models/saved_models/feature_scaler.joblib"

def main():
    print("ğŸš€ [Phase 1] Starting Knowledge Base Seeding (with HEFT + Random Schedulers)...")
    
    workflow_manager = WorkflowManager(WORKFLOW_CONFIG_FILE)
    gnn_encoder = GNNEncoder(GNN_IN_CHANNELS, GNN_HIDDEN_CHANNELS, GNN_OUT_CHANNELS)
    knowledge_base = KnowledgeBase(dimension=KB_DIMENSION)
    config_params = {"platform_file": PLATFORM_FILE}
    wrench_runner = WrenchExperimentRunner(schedulers={}, config=config_params)
    print("âœ… Components initialized.")

    seeding_workflows = workflow_manager.generate_training_workflows()
    print(f"âœ… Generated {len(seeding_workflows)} workflows.")

    print("\n[Step 3a/6] Extracting features to fit the scaler...")
    all_node_features = []
    successful_workflows = []
    for wf_file in seeding_workflows:
        try:
            with open(wf_file, 'r') as f:
                wf_data = json.load(f)
            for task in wf_data['workflow']['tasks']:
                features = [
                    float(task.get('runtime', 0.0)),
                    float(task.get('flops', 0.0)),
                    float(task.get('memory', 0.0))
                ]
                all_node_features.append(features)
            successful_workflows.append(wf_file)
        except Exception:
            continue

    if not all_node_features:
        print("âŒ Could not extract any features. Aborting.")
        return

    feature_scaler = StandardScaler()
    feature_scaler.fit(all_node_features)
    joblib.dump(feature_scaler, FEATURE_SCALER_PATH)
    print(f"âœ… Feature scaler fitted and saved to {FEATURE_SCALER_PATH}")

    # --- æ ¸å¿ƒä¿®æ”¹ï¼šå®šä¹‰ç”¨äº seeding çš„è°ƒåº¦å™¨åˆ—è¡¨ ---
    seeding_schedulers = {
        "HEFT": HEFTScheduler
        # "Random": RandomScheduler
    }
    print(f"\n[Step 3b/6] Simulating workflows with schedulers: {list(seeding_schedulers.keys())}...")
    
    all_embeddings = []
    all_metadata = []

    # å¤–å±‚å¾ªç¯éå†ä¸åŒçš„è°ƒåº¦å™¨
    for scheduler_name, scheduler_class in seeding_schedulers.items():
        print(f"\n--- Seeding with {scheduler_name} Scheduler ---")
        
        # å†…å±‚å¾ªç¯éå†æ‰€æœ‰å·¥ä½œæµ
        for i, wf_file in enumerate(successful_workflows):
            wf_path = Path(wf_file)
            print(f"  Processing workflow {i+1}/{len(successful_workflows)}: {wf_path.name}")
            
            makespan, decisions = wrench_runner.run_single_seeding_simulation(
                scheduler_class=scheduler_class, # ä½¿ç”¨å½“å‰å¾ªç¯çš„è°ƒåº¦å™¨
                workflow_file=str(wf_path)
            )

            if makespan < 0:
                print(f"  âŒ Simulation failed for {wf_path.name}. Skipping.")
                continue
            
            print(f"  â¹ï¸ Simulation finished. Makespan: {makespan:.2f}s.")
            
            try:
                pyg_data = workflow_json_to_pyg_data(str(wf_path), feature_scaler)
                graph_embedding = gnn_encoder(pyg_data)
            except Exception as e:
                print(f"  âŒ Error encoding workflow {wf_path.name} to graph: {e}")
                continue

            all_embeddings.append(graph_embedding.detach().numpy().flatten())
            all_metadata.append({
                "workflow_file": wf_path.name,
                "makespan": makespan,
                "scheduler_used": scheduler_name, # è®°å½•ä½¿ç”¨äº†å“ªä¸ªè°ƒåº¦å™¨
                "decisions": json.dumps(decisions) 
            })
            
    if not all_embeddings:
        print("âŒ No experience was collected. Cannot build knowledge base.")
        return
        
    knowledge_base.add(np.array(all_embeddings), all_metadata)
    knowledge_base.save()
    print(f"\nâœ… Knowledge Base saved successfully with {len(all_embeddings)} entries from {len(seeding_schedulers)} schedulers.")
    print("\nğŸ‰ [Phase 1] Completed! ğŸ‰")

if __name__ == "__main__":
    main()