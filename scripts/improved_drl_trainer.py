#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
WASS-RAG æ”¹è¿›çš„DRLè®­ç»ƒå™¨
å®ç°å¯†é›†å¥–åŠ±å‡½æ•°ä»¥æé«˜è®­ç»ƒæ•ˆæœ
"""

import os
import sys
import json
import time
import random
from pathlib import Path
from typing import Dict, List, Tuple, Any
from dataclasses import dataclass
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque, namedtuple
import yaml

# å…ˆæ·»åŠ é¡¹ç›®è·¯å¾„å†å¯¼å…¥æœ¬åœ°åŒ…
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

from src.drl.reward import compute_step_reward, compute_final_reward, StepContext, EpisodeStats  # noqa: E402

# ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨ï¼ˆå†’çƒŸè¿è¡Œå¯èƒ½å°šæœªåˆ›å»ºï¼‰
Path('results').mkdir(exist_ok=True)

@dataclass
class TaskState:
    """ä»»åŠ¡çŠ¶æ€"""
    id: str
    computation_size: float
    parents: List[str]
    children: List[str]
    is_critical_path: bool
    data_locality_score: float

@dataclass
class NodeState:
    """èŠ‚ç‚¹çŠ¶æ€"""
    id: str
    speed: float
    current_load: float
    available_time: float
    data_availability: Dict[str, float]  # æ•°æ®å¯ç”¨æ€§è¯„åˆ†

@dataclass
class EnvironmentState:
    """ç¯å¢ƒçŠ¶æ€"""
    current_time: float
    pending_tasks: List[TaskState]
    node_states: List[NodeState]
    workflow_progress: float
    critical_path_length: float

class DenseRewardCalculator:
    """(Deprecated soon) ä¿ç•™æ—§æ¥å£ä»¥é˜²å…¼å®¹é—®é¢˜ï¼Œä½†å†…éƒ¨å§”æ‰˜åˆ°æ–° shapingã€‚"""
    def calculate_step_reward(self, task: TaskState, chosen_node: NodeState, all_nodes: List[NodeState], environment: EnvironmentState) -> float:
        # æ—§æ¥å£ä¿æŒä½†ç°åœ¨ç›´æ¥è¿”å›0ï¼ˆé¿å…è¯¯ç”¨ï¼‰ï¼ŒçœŸå®å¥–åŠ±åœ¨è®­ç»ƒå¾ªç¯å¤–éƒ¨æ„é€  StepContext
        return 0.0
    def calculate_final_reward(self, final_makespan: float, baseline_makespan: float) -> float:
        stats = EpisodeStats(makespan=final_makespan)
        return compute_final_reward(stats)

class ImprovedDQN(nn.Module):
    """æ”¹è¿›çš„DQNç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dims: List[int] = None):
        super().__init__()
        
        if hidden_dims is None:
            hidden_dims = [256, 128, 64]
        
        layers = []
        current_dim = state_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(current_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.1)
            ])
            current_dim = hidden_dim
        
        layers.append(nn.Linear(current_dim, action_dim))
        
        self.network = nn.Sequential(*layers)
        
        # ä½¿ç”¨Xavieråˆå§‹åŒ–
        for layer in self.network:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.zeros_(layer.bias)
    
    def forward(self, x):
        return self.network(x)

class ImprovedDQNAgent:
    """æ”¹è¿›çš„DQNæ™ºèƒ½ä½“"""
    
    def __init__(self,
                 state_dim: int,
                 action_dim: int,
                 learning_rate: float = 1e-3,
                 epsilon_start: float = 1.0,
                 epsilon_end: float = 0.1,
                 epsilon_decay: float = 0.995,
                 gamma: float = 0.99,
                 memory_size: int = 10000,
                 batch_size: int = 64,
                 target_update_freq: int = 100,
                 device: str = None):
        
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.batch_size = batch_size
        self.target_update_freq = target_update_freq
        
        # ç½‘ç»œ
        self.q_network = ImprovedDQN(state_dim, action_dim).to(self.device)
        self.target_network = ImprovedDQN(state_dim, action_dim).to(self.device)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)
        
        # ç»éªŒå›æ”¾
        self.memory = deque(maxlen=memory_size)
        self.experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])
        
        # æ¢ç´¢ç­–ç•¥
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
        
        # è®­ç»ƒç»Ÿè®¡
        self.training_step = 0
        self.update_target()
    
    def update_target(self):
        """æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        self.target_network.load_state_dict(self.q_network.state_dict())
    
    def remember(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒ"""
        experience = self.experience(state, action, reward, next_state, done)
        self.memory.append(experience)
    
    def act(self, state, training=True):
        """é€‰æ‹©åŠ¨ä½œ"""
        if training and random.random() < self.epsilon:
            return random.randint(0, self.action_dim - 1)
        
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            q_values = self.q_network(state_tensor)
            return q_values.argmax().item()
    
    def replay(self):
        """ç»éªŒå›æ”¾è®­ç»ƒ"""
        if len(self.memory) < self.batch_size:
            return None
        
        # é‡‡æ ·ç»éªŒ
        batch = random.sample(self.memory, self.batch_size)
        states = torch.FloatTensor([e.state for e in batch]).to(self.device)
        actions = torch.LongTensor([e.action for e in batch]).to(self.device)
        rewards = torch.FloatTensor([e.reward for e in batch]).to(self.device)
        next_states = torch.FloatTensor([e.next_state for e in batch]).to(self.device)
        dones = torch.BoolTensor([e.done for e in batch]).to(self.device)
        
        # è®¡ç®—Qå€¼
        current_q = self.q_network(states).gather(1, actions.unsqueeze(1))
        next_q = self.target_network(next_states).max(1)[0].detach()
        target_q = rewards + (self.gamma * next_q * ~dones)
        
        # è®¡ç®—æŸå¤±
        loss = nn.MSELoss()(current_q.squeeze(), target_q)
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)
        
        self.optimizer.step()
        
        # æ›´æ–°æ¢ç´¢ç‡
        if self.epsilon > self.epsilon_end:
            self.epsilon *= self.epsilon_decay
        
        # å®šæœŸæ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.training_step += 1
        if self.training_step % self.target_update_freq == 0:
            self.update_target()
        
        return loss.item()

class WRENCHDRLTrainer:
    """WRENCH DRLè®­ç»ƒå™¨"""
    
    def __init__(self, config_path: str):
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.reward_calculator = DenseRewardCalculator()
        self.agent = None
        self.training_history = []
        # æ–°å¢ï¼šè¯»å–æ‰©å±•è®­ç»ƒ/æ—¥å¿—é…ç½®
        self.drl_cfg = self.config.get('drl', {})
        self.checkpoint_cfg = self.config.get('checkpoint', {})
        self.logging_cfg = self.config.get('logging', {})
        Path(self.checkpoint_cfg.get('dir', 'models/checkpoints/')).mkdir(parents=True, exist_ok=True)
        Path(self.logging_cfg.get('metrics_file', 'results/training_metrics.jsonl')).parent.mkdir(parents=True, exist_ok=True)
        self.best_makespan = float('inf')
    
    def create_mock_environment(self) -> Tuple[EnvironmentState, List[TaskState], List[NodeState]]:
        """åˆ›å»ºæ¨¡æ‹Ÿè®­ç»ƒç¯å¢ƒ"""
        # åˆ›å»ºèŠ‚ç‚¹çŠ¶æ€
        node_states = []
        for i in range(4):
            node_states.append(NodeState(
                id=f"ComputeHost{i+1}",
                speed=2.0 + i * 0.5,  # ä¸åŒçš„å¤„ç†é€Ÿåº¦
                current_load=random.uniform(0, 0.5),
                available_time=random.uniform(0, 10),
                data_availability={f"task_{j}": random.random() for j in range(20)}
            ))
        
        # åˆ›å»ºä»»åŠ¡çŠ¶æ€
        task_states = []
        for i in range(20):
            task_states.append(TaskState(
                id=f"task_{i}",
                computation_size=random.uniform(1e9, 1e10),
                parents=[f"task_{j}" for j in range(max(0, i-2), i)],
                children=[f"task_{j}" for j in range(i+1, min(20, i+3))],
                is_critical_path=random.random() > 0.7,
                data_locality_score=random.random()
            ))
        
        # åˆ›å»ºç¯å¢ƒçŠ¶æ€
        environment = EnvironmentState(
            current_time=0.0,
            pending_tasks=task_states,
            node_states=node_states,
            workflow_progress=0.0,
            critical_path_length=100.0
        )
        
        return environment, task_states, node_states
    
    def extract_state_features(self, 
                             current_task: TaskState, 
                             node_states: List[NodeState],
                             environment: EnvironmentState) -> np.ndarray:
        """æå–çŠ¶æ€ç‰¹å¾"""
        features = []
        
        # ä»»åŠ¡ç‰¹å¾
        features.extend([
            current_task.computation_size / 1e10,  # å½’ä¸€åŒ–
            len(current_task.parents),
            len(current_task.children),
            float(current_task.is_critical_path),
            current_task.data_locality_score
        ])
        
        # èŠ‚ç‚¹ç‰¹å¾
        for node in node_states:
            features.extend([
                node.speed / 5.0,  # å½’ä¸€åŒ–
                node.current_load,
                node.available_time / 100.0,  # å½’ä¸€åŒ–
                node.data_availability.get(current_task.id, 0.0)
            ])
        
        # ç¯å¢ƒç‰¹å¾
        features.extend([
            environment.workflow_progress,
            environment.current_time / 1000.0,  # å½’ä¸€åŒ–
            len(environment.pending_tasks) / 20.0  # å½’ä¸€åŒ–
        ])
        
        return np.array(features, dtype=np.float32)
    
    def simulate_step(self, 
                     task: TaskState, 
                     action: int, 
                     node_states: List[NodeState],
                     environment: EnvironmentState) -> Tuple[float, EnvironmentState, bool]:
        """æ¨¡æ‹Ÿä¸€æ­¥æ‰§è¡Œ"""
        chosen_node = node_states[action]
        
        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        execution_time = task.computation_size / chosen_node.speed
        # å¥–åŠ±ä¸åœ¨æ­¤è®¡ç®—ï¼ˆå¤–éƒ¨æ ¹æ®æ›´æ–°åçš„å…¨å±€ä¿¡æ¯æ„é€  StepContextï¼‰
        # æ›´æ–°ç¯å¢ƒ
        new_environment = EnvironmentState(
            current_time=environment.current_time + execution_time,
            pending_tasks=[t for t in environment.pending_tasks if t.id != task.id],
            node_states=node_states,
            workflow_progress=environment.workflow_progress + 1.0/20.0,
            critical_path_length=environment.critical_path_length
        )
        
        # æ›´æ–°èŠ‚ç‚¹çŠ¶æ€
        chosen_node.current_load += 0.1
        chosen_node.available_time += execution_time
        
        # æ£€æŸ¥æ˜¯å¦ç»“æŸ
        done = len(new_environment.pending_tasks) == 0
        return 0.0, new_environment, done
    
    def train_episode(self) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepisode"""
        environment, task_states, node_states = self.create_mock_environment()
        
        total_reward = 0.0
        total_makespan = 0.0
        step_count = 0
        
        current_tasks = task_states.copy()

        reward_debug_path = self.logging_cfg.get('reward_debug', 'results/reward_debug.log')
        debug_file = None
        try:
            debug_file = open(reward_debug_path, 'a')
        except Exception:
            debug_file = None

        # å‡†å¤‡å¢å¼ºå‹ StepContext ç»Ÿè®¡
        total_cp_tasks = sum(1 for t in task_states if t.is_critical_path) or 1
        baseline_avg_wait = np.mean([n.available_time for n in node_states]) or 1.0
        completed_ids = set()
        task_map = {t.id: t for t in task_states}

        while current_tasks and step_count < 50:  # é™åˆ¶æœ€å¤§æ­¥æ•°
            # é€‰æ‹©å½“å‰ä»»åŠ¡ï¼ˆç®€åŒ–ï¼šæŒ‰é¡ºåºï¼‰
            current_task = current_tasks[0]
            
            # æå–çŠ¶æ€ç‰¹å¾
            state = self.extract_state_features(current_task, node_states, environment)
            
            # æ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œ
            action = self.agent.act(state, training=True)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            _, new_environment, done = self.simulate_step(
                current_task, action, node_states, environment
            )
            # æ›´æ–°å®Œæˆé›†
            completed_ids.add(current_task.id)

            # æ„é€ å¢å¼º StepContext
            try:
                completed_cp = sum(1 for cid in completed_ids if task_map[cid].is_critical_path)
                ctx = StepContext(
                    completed_critical_path_tasks=completed_cp,
                    total_critical_path_tasks=total_cp_tasks,
                    node_busy_times={n.id: n.current_load for n in node_states},
                    ready_task_count=len(current_tasks)-1,  # å»æ‰å½“å‰å³å°†è°ƒåº¦çš„
                    total_nodes=len(node_states),
                    avg_queue_wait=np.mean([n.available_time for n in node_states]),
                    queue_wait_baseline=baseline_avg_wait
                )
                step_reward, _metrics = compute_step_reward(ctx, debug_writer=debug_file)
            except Exception:
                step_reward = 0.0
            
            # æå–ä¸‹ä¸€çŠ¶æ€ç‰¹å¾
            if len(current_tasks) > 1:
                next_task = current_tasks[1]
                next_state = self.extract_state_features(next_task, node_states, new_environment)
            else:
                next_state = np.zeros_like(state)
            
            # å­˜å‚¨ç»éªŒ
            self.agent.remember(state, action, step_reward, next_state, done)
            
            # æ›´æ–°çŠ¶æ€
            environment = new_environment
            current_tasks = current_tasks[1:]
            total_reward += step_reward
            total_makespan = environment.current_time
            step_count += 1
            
            # è®­ç»ƒæ™ºèƒ½ä½“
            if step_count % 4 == 0:  # æ¯4æ­¥è®­ç»ƒä¸€æ¬¡
                loss = self.agent.replay()
            
            if done:
                break

        # è®¡ç®—æœ€ç»ˆå¥–åŠ± (æ–° makespan ç¨€ç–å¥–åŠ±)
        final_reward = compute_final_reward(EpisodeStats(makespan=total_makespan))
        total_reward += final_reward
        # å¯é€‰ï¼šå°†æœ€ç»ˆå¥–åŠ±å†™å…¥æœ€åä¸€ä¸ª transitionï¼ˆå¯åœ¨æœªæ¥æ‰©å±•ï¼‰

        if debug_file:
            try:
                debug_file.write(f"FINAL\tmakespan={total_makespan:.4f}\treward={final_reward:.4f}\n")
                debug_file.close()
            except Exception:
                pass

        return {
            'total_reward': total_reward,
            'makespan': total_makespan,
            'step_count': step_count,
            'epsilon': self.agent.epsilon
        }
    
    def train(self, episodes: int = 1000) -> Dict[str, Any]:
        """è®­ç»ƒDRLæ™ºèƒ½ä½“"""
        print(f"ğŸš€ å¼€å§‹æ”¹è¿›çš„DRLè®­ç»ƒ: {episodes} episodes (é…ç½® episodes={self.drl_cfg.get('episodes', episodes)})")
        
        # åˆå§‹åŒ–æ™ºèƒ½ä½“
        state_dim = 5 + 4 * 4 + 3  # ä»»åŠ¡ç‰¹å¾ + èŠ‚ç‚¹ç‰¹å¾ + ç¯å¢ƒç‰¹å¾
        action_dim = 4  # 4ä¸ªèŠ‚ç‚¹
        
        self.agent = ImprovedDQNAgent(
            state_dim=state_dim,
            action_dim=action_dim,
            **self.config.get('drl', {})
        )
        
        # è®­ç»ƒå¾ªç¯
        best_makespan = float('inf')
        recent_rewards = deque(maxlen=100)
        
        log_interval = self.drl_cfg.get('log_interval', 50)
        eval_interval = self.drl_cfg.get('eval_interval', 100)
        checkpoint_interval = self.drl_cfg.get('checkpoint_interval', 100)
        rolling_window = self.drl_cfg.get('rolling_window', 100)
        metrics_path = self.logging_cfg.get('metrics_file', 'results/training_metrics.jsonl')
        ckpt_dir = Path(self.checkpoint_cfg.get('dir', 'models/checkpoints/'))
        keep_last = self.checkpoint_cfg.get('keep_last', 5)
        save_best = self.checkpoint_cfg.get('save_best', True)
        kept_ckpts = []

        for episode in range(episodes):
            episode_results = self.train_episode()
            self.training_history.append(episode_results)
            recent_rewards.append(episode_results['total_reward'])
            
            # æ›´æ–°æœ€ä½³æ€§èƒ½
            if episode_results['makespan'] < best_makespan:
                best_makespan = episode_results['makespan']
            
            # æ‰“å°è¿›åº¦
            if episode % log_interval == 0:
                avg_reward = np.mean(recent_rewards) if recent_rewards else 0
                print(f"Episode {episode}: "
                      f"å¹³å‡å¥–åŠ±={avg_reward:.3f}, "
                      f"Makespan={episode_results['makespan']:.2f}, "
                      f"Îµ={episode_results['epsilon']:.3f}")
            # å†™å…¥æµå¼æŒ‡æ ‡æ—¥å¿—
            try:
                with open(metrics_path, 'a') as mf:
                    mf.write(json.dumps({
                        'episode': episode,
                        'reward': episode_results['total_reward'],
                        'makespan': episode_results['makespan'],
                        'epsilon': episode_results['epsilon'],
                        'timestamp': time.time()
                    }) + '\n')
            except Exception as e:
                print(f"âš ï¸ å†™å…¥æŒ‡æ ‡æ—¥å¿—å¤±è´¥: {e}")

            # æ£€æŸ¥ç‚¹ä¿å­˜
            if checkpoint_interval and episode % checkpoint_interval == 0 and episode > 0:
                ckpt_path = ckpt_dir / f"episode_{episode}.pth"
                self.save_model(str(ckpt_path))
                kept_ckpts.append(ckpt_path)
                # æ§åˆ¶æ•°é‡
                if len(kept_ckpts) > keep_last:
                    old = kept_ckpts.pop(0)
                    try:
                        old.unlink()
                    except Exception:
                        pass
            # ä¿å­˜æœ€ä½³
            if save_best and episode_results['makespan'] < self.best_makespan:
                self.best_makespan = episode_results['makespan']
                best_path = ckpt_dir / 'best_model.pth'
                self.save_model(str(best_path))
            # è¯„ä¼°é’©å­å ä½
            if eval_interval and episode % eval_interval == 0 and episode > 0:
                pass  # å¯åœ¨æ­¤æ¥å…¥éªŒè¯ç¯å¢ƒ
        
        # è®­ç»ƒå®Œæˆç»Ÿè®¡
        final_avg_reward = np.mean([h['total_reward'] for h in self.training_history[-100:]])
        final_avg_makespan = np.mean([h['makespan'] for h in self.training_history[-100:]])
        
        training_summary = {
            'total_episodes': episodes,
            'final_avg_reward': final_avg_reward,
            'final_avg_makespan': final_avg_makespan,
            'best_makespan': best_makespan,
            'training_history': self.training_history
        }
        print(f"âœ… DRLè®­ç»ƒå®Œæˆ! (æ–°å¥–åŠ±æ¡†æ¶é›†æˆ)")
        print(f"   æœ€ç»ˆå¹³å‡å¥–åŠ±: {final_avg_reward:.3f}")
        print(f"   æœ€ç»ˆå¹³å‡Makespan: {final_avg_makespan:.2f}s")
        print(f"   æœ€ä½³Makespan: {best_makespan:.2f}s")

        return training_summary
    
    def save_model(self, model_path: str):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        if self.agent is None:
            raise ValueError("æ™ºèƒ½ä½“å°šæœªåˆå§‹åŒ–æˆ–è®­ç»ƒ")
        
        checkpoint = {
            'q_network_state_dict': self.agent.q_network.state_dict(),
            'target_network_state_dict': self.agent.target_network.state_dict(),
            'optimizer_state_dict': self.agent.optimizer.state_dict(),
            'training_step': self.agent.training_step,
            'epsilon': self.agent.epsilon,
            'config': self.config,
            'training_history': self.training_history,
            'metadata': {
                'state_dim': self.agent.state_dim,
                'action_dim': self.agent.action_dim,
                'training_completed': True,
                'final_performance': {
                    'avg_makespan': np.mean([h['makespan'] for h in self.training_history[-100:]]),
                    'best_makespan': min(h['makespan'] for h in self.training_history)
                }
            }
        }
        
        torch.save(checkpoint, model_path)
        print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        # ä¿å­˜è®­ç»ƒå†å²
        history_path = model_path.replace('.pth', '_history.json')
        with open(history_path, 'w') as f:
            json.dump(self.training_history, f, indent=2)
        print(f"ğŸ“Š è®­ç»ƒå†å²å·²ä¿å­˜: {history_path}")

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='WASS-RAG æ”¹è¿›DRLè®­ç»ƒå™¨')
    parser.add_argument('--config', default='configs/experiment.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--episodes', type=int, default=1000, help='è®­ç»ƒepisodeæ•°')
    parser.add_argument('--output', default='models/improved_wass_drl.pth', help='è¾“å‡ºæ¨¡å‹è·¯å¾„')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(args.output).parent.mkdir(parents=True, exist_ok=True)
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = WRENCHDRLTrainer(args.config)
    
    # è®­ç»ƒ
    results = trainer.train(args.episodes)
    
    # ä¿å­˜æ¨¡å‹
    trainer.save_model(args.output)
    
    # ä¿å­˜è®­ç»ƒç»“æœæ‘˜è¦
    summary_path = args.output.replace('.pth', '_summary.json')
    with open(summary_path, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"ğŸ‰ è®­ç»ƒå®Œæˆ! æ¨¡å‹å’Œç»“æœå·²ä¿å­˜åˆ° {Path(args.output).parent}")

if __name__ == "__main__":
    main()
