#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
WASS-RAG æ”¹è¿›çš„DRLè®­ç»ƒå™¨
å®ç°å¯†é›†å¥–åŠ±å‡½æ•°ä»¥æé«˜è®­ç»ƒæ•ˆæœ
"""

import os
import sys
import json
import time
import random
from pathlib import Path
from typing import Dict, List, Tuple, Any
from dataclasses import dataclass
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque, namedtuple
import yaml

# å…ˆæ·»åŠ é¡¹ç›®è·¯å¾„å†å¯¼å…¥æœ¬åœ°åŒ…
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

from src.drl.reward import compute_step_reward, compute_final_reward, StepContext, EpisodeStats, WEIGHTS  # noqa: E402
import math

# ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨ï¼ˆå†’çƒŸè¿è¡Œå¯èƒ½å°šæœªåˆ›å»ºï¼‰
Path('results').mkdir(exist_ok=True)

@dataclass
class TaskState:
    """ä»»åŠ¡çŠ¶æ€"""
    id: str
    computation_size: float
    parents: List[str]
    children: List[str]
    is_critical_path: bool
    data_locality_score: float

@dataclass
class NodeState:
    """èŠ‚ç‚¹çŠ¶æ€"""
    id: str
    speed: float
    current_load: float
    available_time: float
    data_availability: Dict[str, float]  # æ•°æ®å¯ç”¨æ€§è¯„åˆ†

@dataclass
class EnvironmentState:
    """ç¯å¢ƒçŠ¶æ€"""
    current_time: float
    pending_tasks: List[TaskState]
    node_states: List[NodeState]
    workflow_progress: float
    critical_path_length: float

class DenseRewardCalculator:
    """(Deprecated soon) ä¿ç•™æ—§æ¥å£ä»¥é˜²å…¼å®¹é—®é¢˜ï¼Œä½†å†…éƒ¨å§”æ‰˜åˆ°æ–° shapingã€‚"""
    def calculate_step_reward(self, task: TaskState, chosen_node: NodeState, all_nodes: List[NodeState], environment: EnvironmentState) -> float:
        # æ—§æ¥å£ä¿æŒä½†ç°åœ¨ç›´æ¥è¿”å›0ï¼ˆé¿å…è¯¯ç”¨ï¼‰ï¼ŒçœŸå®å¥–åŠ±åœ¨è®­ç»ƒå¾ªç¯å¤–éƒ¨æ„é€  StepContext
        return 0.0
    def calculate_final_reward(self, final_makespan: float, baseline_makespan: float) -> float:
        stats = EpisodeStats(makespan=final_makespan)
        return compute_final_reward(stats)

class AdvancedDQN(nn.Module):
    """é«˜çº§DQNç½‘ç»œï¼Œå¼•å…¥æ³¨æ„åŠ›æœºåˆ¶å’Œæ›´æ·±å±‚æ¬¡ç»“æ„"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dims: List[int] = None):
        super().__init__()
        
        if hidden_dims is None:
            hidden_dims = [512, 256, 128, 64]  # æ›´æ·±çš„ç½‘ç»œç»“æ„
        self.hidden_dims = hidden_dims
        
        # ç‰¹å¾æå–å±‚
        feature_layers = []
        current_dim = state_dim
        
        for i, hidden_dim in enumerate(hidden_dims[:-1]):
            feature_layers.extend([
                nn.Linear(current_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),  # ä½¿ç”¨LayerNormä»£æ›¿BatchNormï¼Œå¯¹å°æ‰¹é‡æ›´ç¨³å®š
                nn.ReLU(),
                nn.Dropout(0.1 if i < 2 else 0.05)  # å‰å±‚ä½¿ç”¨æ›´é«˜dropout
            ])
            current_dim = hidden_dim
        
        self.feature_extractor = nn.Sequential(*feature_layers)
        
        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.Sequential(
            nn.Linear(current_dim, current_dim // 2),
            nn.Tanh(),
            nn.Linear(current_dim // 2, 1)
        )
        
        # ä»·å€¼æµ (Value Stream)
        value_layers = []
        value_input_dim = current_dim
        for hidden_dim in [hidden_dims[-1], hidden_dims[-1] // 2]:
            value_layers.extend([
                nn.Linear(value_input_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.05)
            ])
            value_input_dim = hidden_dim
        value_layers.append(nn.Linear(value_input_dim, 1))
        self.value_stream = nn.Sequential(*value_layers)
        
        # ä¼˜åŠ¿æµ (Advantage Stream)
        advantage_layers = []
        advantage_input_dim = current_dim
        for hidden_dim in [hidden_dims[-1], hidden_dims[-1] // 2]:
            advantage_layers.extend([
                nn.Linear(advantage_input_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.05)
            ])
            advantage_input_dim = hidden_dim
        advantage_layers.append(nn.Linear(advantage_input_dim, action_dim))
        self.advantage_stream = nn.Sequential(*advantage_layers)
        
        # ä½¿ç”¨Xavieråˆå§‹åŒ–
        self._initialize_weights()
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
    
    def forward(self, x):
        # ç‰¹å¾æå–
        features = self.feature_extractor(x)
        
        # åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶
        attention_weights = self.attention(features)
        attention_weights = torch.softmax(attention_weights, dim=0)
        attended_features = features * attention_weights
        
        # Dueling DQNæ¶æ„
        value = self.value_stream(attended_features)
        advantage = self.advantage_stream(attended_features)
        
        # Qå€¼ = ä»·å€¼ + (ä¼˜åŠ¿ - å¹³å‡ä¼˜åŠ¿)
        q_values = value + advantage - advantage.mean(dim=1, keepdim=True)
        
        return q_values

class ImprovedDQN(nn.Module):
    """æ”¹è¿›çš„DQNç½‘ç»œ - ä¿ç•™åŸå®ç°ä½œä¸ºå¤‡é€‰"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dims: List[int] = None):
        super().__init__()
        
        if hidden_dims is None:
            hidden_dims = [256, 128, 64]
        
        layers = []
        current_dim = state_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(current_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.1)
            ])
            current_dim = hidden_dim
        
        layers.append(nn.Linear(current_dim, action_dim))
        
        self.network = nn.Sequential(*layers)
        
        # ä½¿ç”¨Xavieråˆå§‹åŒ–
        for layer in self.network:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.zeros_(layer.bias)
    
    def forward(self, x):
        return self.network(x)

class ImprovedDQNAgent:
    """æ”¹è¿›çš„DQNæ™ºèƒ½ä½“ï¼Œæ”¯æŒå¤šç§ç½‘ç»œæ¶æ„"""
    
    def __init__(self, state_dim: int, action_dim: int, learning_rate: float = 0.001,
                 epsilon_start: float = 1.0,
                 epsilon_end: float = 0.1,
                 epsilon_decay: float = 0.995,
                 gamma: float = 0.99,
                 memory_size: int = 10000,
                 batch_size: int = 64,
                 target_update_freq: int = 100,
                 device: str = None,
                 network_type: str = "advanced",  # æ–°å¢ï¼šç½‘ç»œç±»å‹é€‰æ‹©
                 hidden_dims: List[int] = None,
                 exploration_strategy: str = "adaptive_epsilon",  # æ–°å¢ï¼šæ¢ç´¢ç­–ç•¥ç±»å‹
                 use_ucb: bool = False,  # æ–°å¢ï¼šæ˜¯å¦ä½¿ç”¨UCBæ¢ç´¢
                 ucb_c: float = 2.0,  # æ–°å¢ï¼šUCBæ¢ç´¢å‚æ•°
                 use_boltzmann: bool = False,  # æ–°å¢ï¼šæ˜¯å¦ä½¿ç”¨ç»å°”å…¹æ›¼æ¢ç´¢
                 boltzmann_tau: float = 1.0):  # æ–°å¢ï¼šç»å°”å…¹æ›¼æ¸©åº¦å‚æ•°
        
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.batch_size = batch_size
        self.target_update_freq = target_update_freq
        self.network_type = network_type
        
        # æ¢ç´¢ç­–ç•¥å‚æ•°
        self.exploration_strategy = exploration_strategy
        self.use_ucb = use_ucb
        self.ucb_c = ucb_c
        self.use_boltzmann = use_boltzmann
        self.boltzmann_tau = boltzmann_tau
        
        # åŠ¨ä½œè®¿é—®è®¡æ•°ï¼ˆç”¨äºUCBæ¢ç´¢ï¼‰
        self.action_counts = np.zeros(action_dim)
        self.action_values = np.zeros(action_dim)
        
        # æ¢ç´¢ç»Ÿè®¡
        self.exploration_history = deque(maxlen=1000)
        self.exploitation_history = deque(maxlen=1000)
        self.recent_rewards = deque(maxlen=100)  # ç”¨äºè‡ªé€‚åº”æ¢ç´¢ç‡è°ƒæ•´
        
        # æ ¹æ®ç½‘ç»œç±»å‹é€‰æ‹©ç½‘ç»œæ¶æ„
        if network_type == "advanced":
            # ä½¿ç”¨é«˜çº§DQNç½‘ç»œï¼ˆå¸¦æ³¨æ„åŠ›å’ŒDuelingç»“æ„ï¼‰
            self.q_network = AdvancedDQN(state_dim, action_dim, hidden_dims).to(self.device)
            self.target_network = AdvancedDQN(state_dim, action_dim, hidden_dims).to(self.device)
        else:
            # ä½¿ç”¨æ ‡å‡†DQNç½‘ç»œ
            self.q_network = ImprovedDQN(state_dim, action_dim, hidden_dims).to(self.device)
            self.target_network = ImprovedDQN(state_dim, action_dim, hidden_dims).to(self.device)
        
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)
        
        # ç»éªŒå›æ”¾
        self.memory = deque(maxlen=memory_size)
        self.experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])
        
        # æ¢ç´¢ç­–ç•¥
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
        
        # è‡ªé€‚åº”æ¢ç´¢å‚æ•°
        self.performance_window = deque(maxlen=50)  # æ€§èƒ½çª—å£
        self.stable_performance_threshold = 0.05  # æ€§èƒ½ç¨³å®šé˜ˆå€¼
        self.min_epsilon = epsilon_end
        self.max_epsilon = epsilon_start
        
        # è®­ç»ƒç»Ÿè®¡
        self.training_step = 0
        self.update_target()
        
        # æ€§èƒ½ç›‘æ§
        self.loss_history = deque(maxlen=1000)
        self.q_value_history = deque(maxlen=1000)
    
    def update_target(self):
        """æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        self.target_network.load_state_dict(self.q_network.state_dict())
    
    def update_action_dim(self, new_action_dim):
        """æ›´æ–°åŠ¨ä½œç»´åº¦ï¼Œç”¨äºè¯¾ç¨‹å­¦ä¹ é˜¶æ®µåˆ‡æ¢"""
        if new_action_dim == self.action_dim:
            return  # åŠ¨ä½œç»´åº¦æ²¡æœ‰å˜åŒ–ï¼Œæ— éœ€æ›´æ–°
        
        print(f"ğŸ”„ æ›´æ–°åŠ¨ä½œç»´åº¦: {self.action_dim} -> {new_action_dim}")
        
        # ä¿å­˜å½“å‰ç½‘ç»œçŠ¶æ€
        q_network_state = self.q_network.state_dict()
        target_network_state = self.target_network.state_dict()
        
        # æ›´æ–°åŠ¨ä½œç»´åº¦
        self.action_dim = new_action_dim
        
        # é‡æ–°åˆ›å»ºç½‘ç»œ
        if self.network_type == "advanced":
            # ä½¿ç”¨é«˜çº§DQNç½‘ç»œï¼ˆå¸¦æ³¨æ„åŠ›å’ŒDuelingç»“æ„ï¼‰
            self.q_network = AdvancedDQN(self.state_dim, self.action_dim, self.q_network.hidden_dims).to(self.device)
            self.target_network = AdvancedDQN(self.state_dim, self.action_dim, self.target_network.hidden_dims).to(self.device)
        else:
            # ä½¿ç”¨æ ‡å‡†DQNç½‘ç»œ
            self.q_network = ImprovedDQN(self.state_dim, self.action_dim, self.q_network.hidden_dims).to(self.device)
            self.target_network = ImprovedDQN(self.state_dim, self.action_dim, self.target_network.hidden_dims).to(self.device)
        
        # å°è¯•åŠ è½½ä¹‹å‰çš„çŠ¶æ€ï¼ˆä»…åŠ è½½å…¼å®¹çš„å±‚ï¼‰
        try:
            # å¯¹äºå…¼å®¹çš„å±‚ï¼ŒåŠ è½½ä¹‹å‰çš„çŠ¶æ€
            q_compatible_state = {k: v for k, v in q_network_state.items() if k in self.q_network.state_dict() and v.shape == self.q_network.state_dict()[k].shape}
            target_compatible_state = {k: v for k, v in target_network_state.items() if k in self.target_network.state_dict() and v.shape == self.target_network.state_dict()[k].shape}
            
            self.q_network.load_state_dict(q_compatible_state, strict=False)
            self.target_network.load_state_dict(target_compatible_state, strict=False)
            
            print(f"âœ… æˆåŠŸè¿ç§»ç½‘ç»œçŠ¶æ€åˆ°æ–°çš„åŠ¨ä½œç»´åº¦")
        except Exception as e:
            print(f"âš ï¸ è¿ç§»ç½‘ç»œçŠ¶æ€å¤±è´¥: {e}ï¼Œå°†é‡æ–°åˆå§‹åŒ–ç½‘ç»œ")
        
        # é‡æ–°åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.optimizer.param_groups[0]['lr'])
        
        # é‡ç½®åŠ¨ä½œè®¿é—®è®¡æ•°ï¼ˆç”¨äºUCBæ¢ç´¢ï¼‰
        self.action_counts = np.zeros(self.action_dim)
        self.action_values = np.zeros(self.action_dim)
        
        # æ¸…ç©ºç»éªŒå›æ”¾ï¼Œå› ä¸ºæ—§ç»éªŒå¯èƒ½ä¸é€‚ç”¨äºæ–°çš„åŠ¨ä½œç©ºé—´
        self.memory.clear()
        print(f"ğŸ§¹ æ¸…ç©ºç»éªŒå›æ”¾ï¼Œä»¥é€‚åº”æ–°çš„åŠ¨ä½œç©ºé—´")
    
    def remember(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒ"""
        experience = self.experience(state, action, reward, next_state, done)
        self.memory.append(experience)
    
    def act(self, state, training=True):
        """é€‰æ‹©åŠ¨ä½œï¼Œæ”¯æŒå¤šç§æ¢ç´¢ç­–ç•¥"""
        if not training:
            # åœ¨æµ‹è¯•é˜¶æ®µï¼Œç›´æ¥é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                q_values = self.q_network(state_tensor)
                return q_values.argmax().item()
        
        # æ ¹æ®æ¢ç´¢ç­–ç•¥é€‰æ‹©åŠ¨ä½œ
        if self.exploration_strategy == "adaptive_epsilon":
            return self._act_with_adaptive_epsilon(state)
        elif self.exploration_strategy == "boltzmann" or self.use_boltzmann:
            return self._act_with_boltzmann(state)
        elif self.exploration_strategy == "ucb" or self.use_ucb:
            return self._act_with_ucb(state)
        else:
            # é»˜è®¤ä½¿ç”¨æ ‡å‡†Îµ-è´ªå©ªç­–ç•¥
            if random.random() < self.epsilon:
                self.exploration_history.append(1)
                return random.randint(0, self.action_dim - 1)
            else:
                self.exploitation_history.append(1)
                with torch.no_grad():
                    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                    q_values = self.q_network(state_tensor)
                    return q_values.argmax().item()
    
    def _act_with_adaptive_epsilon(self, state):
        """ä½¿ç”¨è‡ªé€‚åº”Îµ-è´ªå©ªç­–ç•¥é€‰æ‹©åŠ¨ä½œ"""
        # æ ¹æ®æ€§èƒ½ç¨³å®šæ€§è°ƒæ•´æ¢ç´¢ç‡
        if len(self.performance_window) >= 20:
            recent_performance = list(self.performance_window)[-20:]
            if np.std(recent_performance) < self.stable_performance_threshold * np.mean(recent_performance):
                # æ€§èƒ½ç¨³å®šï¼Œå‡å°‘æ¢ç´¢
                adaptive_epsilon = max(self.min_epsilon, self.epsilon * 0.9)
            else:
                # æ€§èƒ½ä¸ç¨³å®šï¼Œå¢åŠ æ¢ç´¢
                adaptive_epsilon = min(self.max_epsilon, self.epsilon * 1.1)
        else:
            adaptive_epsilon = self.epsilon
        
        if random.random() < adaptive_epsilon:
            self.exploration_history.append(1)
            return random.randint(0, self.action_dim - 1)
        else:
            self.exploitation_history.append(1)
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                q_values = self.q_network(state_tensor)
                return q_values.argmax().item()
    
    def _act_with_boltzmann(self, state):
        """ä½¿ç”¨ç»å°”å…¹æ›¼æ¢ç´¢ç­–ç•¥é€‰æ‹©åŠ¨ä½œ"""
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            q_values = self.q_network(state_tensor).squeeze().cpu().numpy()
            
            # åº”ç”¨ç»å°”å…¹æ›¼åˆ†å¸ƒ
            # æ¸©åº¦å‚æ•°éšæ—¶é—´è¡°å‡ï¼Œä»æ¢ç´¢è½¬å‘åˆ©ç”¨
            current_tau = self.boltzmann_tau * (self.epsilon_decay ** self.training_step)
            current_tau = max(0.1, current_tau)  # ç¡®ä¿æ¸©åº¦ä¸ä¼šè¿‡ä½
            
            # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
            exp_q = np.exp(q_values / current_tau)
            probs = exp_q / np.sum(exp_q)
            
            # æ ¹æ®æ¦‚ç‡åˆ†å¸ƒé€‰æ‹©åŠ¨ä½œ
            action = np.random.choice(self.action_dim, p=probs)
            
            # è®°å½•æ¢ç´¢æˆ–åˆ©ç”¨
            if probs.max() < 0.8:  # å¦‚æœæœ€å¤§æ¦‚ç‡å°äº0.8ï¼Œè®¤ä¸ºæ˜¯æ¢ç´¢
                self.exploration_history.append(1)
            else:
                self.exploitation_history.append(1)
            
            return action
    
    def _act_with_ucb(self, state):
        """ä½¿ç”¨UCBï¼ˆUpper Confidence Boundï¼‰æ¢ç´¢ç­–ç•¥é€‰æ‹©åŠ¨ä½œ"""
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            q_values = self.q_network(state_tensor).squeeze().cpu().numpy()
            
            # è®¡ç®—UCBå€¼
            total_counts = np.sum(self.action_counts)
            if total_counts == 0:
                # å¦‚æœæ‰€æœ‰åŠ¨ä½œéƒ½æœªè¢«å°è¯•è¿‡ï¼Œéšæœºé€‰æ‹©
                action = random.randint(0, self.action_dim - 1)
                self.exploration_history.append(1)
                return action
            
            ucb_values = q_values + self.ucb_c * np.sqrt(np.log(total_counts) / (self.action_counts + 1e-6))
            
            # é€‰æ‹©UCBå€¼æœ€å¤§çš„åŠ¨ä½œ
            action = ucb_values.argmax()
            
            # æ›´æ–°åŠ¨ä½œè®¡æ•°
            self.action_counts[action] += 1
            
            # è®°å½•æ¢ç´¢æˆ–åˆ©ç”¨
            if self.action_counts[action] <= np.mean(self.action_counts):
                self.exploration_history.append(1)
            else:
                self.exploitation_history.append(1)
            
            return action
    
    def replay(self):
        """ç»éªŒå›æ”¾è®­ç»ƒï¼ŒåŠ å…¥æ€§èƒ½ç›‘æ§å’Œæ”¹è¿›è®­ç»ƒè¿‡ç¨‹"""
        if len(self.memory) < self.batch_size:
            return None
        
        # é‡‡æ ·ç»éªŒ
        batch = random.sample(self.memory, self.batch_size)
        states = torch.FloatTensor(np.array([e.state for e in batch])).to(self.device)
        actions = torch.LongTensor([e.action for e in batch]).to(self.device)
        rewards = torch.FloatTensor([e.reward for e in batch]).to(self.device)
        next_states = torch.FloatTensor(np.array([e.next_state for e in batch])).to(self.device)
        dones = torch.BoolTensor([e.done for e in batch]).to(self.device)
        
        # è®¡ç®—Qå€¼
        current_q = self.q_network(states).gather(1, actions.unsqueeze(1))
        
        # Double DQNï¼šä½¿ç”¨å½“å‰ç½‘ç»œé€‰æ‹©åŠ¨ä½œï¼Œç›®æ ‡ç½‘ç»œè¯„ä¼°Qå€¼
        if self.network_type == "advanced":
            # å¯¹äºé«˜çº§ç½‘ç»œï¼Œä½¿ç”¨Double DQN
            next_actions = self.q_network(next_states).argmax(dim=1, keepdim=True)
            next_q = self.target_network(next_states).gather(1, next_actions).detach()
        else:
            # å¯¹äºæ ‡å‡†ç½‘ç»œï¼Œä½¿ç”¨åŸå§‹æ–¹æ³•
            next_q = self.target_network(next_states).max(1)[0].detach()
        
        # è®¡ç®—ç›®æ ‡Qå€¼
        target_q = rewards + (self.gamma * next_q.squeeze() * ~dones)
        
        # è®¡ç®—æŸå¤±ï¼Œä½¿ç”¨HuberæŸå¤±ä»£æ›¿MSEæŸå¤±ï¼Œå¯¹å¼‚å¸¸å€¼æ›´é²æ£’
        loss = nn.SmoothL1Loss()(current_q.squeeze(), target_q)
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ªï¼Œä½¿ç”¨åŠ¨æ€é˜ˆå€¼
        if self.network_type == "advanced":
            # å¯¹äºæ›´æ·±çš„ç½‘ç»œï¼Œä½¿ç”¨æ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 0.5)
        else:
            torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)
        
        self.optimizer.step()
        
        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        self.loss_history.append(loss.item())
        avg_q = current_q.mean().item()
        self.q_value_history.append(avg_q)
        
        # æ›´æ–°æ¢ç´¢ç‡ï¼Œä½¿ç”¨æŒ‡æ•°è¡°å‡
        if self.epsilon > self.epsilon_end:
            self.epsilon *= self.epsilon_decay
        
        # å®šæœŸæ›´æ–°ç›®æ ‡ç½‘ç»œï¼Œä½¿ç”¨è½¯æ›´æ–°
        self.training_step += 1
        if self.training_step % self.target_update_freq == 0:
            if self.network_type == "advanced":
                # å¯¹äºé«˜çº§ç½‘ç»œï¼Œä½¿ç”¨è½¯æ›´æ–°
                tau = 0.001  # è½¯æ›´æ–°ç³»æ•°
                for target_param, param in zip(self.target_network.parameters(), self.q_network.parameters()):
                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)
            else:
                # å¯¹äºæ ‡å‡†ç½‘ç»œï¼Œä½¿ç”¨ç¡¬æ›´æ–°
                self.update_target()
        
        return loss.item()
    
    def update_exploration_parameters(self, episode_reward=None):
        """æ›´æ–°æ¢ç´¢å‚æ•°ï¼Œæ ¹æ®è®­ç»ƒè¿›åº¦å’Œæ€§èƒ½è°ƒæ•´æ¢ç´¢ç­–ç•¥"""
        # æ›´æ–°æ¢ç´¢ç‡ï¼ˆÎµ-è´ªå©ªç›¸å…³ï¼‰
        if self.epsilon > self.epsilon_end:
            self.epsilon *= self.epsilon_decay
        
        # æ›´æ–°ç»å°”å…¹æ›¼æ¸©åº¦å‚æ•°
        if self.use_boltzmann:
            # æ¸©åº¦éšæ—¶é—´è¡°å‡ï¼Œä»æ¢ç´¢è½¬å‘åˆ©ç”¨
            self.boltzmann_tau *= 0.9995
            self.boltzmann_tau = max(0.1, self.boltzmann_tau)
        
        # æ›´æ–°æ€§èƒ½çª—å£
        if episode_reward is not None:
            self.performance_window.append(episode_reward)
            
            # åŸºäºæ€§èƒ½è‡ªé€‚åº”è°ƒæ•´æ¢ç´¢ç­–ç•¥
            if len(self.performance_window) >= 20:
                recent_performance = list(self.performance_window)[-20:]
                perf_std = np.std(recent_performance)
                perf_mean = np.mean(recent_performance)
                
                # å¦‚æœæ€§èƒ½æ³¢åŠ¨å¤§ï¼Œå¢åŠ æ¢ç´¢
                if perf_std > self.stable_performance_threshold * perf_mean:
                    if self.exploration_strategy == "adaptive_epsilon":
                        self.epsilon = min(self.max_epsilon, self.epsilon * 1.05)
                    elif self.use_boltzmann:
                        self.boltzmann_tau = min(2.0, self.boltzmann_tau * 1.05)
                # å¦‚æœæ€§èƒ½ç¨³å®šï¼Œå‡å°‘æ¢ç´¢
                elif perf_std < 0.5 * self.stable_performance_threshold * perf_mean:
                    if self.exploration_strategy == "adaptive_epsilon":
                        self.epsilon = max(self.min_epsilon, self.epsilon * 0.95)
                    elif self.use_boltzmann:
                        self.boltzmann_tau = max(0.1, self.boltzmann_tau * 0.95)
    
    def get_exploration_stats(self):
        """è·å–æ¢ç´¢ç»Ÿè®¡ä¿¡æ¯"""
        exploration_rate = sum(self.exploration_history) / max(1, len(self.exploration_history))
        exploitation_rate = sum(self.exploitation_history) / max(1, len(self.exploitation_history))
        
        return {
            'exploration_rate': exploration_rate,
            'exploitation_rate': exploitation_rate,
            'current_epsilon': self.epsilon,
            'boltzmann_tau': self.boltzmann_tau if self.use_boltzmann else None,
            'ucb_c': self.ucb_c if self.use_ucb else None,
            'action_distribution': self.action_counts / max(1, np.sum(self.action_counts)),
            'exploration_strategy': self.exploration_strategy
        }
    
    def get_performance_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        if not self.loss_history:
            return {}
        
        return {
            'avg_loss': sum(self.loss_history) / len(self.loss_history),
            'avg_q_value': sum(self.q_value_history) / len(self.q_value_history),
            'current_epsilon': self.epsilon,
            'memory_size': len(self.memory),
            'network_type': self.network_type
        }

class WRENCHDRLTrainer:
    """WRENCH DRLè®­ç»ƒå™¨"""
    
    def __init__(self, config_path: str):
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.reward_calculator = DenseRewardCalculator()
        self.agent = None
        self.training_history = []
        # æ–°å¢ï¼šè¯»å–æ‰©å±•è®­ç»ƒ/æ—¥å¿—é…ç½®
        self.drl_cfg = self.config.get('drl', {})
        self.checkpoint_cfg = self.config.get('checkpoint', {})
        self.logging_cfg = self.config.get('logging', {})
        Path(self.checkpoint_cfg.get('dir', 'models/checkpoints/')).mkdir(parents=True, exist_ok=True)
        Path(self.logging_cfg.get('metrics_file', 'results/training_metrics.jsonl')).parent.mkdir(parents=True, exist_ok=True)
        self.best_makespan = float('inf')
        
        # è¯¾ç¨‹å­¦ä¹ ç›¸å…³å‚æ•°
        self.curriculum_stages = [
            {"name": "å…¥é—¨åœºæ™¯", "tasks": 5, "nodes": 4, "complexity": 0.3, "episodes": 300},
            {"name": "ä¸­çº§åœºæ™¯", "tasks": 10, "nodes": 4, "complexity": 0.6, "episodes": 400},
            {"name": "é«˜çº§åœºæ™¯", "tasks": 15, "nodes": 4, "complexity": 0.8, "episodes": 300},
            {"name": "çœŸå®åœºæ™¯", "tasks": 20, "nodes": 4, "complexity": 1.0, "episodes": 200}
        ]
        self.current_stage = 0
        self.stage_episodes_completed = 0
        self.stage_performance_history = []
        
        # è‡ªé€‚åº”å­¦ä¹ ç‡å‚æ•°
        self.learning_rate_schedule = [
            {"episodes": 200, "lr": 0.001},
            {"episodes": 400, "lr": 0.0005},
            {"episodes": 700, "lr": 0.0001},
            {"episodes": 1000, "lr": 0.00005}
        ]
        
        # å…ˆéªŒçŸ¥è¯†é›†æˆå‚æ•°
        self.use_heuristic_guidance = self.drl_cfg.get('use_heuristic_guidance', True)
        self.heuristic_weight = self.drl_cfg.get('heuristic_weight', 0.3)  # å¯å‘å¼æŒ‡å¯¼çš„åˆå§‹æƒé‡
        self.heuristic_decay = self.drl_cfg.get('heuristic_decay', 0.995)  # å¯å‘å¼æƒé‡è¡°å‡ç‡
    
    def create_mock_environment(self, stage: int = None) -> Tuple[EnvironmentState, List[TaskState], List[NodeState]]:
        """åˆ›å»ºæ¨¡æ‹Ÿè®­ç»ƒç¯å¢ƒï¼Œæ”¯æŒè¯¾ç¨‹å­¦ä¹ çš„ä¸åŒé˜¶æ®µ"""
        # è·å–å½“å‰è¯¾ç¨‹é˜¶æ®µ
        if stage is None:
            stage = self.current_stage
        
        curriculum = self.curriculum_stages[stage]
        num_tasks = curriculum["tasks"]
        num_nodes = curriculum["nodes"]
        complexity = curriculum["complexity"]
        
        print(f"ğŸ“š åˆ›å»ºè¯¾ç¨‹å­¦ä¹ ç¯å¢ƒ: {curriculum['name']} (ä»»åŠ¡æ•°: {num_tasks}, èŠ‚ç‚¹æ•°: {num_nodes}, å¤æ‚åº¦: {complexity})")
        
        # åŸºäºplatform.xmlçš„çœŸå®èŠ‚ç‚¹é…ç½®ï¼Œä½†æ ¹æ®è¯¾ç¨‹é˜¶æ®µè°ƒæ•´
        base_node_configs = [
            {"id": "ComputeHost1", "speed": 2.0, "cores": 4, "disk_speed": 200},
            {"id": "ComputeHost2", "speed": 3.0, "cores": 8, "disk_speed": 250},
            {"id": "ComputeHost3", "speed": 2.5, "cores": 6, "disk_speed": 220},
            {"id": "ComputeHost4", "speed": 4.0, "cores": 16, "disk_speed": 300}
        ]
        
        # æ ¹æ®è¯¾ç¨‹é˜¶æ®µé€‰æ‹©èŠ‚ç‚¹
        node_configs = base_node_configs[:num_nodes]
        
        # æ ¹æ®å¤æ‚åº¦è°ƒæ•´èŠ‚ç‚¹æ€§èƒ½
        for config in node_configs:
            # åœ¨ç®€å•é˜¶æ®µï¼ŒèŠ‚ç‚¹æ€§èƒ½å·®å¼‚è¾ƒå°
            if complexity < 0.5:
                speed_factor = 0.8 + 0.4 * random.random()  # 0.8-1.2å€
            else:
                speed_factor = 0.5 + 1.5 * random.random()  # 0.5-2.0å€
            
            config["speed"] *= speed_factor
            config["cores"] = max(2, int(config["cores"] * speed_factor))
            config["disk_speed"] *= speed_factor
        
        # åˆ›å»ºèŠ‚ç‚¹çŠ¶æ€
        node_states = []
        for config in node_configs:
            # æ ¹æ®å¤æ‚åº¦è°ƒæ•´åˆå§‹è´Ÿè½½
            if complexity < 0.5:
                initial_load = random.uniform(0, 0.2)  # ç®€å•é˜¶æ®µè´Ÿè½½è¾ƒä½
            else:
                initial_load = random.uniform(0, 0.4)  # å¤æ‚é˜¶æ®µè´Ÿè½½è¾ƒé«˜
            
            node_states.append(NodeState(
                id=config["id"],
                speed=config["speed"],
                current_load=initial_load,
                available_time=random.uniform(0, 5 * (2 - complexity)),  # å¤æ‚åº¦è¶Šé«˜ï¼Œåˆå§‹å¯ç”¨æ—¶é—´è¶ŠçŸ­
                data_availability={f"task_{j}": random.random() for j in range(num_tasks)}
            ))
        
        # åˆ›å»ºä»»åŠ¡çŠ¶æ€ï¼Œè€ƒè™‘å·¥ä½œæµç»“æ„å’Œå¤æ‚åº¦
        task_states = []
        
        # æ ¹æ®å¤æ‚åº¦è°ƒæ•´ä»»åŠ¡å¤§å°åˆ†å¸ƒ
        if complexity < 0.3:
            # ç®€å•é˜¶æ®µï¼šä»»åŠ¡å¤§å°å·®å¼‚å°
            task_sizes = [(1e9, 3e9)]
        elif complexity < 0.7:
            # ä¸­ç­‰é˜¶æ®µï¼šä»»åŠ¡å¤§å°æœ‰å·®å¼‚
            task_sizes = [(1e9, 3e9), (3e9, 1e10)]
        else:
            # å¤æ‚é˜¶æ®µï¼šä»»åŠ¡å¤§å°å·®å¼‚å¤§
            task_sizes = [(1e9, 3e9), (3e9, 1e10), (1e10, 5e10)]
        
        # ç”Ÿæˆä»»åŠ¡ï¼Œè€ƒè™‘ä¾èµ–å…³ç³»å’Œå¤æ‚åº¦
        for i in range(num_tasks):
            # éšæœºé€‰æ‹©ä»»åŠ¡å¤§å°
            min_size, max_size = random.choice(task_sizes)
            computation_size = random.uniform(min_size, max_size)
            
            # åˆ›å»ºä¾èµ–å…³ç³»ï¼Œå¤æ‚åº¦è¶Šé«˜ä¾èµ–å…³ç³»è¶Šå¤æ‚
            parents = []
            # åŸºç¡€ä¾èµ–æ¦‚ç‡éšå¤æ‚åº¦å¢åŠ 
            base_prob = 0.2 + 0.3 * complexity
            # è¿œè·ç¦»ä¾èµ–æ¦‚ç‡éšå¤æ‚åº¦å¢åŠ 
            long_prob = 0.05 + 0.15 * complexity
            if i > 0:
                if random.random() < base_prob:
                    parents.append(f"task_{i-1}")
                
                if i > 2 and random.random() < long_prob:
                    parents.append(f"task_{random.randint(0, i-2)}")
            
            # åˆ›å»ºå­ä»»åŠ¡å…³ç³»
            children = []
            if i < num_tasks - 1:
                if random.random() < base_prob:
                    children.append(f"task_{i+1}")
                
                if i < num_tasks - 3 and random.random() < long_prob:
                    children.append(f"task_{random.randint(i+2, num_tasks-1)}")
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºå…³é”®è·¯å¾„ä»»åŠ¡ï¼Œå¤æ‚åº¦è¶Šé«˜å…³é”®è·¯å¾„ä»»åŠ¡æ¯”ä¾‹è¶Šé«˜
            critical_prob = 0.1 + 0.2 * complexity
            is_critical_path = random.random() < critical_prob
            
            # è®¡ç®—æ•°æ®å±€éƒ¨æ€§åˆ†æ•°ï¼Œå¤æ‚åº¦è¶Šé«˜æ•°æ®å±€éƒ¨æ€§è¶Šå·®
            if complexity < 0.5:
                data_locality_score = random.uniform(0.6, 1.0)  # ç®€å•é˜¶æ®µæ•°æ®å±€éƒ¨æ€§è¾ƒå¥½
            else:
                data_locality_score = random.uniform(0.3, 0.9)  # å¤æ‚é˜¶æ®µæ•°æ®å±€éƒ¨æ€§è¾ƒå·®
            
            # æ ¹æ®å¤æ‚åº¦è°ƒæ•´æ•°æ®å¤§å°
            if complexity < 0.5:
                data_size = random.uniform(1e6, 5e7)  # ç®€å•é˜¶æ®µæ•°æ®é‡è¾ƒå°
            else:
                data_size = random.uniform(1e6, 1e8)  # å¤æ‚é˜¶æ®µæ•°æ®é‡è¾ƒå¤§
            
            task_states.append(TaskState(
                id=f"task_{i}",
                computation_size=computation_size,
                parents=parents,
                children=children,
                is_critical_path=is_critical_path,
                data_locality_score=data_locality_score
            ))
        
        # åˆ›å»ºç¯å¢ƒçŠ¶æ€ï¼Œæ ¹æ®å¤æ‚åº¦è°ƒæ•´ç½‘ç»œæ¡ä»¶
        if complexity < 0.5:
            # ç®€å•é˜¶æ®µç½‘ç»œæ¡ä»¶å¥½
            network_bandwidth = 1.0 + 0.5 * (1 - complexity)  # 1.0-1.5 GBps
            network_latency = 0.001 * complexity  # 0-0.0005 ms
        else:
            # å¤æ‚é˜¶æ®µç½‘ç»œæ¡ä»¶è¾ƒå·®
            network_bandwidth = 0.5 + 0.5 * (1 - complexity)  # 0.5-1.0 GBps
            network_latency = 0.001 * complexity  # 0.0005-0.001 ms
        
        environment = EnvironmentState(
            current_time=0.0,
            pending_tasks=task_states,
            node_states=node_states,
            workflow_progress=0.0,
            critical_path_length=self._estimate_critical_path_length(task_states, node_states)
        )
        
        return environment, task_states, node_states
    
    def _estimate_critical_path_length(self, task_states: List[TaskState], node_states: List[NodeState]) -> float:
        """ä¼°ç®—å…³é”®è·¯å¾„é•¿åº¦"""
        # æ‰¾å‡ºå…³é”®è·¯å¾„ä¸Šçš„ä»»åŠ¡
        critical_tasks = [t for t in task_states if t.is_critical_path]
        
        if not critical_tasks:
            # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„å…³é”®è·¯å¾„ä»»åŠ¡ï¼Œæ‰¾å‡ºæœ€é•¿è·¯å¾„
            return self._find_longest_path(task_states, node_states)
        
        # è®¡ç®—å…³é”®è·¯å¾„é•¿åº¦
        total_length = 0.0
        fastest_node = max(node_states, key=lambda n: n.speed)
        
        for task in critical_tasks:
            # ä½¿ç”¨æœ€å¿«èŠ‚ç‚¹çš„æ‰§è¡Œæ—¶é—´ä½œä¸ºä¼°ç®—
            execution_time = task.computation_size / fastest_node.speed
            total_length += execution_time
        
        return total_length
    
    def _find_longest_path(self, task_states: List[TaskState], node_states: List[NodeState]) -> float:
        """æ‰¾å‡ºä»»åŠ¡å›¾ä¸­çš„æœ€é•¿è·¯å¾„ï¼ˆå…³é”®è·¯å¾„ï¼‰"""
        # æ„å»ºä»»åŠ¡å›¾
        task_dict = {task.id: task for task in task_states}
        
        # æ‰¾å‡ºæ²¡æœ‰çˆ¶ä»»åŠ¡çš„ä»»åŠ¡ï¼ˆèµ·å§‹ä»»åŠ¡ï¼‰
        start_tasks = [task for task in task_states if not task.parents]
        
        if not start_tasks:
            # å¦‚æœæ²¡æœ‰èµ·å§‹ä»»åŠ¡ï¼ˆå¾ªç¯ä¾èµ–ï¼‰ï¼Œè¿”å›é»˜è®¤å€¼
            return 100.0
        
        # ä½¿ç”¨DFSæ‰¾å‡ºæœ€é•¿è·¯å¾„
        max_path_length = 0.0
        fastest_node = max(node_states, key=lambda n: n.speed)
        
        def dfs(task_id, current_length, visited):
            nonlocal max_path_length
            
            if task_id in visited:
                return
            
            visited.add(task_id)
            task = task_dict[task_id]
            
            # è®¡ç®—å½“å‰ä»»åŠ¡çš„æ‰§è¡Œæ—¶é—´
            execution_time = task.computation_size / fastest_node.speed
            current_length += execution_time
            
            # æ›´æ–°æœ€å¤§è·¯å¾„é•¿åº¦
            max_path_length = max(max_path_length, current_length)
            
            # é€’å½’å¤„ç†å­ä»»åŠ¡
            for child_id in task.children:
                dfs(child_id, current_length, visited.copy())
        
        # ä»æ¯ä¸ªèµ·å§‹ä»»åŠ¡å¼€å§‹DFS
        for start_task in start_tasks:
            dfs(start_task.id, 0.0, set())
        
        return max_path_length or 100.0  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è·¯å¾„ï¼Œè¿”å›é»˜è®¤å€¼
    
    def _get_heuristic_action(self, current_task: TaskState, node_states: List[NodeState], environment: EnvironmentState) -> int:
        """è·å–å¯å‘å¼ç®—æ³•å»ºè®®çš„åŠ¨ä½œï¼Œç”¨äºæŒ‡å¯¼DRLå­¦ä¹ """
        if not self.use_heuristic_guidance:
            return None
        
        # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„å¯å‘å¼åˆ†æ•°
        node_scores = []
        
        for i, node in enumerate(node_states):
            score = 0.0
            
            # 1. è€ƒè™‘èŠ‚ç‚¹é€Ÿåº¦ï¼ˆé€Ÿåº¦è¶Šå¿«åˆ†æ•°è¶Šé«˜ï¼‰
            max_speed = max(n.speed for n in node_states)
            speed_score = node.speed / max_speed
            score += speed_score * 0.3
            
            # 2. è€ƒè™‘èŠ‚ç‚¹å½“å‰è´Ÿè½½ï¼ˆè´Ÿè½½è¶Šä½åˆ†æ•°è¶Šé«˜ï¼‰
            load_score = 1.0 - node.current_load
            score += load_score * 0.25
            
            # 3. è€ƒè™‘èŠ‚ç‚¹å¯ç”¨æ—¶é—´ï¼ˆå¯ç”¨æ—¶é—´è¶ŠçŸ­åˆ†æ•°è¶Šé«˜ï¼‰
            min_available_time = min(n.available_time for n in node_states)
            if min_available_time > 0:
                availability_score = min_available_time / node.available_time
            else:
                availability_score = 1.0 if node.available_time == 0 else 0.0
            score += availability_score * 0.2
            
            # 4. è€ƒè™‘æ•°æ®å±€éƒ¨æ€§ï¼ˆæ•°æ®å¯ç”¨æ€§è¶Šé«˜åˆ†æ•°è¶Šé«˜ï¼‰
            data_availability = node.data_availability.get(current_task.id, 0.0)
            score += data_availability * 0.15
            
            # 5. è€ƒè™‘ä»»åŠ¡æ˜¯å¦åœ¨å…³é”®è·¯å¾„ä¸Šï¼ˆå…³é”®è·¯å¾„ä»»åŠ¡ä¼˜å…ˆåˆ†é…åˆ°é«˜æ€§èƒ½èŠ‚ç‚¹ï¼‰
            if current_task.is_critical_path:
                performance_score = node.speed / max_speed
                score += performance_score * 0.1
            
            node_scores.append((i, score))
        
        # æŒ‰åˆ†æ•°æ’åºï¼Œé€‰æ‹©æœ€ä½³èŠ‚ç‚¹
        node_scores.sort(key=lambda x: x[1], reverse=True)
        return node_scores[0][0]
    
    def _adjust_learning_rate(self, episode: int) -> float:
        """æ ¹æ®è®­ç»ƒè¿›åº¦è°ƒæ•´å­¦ä¹ ç‡"""
        for schedule in self.learning_rate_schedule:
            if episode <= schedule["episodes"]:
                return schedule["lr"]
        return self.learning_rate_schedule[-1]["lr"]
    
    def _should_advance_curriculum(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿›å…¥ä¸‹ä¸€ä¸ªè¯¾ç¨‹é˜¶æ®µ"""
        if self.current_stage >= len(self.curriculum_stages) - 1:
            return False  # å·²ç»æ˜¯æœ€åä¸€ä¸ªé˜¶æ®µ
        
        # æ£€æŸ¥å½“å‰é˜¶æ®µæ˜¯å¦å®Œæˆè¶³å¤Ÿçš„è®­ç»ƒè½®æ•°
        curriculum = self.curriculum_stages[self.current_stage]
        if self.stage_episodes_completed >= curriculum["episodes"]:
            print(f"ğŸ“ å®Œæˆé˜¶æ®µ {self.current_stage+1} çš„ {self.stage_episodes_completed} episodes, å‡†å¤‡è¿›å…¥ä¸‹ä¸€é˜¶æ®µ")
            return True
        
        return False
    
    def _advance_curriculum_stage(self):
        """è¿›å…¥ä¸‹ä¸€ä¸ªè¯¾ç¨‹é˜¶æ®µ"""
        self.current_stage += 1
        self.stage_episodes_completed = 0
        self.stage_performance_history = []
        
        if self.current_stage < len(self.curriculum_stages):
            curriculum = self.curriculum_stages[self.current_stage]
            print(f"ğŸ“ è¿›å…¥è¯¾ç¨‹é˜¶æ®µ {self.current_stage+1}: {curriculum['name']}")
            
            # æ›´æ–°æ™ºèƒ½ä½“çš„åŠ¨ä½œç»´åº¦
            new_action_dim = curriculum["nodes"]
            if hasattr(self.agent, 'update_action_dim'):
                self.agent.update_action_dim(new_action_dim)
            
            # ä¿å­˜å½“å‰æ¨¡å‹ä½œä¸ºé˜¶æ®µæ£€æŸ¥ç‚¹
            stage_ckpt_path = Path(self.checkpoint_cfg.get('dir', 'models/checkpoints/')) / f"stage_{self.current_stage}.pth"
            self.save_model(str(stage_ckpt_path))
            
            # é‡ç½®æ¢ç´¢ç‡ï¼Œé€‚åº”æ–°ç¯å¢ƒ
            if hasattr(self.agent, 'epsilon'):
                self.agent.epsilon = max(0.1, self.agent.epsilon * 1.5)  # å¢åŠ æ¢ç´¢ç‡ä»¥é€‚åº”æ–°ç¯å¢ƒ
        else:
            print("ğŸ“ æ‰€æœ‰è¯¾ç¨‹é˜¶æ®µå·²å®Œæˆï¼")
    
    def _update_heuristic_weight(self):
        """æ›´æ–°å¯å‘å¼æŒ‡å¯¼æƒé‡ï¼Œéšç€è®­ç»ƒè¿›è¡Œé€æ¸å‡å°‘"""
        if self.use_heuristic_guidance and self.heuristic_weight > 0.01:
            self.heuristic_weight *= self.heuristic_decay
    
    def _find_longest_path(self, task_states: List[TaskState], node_states: List[NodeState]) -> float:
        """æ‰¾å‡ºä»»åŠ¡å›¾ä¸­çš„æœ€é•¿è·¯å¾„ï¼ˆå…³é”®è·¯å¾„ï¼‰"""
        # æ„å»ºä»»åŠ¡å›¾
        graph = {task.id: [] for task in task_states}
        for task in task_states:
            for child_id in task.children:
                graph[task.id].append(child_id)
        
        # æ‰¾å‡ºæ‰€æœ‰æ²¡æœ‰çˆ¶èŠ‚ç‚¹çš„ä»»åŠ¡ï¼ˆèµ·å§‹ä»»åŠ¡ï¼‰
        start_tasks = [task.id for task in task_states if not task.parents]
        
        # ä½¿ç”¨DFSæ‰¾å‡ºæœ€é•¿è·¯å¾„
        def dfs(task_id, visited):
            if task_id in visited:
                return 0.0
            
            visited.add(task_id)
            task = next(t for t in task_states if t.id == task_id)
            
            # è®¡ç®—å½“å‰ä»»åŠ¡çš„æ‰§è¡Œæ—¶é—´
            fastest_node = max(node_states, key=lambda n: n.speed)
            execution_time = task.computation_size / fastest_node.speed
            
            # é€’å½’è®¡ç®—å­ä»»åŠ¡çš„æœ€é•¿è·¯å¾„
            max_child_path = 0.0
            for child_id in graph[task_id]:
                child_path = dfs(child_id, visited.copy())
                max_child_path = max(max_child_path, child_path)
            
            return execution_time + max_child_path
        
        # è®¡ç®—æ‰€æœ‰èµ·å§‹ä»»åŠ¡çš„æœ€é•¿è·¯å¾„
        max_path = 0.0
        for start_task in start_tasks:
            path_length = dfs(start_task, set())
            max_path = max(max_path, path_length)
        
        return max_path

    def extract_state_features(self, 
                             current_task: TaskState, 
                             node_states: List[NodeState],
                             environment: EnvironmentState) -> np.ndarray:
        """æå–çŠ¶æ€ç‰¹å¾ï¼Œå¢å¼ºç‰ˆ"""
        features = []
        
        # ä»»åŠ¡ç‰¹å¾ (5ç»´)
        # 1. è®¡ç®—å¤§å°å½’ä¸€åŒ–ï¼ˆä½¿ç”¨å¯¹æ•°å½’ä¸€åŒ–ï¼Œæ›´å¥½åœ°å¤„ç†ä¸åŒå¤§å°çš„ä»»åŠ¡ï¼‰
        features.append(np.log1p(current_task.computation_size / 1e9) / 10.0)  # ä½¿ç”¨log1pé¿å…log(0)
        # 2. çˆ¶ä»»åŠ¡æ•°é‡å½’ä¸€åŒ–
        features.append(len(current_task.parents) / 5.0)  # å‡è®¾æœ€å¤š5ä¸ªçˆ¶ä»»åŠ¡
        # 3. å­ä»»åŠ¡æ•°é‡å½’ä¸€åŒ–
        features.append(len(current_task.children) / 5.0)  # å‡è®¾æœ€å¤š5ä¸ªå­ä»»åŠ¡
        # 4. æ˜¯å¦åœ¨å…³é”®è·¯å¾„ä¸Š
        features.append(float(current_task.is_critical_path))
        # 5. æ•°æ®å±€éƒ¨æ€§åˆ†æ•°
        features.append(current_task.data_locality_score)
        
        # èŠ‚ç‚¹ç‰¹å¾ (æ¯ä¸ªèŠ‚ç‚¹4ç»´ï¼Œå…±nodesÃ—4ç»´)
        max_speed = max(node.speed for node in node_states)
        max_available_time = max(node.available_time for node in node_states) or 1.0
        
        for node in node_states:
            # 1. èŠ‚ç‚¹é€Ÿåº¦å½’ä¸€åŒ–
            features.append(node.speed / max_speed)
            # 2. èŠ‚ç‚¹å½“å‰è´Ÿè½½å½’ä¸€åŒ–
            features.append(node.current_load)
            # 3. èŠ‚ç‚¹å¯ç”¨æ—¶é—´å½’ä¸€åŒ–
            features.append(node.available_time / max_available_time)
            # 4. æ•°æ®å¯ç”¨æ€§
            features.append(node.data_availability.get(current_task.id, 0.0))
        
        # å¦‚æœèŠ‚ç‚¹æ•°å°‘äº4ä¸ªï¼Œç”¨0å¡«å……å‰©ä½™ç‰¹å¾
        num_nodes = len(node_states)
        if num_nodes < 4:
            for _ in range(4 - num_nodes):
                # ä¸ºæ¯ä¸ªç¼ºå¤±çš„èŠ‚ç‚¹æ·»åŠ 4ä¸ª0å€¼ç‰¹å¾
                features.extend([0.0, 0.0, 0.0, 0.0])
        
        # ç¯å¢ƒç‰¹å¾ (6ç»´)
        # 1. å·¥ä½œæµè¿›åº¦
        features.append(environment.workflow_progress)
        # 2. å½“å‰æ—¶é—´å½’ä¸€åŒ–ï¼ˆç›¸å¯¹äºå…³é”®è·¯å¾„é•¿åº¦ï¼‰
        features.append(environment.current_time / (environment.critical_path_length or 1.0))
        # 3. å¾…å¤„ç†ä»»åŠ¡æ•°é‡å½’ä¸€åŒ–
        features.append(len(environment.pending_tasks) / 20.0)
        # 4. å¹³å‡èŠ‚ç‚¹è´Ÿè½½
        avg_load = np.mean([node.current_load for node in node_states])
        features.append(avg_load)
        # 5. èŠ‚ç‚¹è´Ÿè½½æ ‡å‡†å·®ï¼ˆåæ˜ è´Ÿè½½å‡è¡¡æƒ…å†µï¼‰
        load_std = np.std([node.current_load for node in node_states])
        features.append(load_std)
        # 6. å…³é”®è·¯å¾„è¿›åº¦
        critical_tasks = [t for t in environment.pending_tasks if t.is_critical_path]
        total_critical_tasks = len([t for t in environment.pending_tasks + [current_task] if t.is_critical_path])
        critical_progress = 1.0 - (len(critical_tasks) / (total_critical_tasks or 1.0))
        features.append(critical_progress)
        
        # æ•°æ®ä¼ è¾“ç‰¹å¾ (5ç»´)
        # 1. å½“å‰ä»»åŠ¡æ•°æ®å¤§å°å½’ä¸€åŒ–
        task_data_size = getattr(current_task, 'data_size', 0.0)
        features.append(np.log1p(task_data_size / 1e6) / 10.0)  # ä½¿ç”¨logå½’ä¸€åŒ–
        # 2. å¹³å‡æ•°æ®ä¼ è¾“æ—¶é—´ä¼°ç®—
        if task_data_size > 0:
            # ä¼°ç®—ä»å­˜å‚¨èŠ‚ç‚¹åˆ°è®¡ç®—èŠ‚ç‚¹çš„å¹³å‡ä¼ è¾“æ—¶é—´
            avg_transfer_time = task_data_size / (0.5 * 1e9)  # è½¬æ¢ä¸ºå­—èŠ‚/ç§’ï¼Œå‡è®¾0.5GBps
            features.append(min(avg_transfer_time / 10.0, 1.0))  # å½’ä¸€åŒ–å¹¶é™åˆ¶æœ€å¤§å€¼
        else:
            features.append(0.0)
        # 3. æ•°æ®å±€éƒ¨æ€§å·®å¼‚ï¼ˆåæ˜ æ•°æ®åœ¨ä¸åŒèŠ‚ç‚¹çš„åˆ†å¸ƒæƒ…å†µï¼‰
        data_availability_values = [node.data_availability.get(current_task.id, 0.0) for node in node_states]
        data_locality_variance = np.var(data_availability_values) if len(data_availability_values) > 1 else 0.0
        features.append(data_locality_variance)
        # 4. æœ€ä½³æ•°æ®å¯ç”¨æ€§ï¼ˆåæ˜ å“ªä¸ªèŠ‚ç‚¹æœ‰æœ€å¥½çš„æ•°æ®å±€éƒ¨æ€§ï¼‰
        best_data_availability = max(data_availability_values) if data_availability_values else 0.0
        features.append(best_data_availability)
        # 5. æ•°æ®ä¼ è¾“ç“¶é¢ˆæŒ‡æ ‡ï¼ˆåæ˜ æ•°æ®ä¼ è¾“æ˜¯å¦å¯èƒ½æˆä¸ºç“¶é¢ˆï¼‰
        if task_data_size > 0:
            computation_time = current_task.computation_size / max_speed
            transfer_time = task_data_size / (0.5 * 1e9)  # å‡è®¾0.5GBps
            bottleneck_ratio = transfer_time / (computation_time + transfer_time)
            features.append(bottleneck_ratio)
        else:
            features.append(0.0)
        
        # æ€»ç‰¹å¾ç»´åº¦ï¼š5(ä»»åŠ¡) + 16(èŠ‚ç‚¹) + 6(ç¯å¢ƒ) + 5(æ•°æ®ä¼ è¾“) = 32ç»´
        return np.array(features, dtype=np.float32)
    
    def simulate_step(self, 
                     task: TaskState, 
                     action: int, 
                     node_states: List[NodeState],
                     environment: EnvironmentState) -> Tuple[float, EnvironmentState, bool]:
        """æ¨¡æ‹Ÿä¸€æ­¥æ‰§è¡Œï¼Œè€ƒè™‘æ•°æ®ä¼ è¾“å¼€é”€"""
        chosen_node = node_states[action]
        
        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        execution_time = task.computation_size / chosen_node.speed
        
        # è®¡ç®—æ•°æ®ä¼ è¾“æ—¶é—´ï¼ˆå¦‚æœä»»åŠ¡æœ‰æ•°æ®ï¼‰
        transfer_time = 0.0
        task_data_size = getattr(task, 'data_size', 0.0)
        if task_data_size > 0:
            # è·å–æ•°æ®å¯ç”¨æ€§
            data_availability = chosen_node.data_availability.get(task.id, 0.0)
            
            # å¦‚æœæ•°æ®ä¸å®Œå…¨åœ¨æœ¬åœ°ï¼Œéœ€è¦ä¼ è¾“
            if data_availability < 1.0:
                # è®¡ç®—éœ€è¦ä¼ è¾“çš„æ•°æ®é‡
                data_to_transfer = task_data_size * (1.0 - data_availability)
                
                # è€ƒè™‘ç½‘ç»œå¸¦å®½
                effective_bandwidth = 1e9  # è½¬æ¢ä¸ºå­—èŠ‚/ç§’ï¼Œå‡è®¾1GBps
                
                # è®¡ç®—ä¼ è¾“æ—¶é—´
                transfer_time = data_to_transfer / effective_bandwidth
                
                # æ·»åŠ ç½‘ç»œå»¶è¿Ÿï¼ˆç®€åŒ–æ¨¡å‹ï¼šæ¯ä¸ªä¼ è¾“æ“ä½œéƒ½æœ‰ä¸€å›ºå®šå»¶è¿Ÿï¼‰
                transfer_time += 0.001  # å‡è®¾1mså»¶è¿Ÿ
        
        # æ€»æ—¶é—´ = æ‰§è¡Œæ—¶é—´ + ä¼ è¾“æ—¶é—´
        total_time = execution_time + transfer_time
        
        # æ›´æ–°ç¯å¢ƒ
        new_environment = EnvironmentState(
            current_time=environment.current_time + total_time,
            pending_tasks=[t for t in environment.pending_tasks if t.id != task.id],
            node_states=node_states.copy(),  # åˆ›å»ºå‰¯æœ¬ä»¥é¿å…ä¿®æ”¹åŸå§‹çŠ¶æ€
            workflow_progress=environment.workflow_progress + 1.0/20.0,
            critical_path_length=environment.critical_path_length
        )
        
        # æ›´æ–°èŠ‚ç‚¹çŠ¶æ€
        # æ‰¾åˆ°æ–°ç¯å¢ƒä¸­çš„å¯¹åº”èŠ‚ç‚¹å¹¶æ›´æ–°
        for node in new_environment.node_states:
            if node.id == chosen_node.id:
                # æ›´æ–°è´Ÿè½½ï¼ˆè€ƒè™‘æ‰§è¡Œæ—¶é—´å’Œä¼ è¾“æ—¶é—´ï¼‰
                node.current_load += 0.1 * (1.0 + transfer_time / (execution_time + 1e-6))
                # æ›´æ–°å¯ç”¨æ—¶é—´
                node.available_time += total_time
                # æ›´æ–°æ•°æ®å¯ç”¨æ€§ï¼ˆä»»åŠ¡æ•°æ®ç°åœ¨åœ¨èŠ‚ç‚¹ä¸Šï¼‰
                node.data_availability[task.id] = 1.0
                break
        
        # æ£€æŸ¥æ˜¯å¦ç»“æŸ
        done = len(new_environment.pending_tasks) == 0
        return 0.0, new_environment, done
    
    def train_episode(self) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepisodeï¼Œé›†æˆè¯¾ç¨‹å­¦ä¹ å’Œå¯å‘å¼æŒ‡å¯¼"""
        # æ ¹æ®å½“å‰è¯¾ç¨‹é˜¶æ®µåˆ›å»ºç¯å¢ƒ
        curriculum = self.curriculum_stages[self.current_stage]
        environment, task_states, node_states = self.create_mock_environment(
            stage=self.current_stage
        )
        
        step_rewards = []  # è®°å½•æ¯æ­¥çš„å¥–åŠ±
        total_makespan = 0.0
        step_count = 0
        
        current_tasks = task_states.copy()

        reward_debug_path = self.logging_cfg.get('reward_debug', 'results/reward_debug.log')
        debug_file = None
        try:
            debug_file = open(reward_debug_path, 'a')
        except Exception:
            debug_file = None

        # å‡†å¤‡å¢å¼ºå‹ StepContext ç»Ÿè®¡
        total_cp_tasks = sum(1 for t in task_states if t.is_critical_path) or 1
        baseline_avg_wait = np.mean([n.available_time for n in node_states]) or 1.0
        completed_ids = set()
        task_map = {t.id: t for t in task_states}

        while current_tasks and step_count < 50:  # é™åˆ¶æœ€å¤§æ­¥æ•°
            # é€‰æ‹©å½“å‰ä»»åŠ¡ï¼ˆç®€åŒ–ï¼šæŒ‰é¡ºåºï¼‰
            current_task = current_tasks[0]
            
            # æå–çŠ¶æ€ç‰¹å¾
            state = self.extract_state_features(current_task, node_states, environment)
            
            # è·å–å¯å‘å¼åŠ¨ä½œå»ºè®®
            heuristic_action = self._get_heuristic_action(current_task, node_states, environment)
            
            # æ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œ
            action = self.agent.act(state, training=True)
            
            # å¦‚æœä½¿ç”¨å¯å‘å¼æŒ‡å¯¼ï¼Œæ ¹æ®æ¦‚ç‡å†³å®šæ˜¯å¦é‡‡ç”¨å¯å‘å¼åŠ¨ä½œ
            if self.use_heuristic_guidance and heuristic_action is not None and np.random.random() < self.heuristic_weight:
                # ä½¿ç”¨å¯å‘å¼åŠ¨ä½œï¼Œä½†ç»™äºˆéƒ¨åˆ†å¥–åŠ±ä»¥é¼“åŠ±æ¢ç´¢
                original_action = action
                action = heuristic_action
                
                # ç»™äºˆé¢å¤–å¥–åŠ±ä»¥é¼“åŠ±æ™ºèƒ½ä½“å­¦ä¹ å¯å‘å¼ç­–ç•¥
                heuristic_bonus = 0.1 * self.heuristic_weight
            else:
                heuristic_bonus = 0.0
            
            # æ‰§è¡ŒåŠ¨ä½œ
            _, new_environment, done = self.simulate_step(
                current_task, action, node_states, environment
            )
            # æ›´æ–°å®Œæˆé›†
            completed_ids.add(current_task.id)

            # æ„é€ å¢å¼º StepContext
            try:
                completed_cp = sum(1 for cid in completed_ids if task_map[cid].is_critical_path)
                ctx = StepContext(
                    completed_critical_path_tasks=completed_cp,
                    total_critical_path_tasks=total_cp_tasks,
                    node_busy_times={n.id: n.current_load for n in node_states},
                    ready_task_count=len(current_tasks)-1,  # å»æ‰å½“å‰å³å°†è°ƒåº¦çš„
                    total_nodes=len(node_states),
                    avg_queue_wait=np.mean([n.available_time for n in node_states]),
                    queue_wait_baseline=baseline_avg_wait
                )
                # é¢„æµ‹å½“å‰makespanå’ŒåŸºå‡†makespan
                predicted_makespan = total_makespan * (1.0 + (len(current_tasks) / len(task_states)))
                # ä½¿ç”¨å†å²æœ€ä½³makespanä½œä¸ºåŸºå‡†ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨å›ºå®šå€¼
                baseline_makespan = self.best_makespan if self.best_makespan != float('inf') else 100.0
                step_reward, _metrics = compute_step_reward(ctx, predicted_makespan, baseline_makespan, debug_writer=debug_file)
                
                # æ·»åŠ å¯å‘å¼å¥–åŠ±
                step_reward += heuristic_bonus
            except Exception:
                step_reward = 0.0
            
            # è®°å½•æ­¥éª¤å¥–åŠ±
            step_rewards.append(step_reward)
            
            # æå–ä¸‹ä¸€çŠ¶æ€ç‰¹å¾
            if len(current_tasks) > 1:
                next_task = current_tasks[1]
                next_state = self.extract_state_features(next_task, node_states, new_environment)
            else:
                next_state = np.zeros_like(state)
            
            # å­˜å‚¨ç»éªŒ
            self.agent.remember(state, action, step_reward, next_state, done)
            
            # æ›´æ–°çŠ¶æ€
            environment = new_environment
            current_tasks = current_tasks[1:]
            total_makespan = environment.current_time
            step_count += 1
            
            # è®­ç»ƒæ™ºèƒ½ä½“
            if step_count % 4 == 0:  # æ¯4æ­¥è®­ç»ƒä¸€æ¬¡
                loss = self.agent.replay()
            
            if done:
                break

        # è®¡ç®—æœ€ç»ˆå¥–åŠ± (æ–° makespan ç¨€ç–å¥–åŠ±)
        episode_stats = EpisodeStats(makespan=total_makespan)
        final_reward, final_metrics = compute_final_reward(
            makespan=total_makespan,
            stats=episode_stats,
            temperature=0.75,
            baseline_makespan=self.baseline_makespan if hasattr(self, 'baseline_makespan') else None
        )
        
        # è®¡ç®—å¹³å‡å¥–åŠ±è€Œä¸æ˜¯ç´¯åŠ å¥–åŠ±
        avg_step_reward = np.mean(step_rewards) if step_rewards else 0.0
        total_reward = avg_step_reward + final_reward  # å¹³å‡æ­¥éª¤å¥–åŠ± + æœ€ç»ˆå¥–åŠ±
        
        # æ›´æ–°æœ€ä½³makespanè®°å½•
        if total_makespan < self.best_makespan:
            self.best_makespan = total_makespan
        
        # è®°å½•é˜¶æ®µæ€§èƒ½å†å²
        self.stage_performance_history.append({
            'makespan': total_makespan,
            'reward': total_reward,
            'step_count': step_count
        })
        
        # æ›´æ–°è¯¾ç¨‹é˜¶æ®µè®¡æ•°
        self.stage_episodes_completed += 1
        
        # æ›´æ–°å¯å‘å¼æƒé‡
        self._update_heuristic_weight()
        
        if debug_file:
            try:
                debug_file.write(f"FINAL\tstage={self.current_stage+1}\tmakespan={total_makespan:.4f}\tavg_step_reward={avg_step_reward:.4f}\tfinal_reward={final_reward:.4f}\ttotal_reward={total_reward:.4f}\n")
                debug_file.close()
            except Exception:
                pass

        return {
            'total_reward': total_reward,
            'avg_step_reward': avg_step_reward,
            'final_reward': final_reward,
            'makespan': total_makespan,
            'step_count': step_count,
            'epsilon': self.agent.epsilon,
            'stage': self.current_stage,
            'heuristic_weight': self.heuristic_weight
        }
    
    def train(self, episodes: int = 1000) -> Dict[str, Any]:
        """è®­ç»ƒDRLæ™ºèƒ½ä½“ï¼Œä½¿ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥å’Œå¯å‘å¼æŒ‡å¯¼"""
        print(f"ğŸš€ å¼€å§‹é«˜çº§DRLè®­ç»ƒ: {episodes} episodes (é…ç½® episodes={self.drl_cfg.get('episodes', episodes)})")
        print(f"ğŸ“Š è®­ç»ƒç‰¹æ€§: é«˜çº§ç½‘ç»œæ¶æ„ï¼Œè¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œå¯å‘å¼æŒ‡å¯¼")
        print(f"ğŸ“ è¯¾ç¨‹é˜¶æ®µ: {len(self.curriculum_stages)}ä¸ªé˜¶æ®µï¼Œå½“å‰é˜¶æ®µ: {self.current_stage+1}")
        
        # åˆå§‹åŒ–æ™ºèƒ½ä½“
        state_dim = 32  # æ›´æ–°åçš„ç‰¹å¾ç»´åº¦ï¼š5(ä»»åŠ¡) + 16(èŠ‚ç‚¹) + 6(ç¯å¢ƒ) + 5(æ•°æ®ä¼ è¾“)
        curriculum = self.curriculum_stages[self.current_stage]
        action_dim = curriculum["nodes"]  # æ ¹æ®å½“å‰è¯¾ç¨‹é˜¶æ®µçš„èŠ‚ç‚¹æ•°è®¾ç½®åŠ¨ä½œç»´åº¦
        
        # è·å–DRLé…ç½®å‚æ•°
        drl_config = self.config.get('drl', {}).copy()
        
        # åªæå–ImprovedDQNAgentæ¥å—çš„å‚æ•°
        agent_params = {
            'state_dim': state_dim,
            'action_dim': action_dim,
            'learning_rate': drl_config.get('learning_rate', 0.001),
            'epsilon_start': drl_config.get('epsilon_start', 1.0),
            'epsilon_end': drl_config.get('epsilon_end', 0.1),
            'epsilon_decay': drl_config.get('epsilon_decay', 0.995),
            'gamma': drl_config.get('gamma', 0.99),
            'memory_size': drl_config.get('memory_size', 10000),
            'batch_size': drl_config.get('batch_size', 64),
            'target_update_freq': drl_config.get('target_update_freq', 100),
            'network_type': "advanced",  # ä½¿ç”¨é«˜çº§ç½‘ç»œ
            'hidden_dims': [512, 256, 128, 64],  # æ›´æ·±çš„ç½‘ç»œç»“æ„
            'exploration_strategy': "adaptive",  # ä½¿ç”¨è‡ªé€‚åº”æ¢ç´¢ç­–ç•¥
            'use_ucb': True,  # å¯ç”¨UCBæ¢ç´¢
            'ucb_c': 2.0,  # UCBç½®ä¿¡åº¦å‚æ•°
            'use_boltzmann': True,  # å¯ç”¨ç»å°”å…¹æ›¼æ¢ç´¢
            'boltzmann_tau': 1.0,  # åˆå§‹æ¸©åº¦å‚æ•°
        }
        
        # ä½¿ç”¨é«˜çº§ç½‘ç»œæ¶æ„å’Œå¤šæ ·åŒ–æ¢ç´¢ç­–ç•¥
        self.agent = ImprovedDQNAgent(**agent_params)
        
        # è®­ç»ƒå¾ªç¯
        best_makespan = float('inf')
        recent_rewards = deque(maxlen=100)
        recent_losses = deque(maxlen=100)
        
        log_interval = self.drl_cfg.get('log_interval', 50)
        eval_interval = self.drl_cfg.get('eval_interval', 100)
        checkpoint_interval = self.drl_cfg.get('checkpoint_interval', 100)
        rolling_window = self.drl_cfg.get('rolling_window', 100)
        metrics_path = self.logging_cfg.get('metrics_file', 'results/training_metrics.jsonl')
        ckpt_dir = Path(self.checkpoint_cfg.get('dir', 'models/checkpoints/'))
        keep_last = self.checkpoint_cfg.get('keep_last', 5)
        save_best = self.checkpoint_cfg.get('save_best', True)
        kept_ckpts = []

        for episode in range(episodes):
            # åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡
            current_lr = self._adjust_learning_rate(episode)
            for param_group in self.agent.optimizer.param_groups:
                param_group['lr'] = current_lr
            
            episode_results = self.train_episode()
            self.training_history.append(episode_results)
            recent_rewards.append(episode_results['total_reward'])
            
            # è·å–æ™ºèƒ½ä½“æ€§èƒ½ç»Ÿè®¡
            perf_stats = self.agent.get_performance_stats()
            if perf_stats.get('avg_loss') is not None:
                recent_losses.append(perf_stats['avg_loss'])
            
            # æ›´æ–°æœ€ä½³æ€§èƒ½
            if episode_results['makespan'] < best_makespan:
                best_makespan = episode_results['makespan']
            
            # è·å–æ™ºèƒ½ä½“æ¢ç´¢ç»Ÿè®¡
            exploration_stats = self.agent.get_exploration_stats()
            
            # æ‰“å°è¿›åº¦
            if episode % log_interval == 0:
                avg_reward = np.mean(recent_rewards) if recent_rewards else 0
                avg_loss = np.mean(recent_losses) if recent_losses else 0
                curriculum = self.curriculum_stages[self.current_stage]
                print(f"Episode {episode}: "
                      f"é˜¶æ®µ={self.current_stage+1}/{len(self.curriculum_stages)} ({curriculum['name']}), "
                      f"å¹³å‡å¥–åŠ±={avg_reward:.3f}, "
                      f"å¹³å‡æŸå¤±={avg_loss:.4f}, "
                      f"Makespan={episode_results['makespan']:.2f}, "
                      f"æœ€ä½³Makespan={best_makespan:.2f}, "
                      f"æ¢ç´¢ç‡={exploration_stats.get('exploration_rate', 0.0):.3f}, "
                      f"åˆ©ç”¨ç‡={exploration_stats.get('exploitation_rate', 0.0):.3f}, "
                      f"Îµ={exploration_stats.get('epsilon', 0.0):.3f}, "
                      f"æ¸©åº¦={exploration_stats.get('boltzmann_tau', 0.0):.3f}, "
                      f"Qå€¼={perf_stats.get('avg_q_value', 0.0):.3f}, "
                      f"å¯å‘å¼æƒé‡={episode_results['heuristic_weight']:.3f}, "
                      f"å­¦ä¹ ç‡={current_lr:.6f}")
            
            # å†™å…¥æµå¼æŒ‡æ ‡æ—¥å¿—
            try:
                with open(metrics_path, 'a') as mf:
                    mf.write(json.dumps({
                        'episode': episode,
                        'stage': self.current_stage,
                        'stage_name': curriculum['name'],
                        'reward': episode_results['total_reward'],
                        'makespan': episode_results['makespan'],
                        'best_makespan': best_makespan,
                        'exploration_rate': exploration_stats.get('exploration_rate', 0.0),
                        'exploitation_rate': exploration_stats.get('exploitation_rate', 0.0),
                        'epsilon': exploration_stats.get('epsilon', 0.0),
                        'boltzmann_tau': exploration_stats.get('boltzmann_tau', 0.0),
                        'heuristic_weight': episode_results['heuristic_weight'],
                        'learning_rate': current_lr,
                        'avg_loss': perf_stats.get('avg_loss', 0.0),
                        'avg_q_value': perf_stats.get('avg_q_value', 0.0),
                        'network_type': perf_stats.get('network_type', 'unknown'),
                        'exploration_strategy': exploration_stats.get('strategy', 'unknown'),
                        'timestamp': time.time()
                    }) + '\n')
            except Exception as e:
                print(f"âš ï¸ å†™å…¥æŒ‡æ ‡æ—¥å¿—å¤±è´¥: {e}")
            
            # æ›´æ–°æ¢ç´¢å‚æ•°
            self.agent.update_exploration_parameters(episode_results['total_reward'])
            
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥è¿›å…¥ä¸‹ä¸€ä¸ªè¯¾ç¨‹é˜¶æ®µ
            if self._should_advance_curriculum():
                self._advance_curriculum_stage()
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if episode % checkpoint_interval == 0 and episode > 0:
                ckpt_path = ckpt_dir / f"checkpoint_ep{episode}.pth"
                self.save_model(str(ckpt_path))
                kept_ckpts.append(ckpt_path)
                
                # æ¸…ç†æ—§æ£€æŸ¥ç‚¹
                if len(kept_ckpts) > keep_last:
                    try:
                        oldest = kept_ckpts.pop(0)
                        oldest.unlink(missing_ok=True)
                    except Exception:
                        pass
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if save_best and episode_results['makespan'] == best_makespan:
                    best_path = ckpt_dir / "best_model.pth"
                    self.save_model(str(best_path))
                    print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (makespan={best_makespan:.2f})")
        
        # è®­ç»ƒå®Œæˆæ‘˜è¦
        curriculum = self.curriculum_stages[self.current_stage]
        exploration_stats = self.agent.get_exploration_stats()
        
        summary = {
            'episodes_trained': episodes,
            'best_makespan': best_makespan,
            'final_stage': self.current_stage,
            'final_stage_name': curriculum['name'],
            'final_reward': recent_rewards[-1] if recent_rewards else 0,
            'final_epsilon': exploration_stats.get('epsilon', 0.0),
            'final_exploration_rate': exploration_stats.get('exploration_rate', 0.0),
            'final_exploitation_rate': exploration_stats.get('exploitation_rate', 0.0),
            'final_boltzmann_tau': exploration_stats.get('boltzmann_tau', 0.0),
            'final_heuristic_weight': episode_results['heuristic_weight'],
            'network_type': 'advanced',
            'exploration_strategy': exploration_stats.get('strategy', 'adaptive'),
            'curriculum_learning': True,
            'heuristic_guidance': self.use_heuristic_guidance
        }
        
        print(f"âœ… é«˜çº§DRLè®­ç»ƒå®Œæˆï¼æœ€ä½³makespan: {best_makespan:.2f}")
        print(f"ğŸ“ å®Œæˆè¯¾ç¨‹é˜¶æ®µ: {self.current_stage+1}/{len(self.curriculum_stages)} ({curriculum['name']})")
        print(f"ğŸ” æœ€ç»ˆæ¢ç´¢ç­–ç•¥: {exploration_stats.get('strategy', 'adaptive')}")
        print(f"ğŸ“Š æœ€ç»ˆæ¢ç´¢ç‡: {exploration_stats.get('exploration_rate', 0.0):.3f}")
        print(f"ğŸ“Š æœ€ç»ˆåˆ©ç”¨ç‡: {exploration_stats.get('exploitation_rate', 0.0):.3f}")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_path = ckpt_dir / "final_model.pth"
        self.save_model(str(final_path))
        print(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ° {final_path}")
        
        return summary
    
    def save_model(self, model_path: str):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        if self.agent is None:
            raise ValueError("æ™ºèƒ½ä½“å°šæœªåˆå§‹åŒ–æˆ–è®­ç»ƒ")
        
        checkpoint = {
            'q_network_state_dict': self.agent.q_network.state_dict(),
            'target_network_state_dict': self.agent.target_network.state_dict(),
            'optimizer_state_dict': self.agent.optimizer.state_dict(),
            'training_step': self.agent.training_step,
            'epsilon': self.agent.epsilon,
            'config': self.config,
            'training_history': self.training_history,
            'exploration_params': {
                'exploration_strategy': self.agent.exploration_strategy,
                'use_ucb': self.agent.use_ucb,
                'ucb_c': self.agent.ucb_c,
                'use_boltzmann': self.agent.use_boltzmann,
                'boltzmann_tau': self.agent.boltzmann_tau,
                'action_counts': self.agent.action_counts.tolist() if hasattr(self.agent.action_counts, 'tolist') else self.agent.action_counts,
                'action_values': self.agent.action_values.tolist() if hasattr(self.agent.action_values, 'tolist') else self.agent.action_values,
            },
            'metadata': {
                'state_dim': self.agent.state_dim,
                'action_dim': self.agent.action_dim,
                'training_completed': True,
                'final_performance': {
                    'avg_makespan': np.mean([h['makespan'] for h in self.training_history[-100:]]),
                    'best_makespan': min(h['makespan'] for h in self.training_history)
                }
            }
        }
        
        torch.save(checkpoint, model_path)
        print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        # ä¿å­˜è®­ç»ƒå†å²
        history_path = model_path.replace('.pth', '_history.json')
        with open(history_path, 'w') as f:
            json.dump(self.training_history, f, indent=2)
        print(f"ğŸ“Š è®­ç»ƒå†å²å·²ä¿å­˜: {history_path}")

def set_seed(seed_value=42):
    """Set seed for reproducibility."""
    random.seed(seed_value)
    np.random.seed(seed_value)
    torch.manual_seed(seed_value)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed_value)

def main():
    # Set seed for reproducibility
    set_seed(42)

    import argparse

    # Set seed for reproducibility
    random.seed(42)
    np.random.seed(42)
    torch.manual_seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(42)

    
    parser = argparse.ArgumentParser(description='WASS-RAG æ”¹è¿›DRLè®­ç»ƒå™¨ (é›†æˆå¤šæ ·åŒ–æ¢ç´¢ç­–ç•¥)')
    parser.add_argument('--config', default='configs/experiment.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--episodes', type=int, default=1000, help='è®­ç»ƒepisodeæ•°')
    parser.add_argument('--output', default='models/improved_wass_drl.pth', help='è¾“å‡ºæ¨¡å‹è·¯å¾„')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(args.output).parent.mkdir(parents=True, exist_ok=True)
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = WRENCHDRLTrainer(args.config)
    
    # è®­ç»ƒ
    results = trainer.train(args.episodes)
    
    # ä¿å­˜æ¨¡å‹
    trainer.save_model(args.output)
    
    # ä¿å­˜è®­ç»ƒç»“æœæ‘˜è¦
    summary_path = args.output.replace('.pth', '_summary.json')
    with open(summary_path, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"ğŸ‰ è®­ç»ƒå®Œæˆ! æ¨¡å‹å’Œç»“æœå·²ä¿å­˜åˆ° {Path(args.output).parent}")
    print(f"ğŸ“ˆ è®­ç»ƒç‰¹æ€§: é›†æˆå¤šæ ·åŒ–æ¢ç´¢ç­–ç•¥å’Œmakespané¢„æµ‹å¥–åŠ±")

if __name__ == "__main__":
    main()
