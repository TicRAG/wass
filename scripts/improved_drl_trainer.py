#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
WASS-RAG æ”¹è¿›çš„DRLè®­ç»ƒå™¨
å®ç°å¯†é›†å¥–åŠ±å‡½æ•°ä»¥æé«˜è®­ç»ƒæ•ˆæœ
"""

import os
import sys
import json
import time
import random
from pathlib import Path
from typing import Dict, List, Tuple, Any
from dataclasses import dataclass
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque, namedtuple
import yaml

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

@dataclass
class TaskState:
    """ä»»åŠ¡çŠ¶æ€"""
    id: str
    computation_size: float
    parents: List[str]
    children: List[str]
    is_critical_path: bool
    data_locality_score: float

@dataclass
class NodeState:
    """èŠ‚ç‚¹çŠ¶æ€"""
    id: str
    speed: float
    current_load: float
    available_time: float
    data_availability: Dict[str, float]  # æ•°æ®å¯ç”¨æ€§è¯„åˆ†

@dataclass
class EnvironmentState:
    """ç¯å¢ƒçŠ¶æ€"""
    current_time: float
    pending_tasks: List[TaskState]
    node_states: List[NodeState]
    workflow_progress: float
    critical_path_length: float

class DenseRewardCalculator:
    """å¯†é›†å¥–åŠ±è®¡ç®—å™¨"""
    
    def __init__(self, 
                 data_locality_weight: float = 0.3,
                 waiting_time_weight: float = 0.2,
                 critical_path_weight: float = 0.4,
                 final_makespan_weight: float = 0.1):
        self.data_locality_weight = data_locality_weight
        self.waiting_time_weight = waiting_time_weight
        self.critical_path_weight = critical_path_weight
        self.final_makespan_weight = final_makespan_weight
    
    def calculate_step_reward(self, 
                            task: TaskState, 
                            chosen_node: NodeState,
                            all_nodes: List[NodeState],
                            environment: EnvironmentState) -> float:
        """è®¡ç®—å•æ­¥å¥–åŠ±"""
        
        # 1. æ•°æ®å±€éƒ¨æ€§å¥–åŠ±
        data_locality_reward = self._calculate_data_locality_reward(task, chosen_node)
        
        # 2. ç­‰å¾…æ—¶é—´æƒ©ç½š
        waiting_time_penalty = self._calculate_waiting_time_penalty(task, chosen_node, all_nodes)
        
        # 3. å…³é”®è·¯å¾„å¥–åŠ±
        critical_path_reward = self._calculate_critical_path_reward(task, chosen_node, environment)
        
        # 4. è´Ÿè½½å‡è¡¡å¥–åŠ±
        load_balance_reward = self._calculate_load_balance_reward(chosen_node, all_nodes)
        
        # ç»„åˆå¥–åŠ±
        total_reward = (
            self.data_locality_weight * data_locality_reward +
            self.waiting_time_weight * (-waiting_time_penalty) +
            self.critical_path_weight * critical_path_reward +
            0.1 * load_balance_reward  # è¾ƒå°æƒé‡çš„è´Ÿè½½å‡è¡¡
        )
        
        return total_reward
    
    def _calculate_data_locality_reward(self, task: TaskState, chosen_node: NodeState) -> float:
        """è®¡ç®—æ•°æ®å±€éƒ¨æ€§å¥–åŠ±"""
        # å¦‚æœä»»åŠ¡æ•°æ®å·²åœ¨é€‰å®šèŠ‚ç‚¹ä¸Šï¼Œç»™äºˆæ­£å¥–åŠ±
        data_score = task.data_locality_score * chosen_node.data_availability.get(task.id, 0.0)
        return min(data_score, 1.0)  # é™åˆ¶åœ¨[0,1]èŒƒå›´å†…
    
    def _calculate_waiting_time_penalty(self, task: TaskState, chosen_node: NodeState, all_nodes: List[NodeState]) -> float:
        """è®¡ç®—ç­‰å¾…æ—¶é—´æƒ©ç½š"""
        # è®¡ç®—åœ¨æ‰€é€‰èŠ‚ç‚¹ä¸Šçš„ç›¸å¯¹ç­‰å¾…æ—¶é—´
        min_available_time = min(node.available_time for node in all_nodes)
        relative_waiting_time = (chosen_node.available_time - min_available_time) / max(1.0, min_available_time)
        return min(relative_waiting_time, 2.0)  # é™åˆ¶æƒ©ç½šä¸Šé™
    
    def _calculate_critical_path_reward(self, task: TaskState, chosen_node: NodeState, environment: EnvironmentState) -> float:
        """è®¡ç®—å…³é”®è·¯å¾„å¥–åŠ±"""
        if not task.is_critical_path:
            return 0.0
        
        # å…³é”®è·¯å¾„ä»»åŠ¡é€‰æ‹©æœ€å¿«èŠ‚ç‚¹åº”è¯¥å¾—åˆ°æ›´é«˜å¥–åŠ±
        execution_time = task.computation_size / chosen_node.speed
        optimal_execution_time = task.computation_size / max(node.speed for node in environment.node_states)
        
        # å¥–åŠ±ä¸æ‰§è¡Œæ—¶é—´çš„æ”¹è¿›æˆæ­£æ¯”
        improvement_ratio = optimal_execution_time / execution_time
        return min(improvement_ratio, 2.0) - 1.0  # å½’ä¸€åŒ–åˆ°[-1,1]
    
    def _calculate_load_balance_reward(self, chosen_node: NodeState, all_nodes: List[NodeState]) -> float:
        """è®¡ç®—è´Ÿè½½å‡è¡¡å¥–åŠ±"""
        avg_load = np.mean([node.current_load for node in all_nodes])
        load_deviation = abs(chosen_node.current_load - avg_load) / max(avg_load, 1.0)
        return max(0.0, 1.0 - load_deviation)  # è´Ÿè½½è¶Šæ¥è¿‘å¹³å‡å€¼å¥–åŠ±è¶Šé«˜
    
    def calculate_final_reward(self, final_makespan: float, baseline_makespan: float) -> float:
        """è®¡ç®—æœ€ç»ˆå¥–åŠ±"""
        if baseline_makespan <= 0:
            return 0.0
        
        improvement = (baseline_makespan - final_makespan) / baseline_makespan
        return improvement * 10.0  # æ”¾å¤§æœ€ç»ˆå¥–åŠ±ä¿¡å·

class ImprovedDQN(nn.Module):
    """æ”¹è¿›çš„DQNç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dims: List[int] = None):
        super().__init__()
        
        if hidden_dims is None:
            hidden_dims = [256, 128, 64]
        
        layers = []
        current_dim = state_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(current_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.1)
            ])
            current_dim = hidden_dim
        
        layers.append(nn.Linear(current_dim, action_dim))
        
        self.network = nn.Sequential(*layers)
        
        # ä½¿ç”¨Xavieråˆå§‹åŒ–
        for layer in self.network:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.zeros_(layer.bias)
    
    def forward(self, x):
        return self.network(x)

class ImprovedDQNAgent:
    """æ”¹è¿›çš„DQNæ™ºèƒ½ä½“"""
    
    def __init__(self,
                 state_dim: int,
                 action_dim: int,
                 learning_rate: float = 1e-3,
                 epsilon_start: float = 1.0,
                 epsilon_end: float = 0.1,
                 epsilon_decay: float = 0.995,
                 gamma: float = 0.99,
                 memory_size: int = 10000,
                 batch_size: int = 64,
                 target_update_freq: int = 100,
                 device: str = None):
        
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.batch_size = batch_size
        self.target_update_freq = target_update_freq
        
        # ç½‘ç»œ
        self.q_network = ImprovedDQN(state_dim, action_dim).to(self.device)
        self.target_network = ImprovedDQN(state_dim, action_dim).to(self.device)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)
        
        # ç»éªŒå›æ”¾
        self.memory = deque(maxlen=memory_size)
        self.experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])
        
        # æ¢ç´¢ç­–ç•¥
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
        
        # è®­ç»ƒç»Ÿè®¡
        self.training_step = 0
        self.update_target()
    
    def update_target(self):
        """æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        self.target_network.load_state_dict(self.q_network.state_dict())
    
    def remember(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒ"""
        experience = self.experience(state, action, reward, next_state, done)
        self.memory.append(experience)
    
    def act(self, state, training=True):
        """é€‰æ‹©åŠ¨ä½œ"""
        if training and random.random() < self.epsilon:
            return random.randint(0, self.action_dim - 1)
        
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            q_values = self.q_network(state_tensor)
            return q_values.argmax().item()
    
    def replay(self):
        """ç»éªŒå›æ”¾è®­ç»ƒ"""
        if len(self.memory) < self.batch_size:
            return None
        
        # é‡‡æ ·ç»éªŒ
        batch = random.sample(self.memory, self.batch_size)
        states = torch.FloatTensor([e.state for e in batch]).to(self.device)
        actions = torch.LongTensor([e.action for e in batch]).to(self.device)
        rewards = torch.FloatTensor([e.reward for e in batch]).to(self.device)
        next_states = torch.FloatTensor([e.next_state for e in batch]).to(self.device)
        dones = torch.BoolTensor([e.done for e in batch]).to(self.device)
        
        # è®¡ç®—Qå€¼
        current_q = self.q_network(states).gather(1, actions.unsqueeze(1))
        next_q = self.target_network(next_states).max(1)[0].detach()
        target_q = rewards + (self.gamma * next_q * ~dones)
        
        # è®¡ç®—æŸå¤±
        loss = nn.MSELoss()(current_q.squeeze(), target_q)
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)
        
        self.optimizer.step()
        
        # æ›´æ–°æ¢ç´¢ç‡
        if self.epsilon > self.epsilon_end:
            self.epsilon *= self.epsilon_decay
        
        # å®šæœŸæ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.training_step += 1
        if self.training_step % self.target_update_freq == 0:
            self.update_target()
        
        return loss.item()

class WRENCHDRLTrainer:
    """WRENCH DRLè®­ç»ƒå™¨"""
    
    def __init__(self, config_path: str):
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.reward_calculator = DenseRewardCalculator()
        self.agent = None
        self.training_history = []
    
    def create_mock_environment(self) -> Tuple[EnvironmentState, List[TaskState], List[NodeState]]:
        """åˆ›å»ºæ¨¡æ‹Ÿè®­ç»ƒç¯å¢ƒ"""
        # åˆ›å»ºèŠ‚ç‚¹çŠ¶æ€
        node_states = []
        for i in range(4):
            node_states.append(NodeState(
                id=f"ComputeHost{i+1}",
                speed=2.0 + i * 0.5,  # ä¸åŒçš„å¤„ç†é€Ÿåº¦
                current_load=random.uniform(0, 0.5),
                available_time=random.uniform(0, 10),
                data_availability={f"task_{j}": random.random() for j in range(20)}
            ))
        
        # åˆ›å»ºä»»åŠ¡çŠ¶æ€
        task_states = []
        for i in range(20):
            task_states.append(TaskState(
                id=f"task_{i}",
                computation_size=random.uniform(1e9, 1e10),
                parents=[f"task_{j}" for j in range(max(0, i-2), i)],
                children=[f"task_{j}" for j in range(i+1, min(20, i+3))],
                is_critical_path=random.random() > 0.7,
                data_locality_score=random.random()
            ))
        
        # åˆ›å»ºç¯å¢ƒçŠ¶æ€
        environment = EnvironmentState(
            current_time=0.0,
            pending_tasks=task_states,
            node_states=node_states,
            workflow_progress=0.0,
            critical_path_length=100.0
        )
        
        return environment, task_states, node_states
    
    def extract_state_features(self, 
                             current_task: TaskState, 
                             node_states: List[NodeState],
                             environment: EnvironmentState) -> np.ndarray:
        """æå–çŠ¶æ€ç‰¹å¾"""
        features = []
        
        # ä»»åŠ¡ç‰¹å¾
        features.extend([
            current_task.computation_size / 1e10,  # å½’ä¸€åŒ–
            len(current_task.parents),
            len(current_task.children),
            float(current_task.is_critical_path),
            current_task.data_locality_score
        ])
        
        # èŠ‚ç‚¹ç‰¹å¾
        for node in node_states:
            features.extend([
                node.speed / 5.0,  # å½’ä¸€åŒ–
                node.current_load,
                node.available_time / 100.0,  # å½’ä¸€åŒ–
                node.data_availability.get(current_task.id, 0.0)
            ])
        
        # ç¯å¢ƒç‰¹å¾
        features.extend([
            environment.workflow_progress,
            environment.current_time / 1000.0,  # å½’ä¸€åŒ–
            len(environment.pending_tasks) / 20.0  # å½’ä¸€åŒ–
        ])
        
        return np.array(features, dtype=np.float32)
    
    def simulate_step(self, 
                     task: TaskState, 
                     action: int, 
                     node_states: List[NodeState],
                     environment: EnvironmentState) -> Tuple[float, EnvironmentState, bool]:
        """æ¨¡æ‹Ÿä¸€æ­¥æ‰§è¡Œ"""
        chosen_node = node_states[action]
        
        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        execution_time = task.computation_size / chosen_node.speed
        
        # è®¡ç®—æ­¥éª¤å¥–åŠ±
        step_reward = self.reward_calculator.calculate_step_reward(
            task, chosen_node, node_states, environment
        )
        
        # æ›´æ–°ç¯å¢ƒ
        new_environment = EnvironmentState(
            current_time=environment.current_time + execution_time,
            pending_tasks=[t for t in environment.pending_tasks if t.id != task.id],
            node_states=node_states,
            workflow_progress=environment.workflow_progress + 1.0/20.0,
            critical_path_length=environment.critical_path_length
        )
        
        # æ›´æ–°èŠ‚ç‚¹çŠ¶æ€
        chosen_node.current_load += 0.1
        chosen_node.available_time += execution_time
        
        # æ£€æŸ¥æ˜¯å¦ç»“æŸ
        done = len(new_environment.pending_tasks) == 0
        
        return step_reward, new_environment, done
    
    def train_episode(self) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepisode"""
        environment, task_states, node_states = self.create_mock_environment()
        
        total_reward = 0.0
        total_makespan = 0.0
        step_count = 0
        
        current_tasks = task_states.copy()
        
        while current_tasks and step_count < 50:  # é™åˆ¶æœ€å¤§æ­¥æ•°
            # é€‰æ‹©å½“å‰ä»»åŠ¡ï¼ˆç®€åŒ–ï¼šæŒ‰é¡ºåºï¼‰
            current_task = current_tasks[0]
            
            # æå–çŠ¶æ€ç‰¹å¾
            state = self.extract_state_features(current_task, node_states, environment)
            
            # æ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œ
            action = self.agent.act(state, training=True)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            step_reward, new_environment, done = self.simulate_step(
                current_task, action, node_states, environment
            )
            
            # æå–ä¸‹ä¸€çŠ¶æ€ç‰¹å¾
            if len(current_tasks) > 1:
                next_task = current_tasks[1]
                next_state = self.extract_state_features(next_task, node_states, new_environment)
            else:
                next_state = np.zeros_like(state)
            
            # å­˜å‚¨ç»éªŒ
            self.agent.remember(state, action, step_reward, next_state, done)
            
            # æ›´æ–°çŠ¶æ€
            environment = new_environment
            current_tasks = current_tasks[1:]
            total_reward += step_reward
            total_makespan = environment.current_time
            step_count += 1
            
            # è®­ç»ƒæ™ºèƒ½ä½“
            if step_count % 4 == 0:  # æ¯4æ­¥è®­ç»ƒä¸€æ¬¡
                loss = self.agent.replay()
            
            if done:
                break
        
        # è®¡ç®—æœ€ç»ˆå¥–åŠ±
        baseline_makespan = 200.0  # åŸºå‡†makespan
        final_reward = self.reward_calculator.calculate_final_reward(total_makespan, baseline_makespan)
        total_reward += final_reward
        
        return {
            'total_reward': total_reward,
            'makespan': total_makespan,
            'step_count': step_count,
            'epsilon': self.agent.epsilon
        }
    
    def train(self, episodes: int = 1000) -> Dict[str, Any]:
        """è®­ç»ƒDRLæ™ºèƒ½ä½“"""
        print(f"ğŸš€ å¼€å§‹æ”¹è¿›çš„DRLè®­ç»ƒ: {episodes} episodes")
        
        # åˆå§‹åŒ–æ™ºèƒ½ä½“
        state_dim = 5 + 4 * 4 + 3  # ä»»åŠ¡ç‰¹å¾ + èŠ‚ç‚¹ç‰¹å¾ + ç¯å¢ƒç‰¹å¾
        action_dim = 4  # 4ä¸ªèŠ‚ç‚¹
        
        self.agent = ImprovedDQNAgent(
            state_dim=state_dim,
            action_dim=action_dim,
            **self.config.get('drl', {})
        )
        
        # è®­ç»ƒå¾ªç¯
        best_makespan = float('inf')
        recent_rewards = deque(maxlen=100)
        
        for episode in range(episodes):
            episode_results = self.train_episode()
            self.training_history.append(episode_results)
            recent_rewards.append(episode_results['total_reward'])
            
            # æ›´æ–°æœ€ä½³æ€§èƒ½
            if episode_results['makespan'] < best_makespan:
                best_makespan = episode_results['makespan']
            
            # æ‰“å°è¿›åº¦
            if episode % 50 == 0:
                avg_reward = np.mean(recent_rewards) if recent_rewards else 0
                print(f"Episode {episode}: "
                      f"å¹³å‡å¥–åŠ±={avg_reward:.3f}, "
                      f"Makespan={episode_results['makespan']:.2f}, "
                      f"Îµ={episode_results['epsilon']:.3f}")
        
        # è®­ç»ƒå®Œæˆç»Ÿè®¡
        final_avg_reward = np.mean([h['total_reward'] for h in self.training_history[-100:]])
        final_avg_makespan = np.mean([h['makespan'] for h in self.training_history[-100:]])
        
        training_summary = {
            'total_episodes': episodes,
            'final_avg_reward': final_avg_reward,
            'final_avg_makespan': final_avg_makespan,
            'best_makespan': best_makespan,
            'training_history': self.training_history
        }
        
        print(f"âœ… DRLè®­ç»ƒå®Œæˆ!")
        print(f"   æœ€ç»ˆå¹³å‡å¥–åŠ±: {final_avg_reward:.3f}")
        print(f"   æœ€ç»ˆå¹³å‡Makespan: {final_avg_makespan:.2f}s")
        print(f"   æœ€ä½³Makespan: {best_makespan:.2f}s")
        
        return training_summary
    
    def save_model(self, model_path: str):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        if self.agent is None:
            raise ValueError("æ™ºèƒ½ä½“å°šæœªåˆå§‹åŒ–æˆ–è®­ç»ƒ")
        
        checkpoint = {
            'q_network_state_dict': self.agent.q_network.state_dict(),
            'target_network_state_dict': self.agent.target_network.state_dict(),
            'optimizer_state_dict': self.agent.optimizer.state_dict(),
            'training_step': self.agent.training_step,
            'epsilon': self.agent.epsilon,
            'config': self.config,
            'training_history': self.training_history,
            'metadata': {
                'state_dim': self.agent.state_dim,
                'action_dim': self.agent.action_dim,
                'training_completed': True,
                'final_performance': {
                    'avg_makespan': np.mean([h['makespan'] for h in self.training_history[-100:]]),
                    'best_makespan': min(h['makespan'] for h in self.training_history)
                }
            }
        }
        
        torch.save(checkpoint, model_path)
        print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        # ä¿å­˜è®­ç»ƒå†å²
        history_path = model_path.replace('.pth', '_history.json')
        with open(history_path, 'w') as f:
            json.dump(self.training_history, f, indent=2)
        print(f"ğŸ“Š è®­ç»ƒå†å²å·²ä¿å­˜: {history_path}")

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='WASS-RAG æ”¹è¿›DRLè®­ç»ƒå™¨')
    parser.add_argument('--config', default='configs/experiment.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--episodes', type=int, default=1000, help='è®­ç»ƒepisodeæ•°')
    parser.add_argument('--output', default='models/improved_wass_drl.pth', help='è¾“å‡ºæ¨¡å‹è·¯å¾„')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(args.output).parent.mkdir(parents=True, exist_ok=True)
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = WRENCHDRLTrainer(args.config)
    
    # è®­ç»ƒ
    results = trainer.train(args.episodes)
    
    # ä¿å­˜æ¨¡å‹
    trainer.save_model(args.output)
    
    # ä¿å­˜è®­ç»ƒç»“æœæ‘˜è¦
    summary_path = args.output.replace('.pth', '_summary.json')
    with open(summary_path, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"ğŸ‰ è®­ç»ƒå®Œæˆ! æ¨¡å‹å’Œç»“æœå·²ä¿å­˜åˆ° {Path(args.output).parent}")

if __name__ == "__main__":
    main()
