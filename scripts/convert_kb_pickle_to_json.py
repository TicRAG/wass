#!/usr/bin/env python3
"""Convert legacy wrench_rag_knowledge_base.pkl into unified JSON format.

Usage:
  python scripts/convert_kb_pickle_to_json.py \
      --input data/wrench_rag_knowledge_base.pkl \
      --output data/wrench_rag_knowledge_base.json

The script is resilient to partial / malformed pickle contents. It extracts a
minimal, scheduler-agnostic subset of fields required by the new
JSONKnowledgeBase structure, and preserves original file for audit.
"""
from __future__ import annotations
import argparse
import pickle
import json
import sys
from pathlib import Path
from types import SimpleNamespace

ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(ROOT))

try:
    from src.knowledge_base.json_kb import JSONKnowledgeBase, KnowledgeCase
except Exception as e:
    print(f"Failed to import JSON KB module: {e}")
    raise

def coerce_float(v, default=0.0):
    try:
        return float(v)
    except Exception:
        return default

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--input", default="data/wrench_rag_knowledge_base.pkl")
    ap.add_argument("--output", default="data/wrench_rag_knowledge_base.json")
    ap.add_argument("--limit", type=int, default=0, help="Optionally limit number of cases")
    args = ap.parse_args()

    in_path = Path(args.input)
    out_path = Path(args.output)
    if not in_path.exists():
     print(f"Metadata written -> {out_path.with_suffix('.json.meta')}")

    # Provide a lightweight placeholder to satisfy attribute resolution
    class WRENCHKnowledgeCase(SimpleNamespace):
        pass
    globals()['WRENCHKnowledgeCase'] = WRENCHKnowledgeCase

    with in_path.open('rb') as f:
        try:
            data = pickle.load(f)
        except Exception as e:
            print(f"Failed to load pickle: {e}")
            sys.exit(2)

    # Accept structures: {'cases': [...]}, list, single object
    if isinstance(data, dict):
        raw_cases = data.get('cases') or data.get('sample_cases') or []
    elif isinstance(data, list):
        raw_cases = data
    else:
        raw_cases = [data]

    kb = JSONKnowledgeBase()

    def emit(case_like, idx):
        # Accept dataclass/object/dict
        if hasattr(case_like, '__dict__') and not isinstance(case_like, dict):
            case_dict = case_like.__dict__
        elif isinstance(case_like, dict):
            case_dict = case_like
        else:
            return

        kc = KnowledgeCase(
            workflow_id=str(case_dict.get('workflow_id', f'wf_{idx}')),
            task_id=str(case_dict.get('task_id', f'task_{idx}')),
            scheduler_type=str(case_dict.get('scheduler_type', 'unknown')),
            chosen_node=str(case_dict.get('chosen_node', 'ComputeHost1')),
            task_execution_time=coerce_float(case_dict.get('task_execution_time', 0.0)),
            workflow_makespan=coerce_float(case_dict.get('workflow_makespan', case_dict.get('makespan', 0.0))),
            task_features=list(map(float, case_dict.get('task_features', [])[:32])) if case_dict.get('task_features') is not None else None,
            workflow_embedding=list(map(float, case_dict.get('workflow_embedding', [])[:64])) if case_dict.get('workflow_embedding') is not None else None,
            quality_score=None
        )
        kb.add_case(kc)

    for i, c in enumerate(raw_cases):
        if args.limit and i >= args.limit:
            break
        try:
            emit(c, i)
        except Exception:
            continue

    kb.save_json(out_path)
    print(f"Converted {len(kb.cases)} cases -> {out_path}")

    # Write a lightweight metadata sidecar for auditing
    meta = {
        "source_pickle": str(in_path),
        "output_json": str(out_path),
        "case_count": len(kb.cases),
        "note": "Generated by convert_kb_pickle_to_json.py"
    }
    with out_path.with_suffix('.json.meta').open('w', encoding='utf-8') as mf:
        json.dump(meta, mf, ensure_ascii=False, indent=2)
    print(f"Metadata written -> {out_path.with_suffix('.json.meta')}")

if __name__ == "__main__":
    main()
