#!/usr/bin/env python3
"""
åŸºäºWRENCHçš„DRLæ™ºèƒ½ä½“è®­ç»ƒè„šæœ¬
é€šè¿‡çœŸå®çš„WRENCHä»¿çœŸç¯å¢ƒè®­ç»ƒæ·±åº¦å¼ºåŒ–å­¦ä¹ è°ƒåº¦å™¨
"""

import sys
import os
import json
import time
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
from pathlib import Path
from typing import Dict, List, Tuple, Any
import yaml

# ç¡®ä¿èƒ½å¯¼å…¥WRENCH
try:
    import wrench
except ImportError:
    print("Error: WRENCH not available. Please install wrench-python-api.")
    sys.exit(1)

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, str(parent_dir))

def load_config(cfg_path: str) -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(cfg_path, 'r', encoding='utf-8') as f:
        cfg = yaml.safe_load(f) or {}
    
    # Process includes
    if 'include' in cfg:
        base_dir = os.path.dirname(cfg_path)
        for include_file in cfg['include']:
            include_path = os.path.join(base_dir, include_file)
            if os.path.exists(include_path):
                with open(include_path, 'r', encoding='utf-8') as f:
                    include_cfg = yaml.safe_load(f) or {}
                    for key, value in include_cfg.items():
                        if key not in cfg:
                            cfg[key] = value
    
    # ğŸ† é›†æˆè°ƒä¼˜åçš„æœ€ä½³è¶…å‚æ•°é…ç½®
    optimized_params = get_optimized_hyperparameters()
    cfg.update(optimized_params)
    
    return cfg

def get_optimized_hyperparameters() -> Dict:
    """
    è·å–è°ƒä¼˜åçš„æœ€ä½³è¶…å‚æ•°é…ç½®
    åŸºäº100æ¬¡è¶…å‚æ•°æœç´¢çš„æœ€ä¼˜ç»“æœ
    """
    # å°è¯•ä»è°ƒä¼˜ç»“æœæ–‡ä»¶åŠ è½½
    tuned_config_path = "/data/workspace/wass/results/local_hyperparameter_tuning/best_hyperparameters_for_training.yaml"
    
    if os.path.exists(tuned_config_path):
        print("ğŸ“Š åŠ è½½è°ƒä¼˜åçš„æœ€ä½³è¶…å‚æ•°é…ç½®...")
        try:
            with open(tuned_config_path, 'r') as f:
                tuned_config = yaml.safe_load(f)
            
            # è½¬æ¢ä¸ºè®­ç»ƒè„šæœ¬éœ€è¦çš„æ ¼å¼
            optimized = {
                'learning_rate': tuned_config['training']['learning_rate'],
                'gamma': tuned_config['training']['gamma'],
                'epsilon_start': tuned_config['training']['epsilon_start'],
                'epsilon_end': tuned_config['training']['epsilon_end'],
                'epsilon_decay': tuned_config['training']['epsilon_decay'],
                'batch_size': tuned_config['training']['batch_size'],
                'memory_size': tuned_config['training']['memory_size'],
                'target_update_freq': tuned_config['training']['target_update_freq'],
                'hidden_dim_1': tuned_config['model']['hidden_layers'][0],
                'hidden_dim_2': tuned_config['model']['hidden_layers'][1],
                'dropout_rate': tuned_config['model']['dropout_rate'],
                # å¯†é›†å¥–åŠ±æƒé‡
                'data_locality_weight': tuned_config['reward_weights']['data_locality_weight'],
                'waiting_time_weight': tuned_config['reward_weights']['waiting_time_weight'],
                'critical_path_weight': tuned_config['reward_weights']['critical_path_weight'],
                'load_balancing_weight': tuned_config['reward_weights']['load_balancing_weight']
            }
            
            print(f"  âœ… å­¦ä¹ ç‡: {optimized['learning_rate']}")
            print(f"  âœ… Gamma: {optimized['gamma']}")
            print(f"  âœ… ç½‘ç»œç»“æ„: [{optimized['hidden_dim_1']}, {optimized['hidden_dim_2']}]")
            print(f"  âœ… æ‰¹æ¬¡å¤§å°: {optimized['batch_size']}")
            print(f"  âœ… å…³é”®è·¯å¾„æƒé‡: {optimized['critical_path_weight']}")
            
            return optimized
            
        except Exception as e:
            print(f"âš ï¸ åŠ è½½è°ƒä¼˜é…ç½®å¤±è´¥: {e}")
    
    # å¦‚æœæ²¡æœ‰è°ƒä¼˜ç»“æœï¼Œä½¿ç”¨ç¡¬ç¼–ç çš„æœ€ä½³é…ç½®
    print("ğŸ“Š ä½¿ç”¨ç¡¬ç¼–ç çš„æœ€ä½³è¶…å‚æ•°é…ç½®...")
    return {
        'learning_rate': 0.0005,  # è°ƒä¼˜å¾—å‡ºçš„æœ€ä½³å­¦ä¹ ç‡
        'gamma': 0.99,           # è°ƒä¼˜å¾—å‡ºçš„æœ€ä½³æŠ˜æ‰£å› å­
        'epsilon_start': 1.0,
        'epsilon_end': 0.01,
        'epsilon_decay': 0.995,
        'batch_size': 64,        # è°ƒä¼˜å¾—å‡ºçš„æœ€ä½³æ‰¹æ¬¡å¤§å°
        'memory_size': 2000,
        'target_update_freq': 100,
        'hidden_dim_1': 256,     # è°ƒä¼˜å¾—å‡ºçš„æœ€ä½³ç½‘ç»œç»“æ„
        'hidden_dim_2': 128,
        'dropout_rate': 0.2,
        # å¯†é›†å¥–åŠ±æƒé‡ (æŒ‰æŠ€æœ¯æŠ¥å‘Šè®¾è®¡)
        'data_locality_weight': 0.2,
        'waiting_time_weight': 0.1,
        'critical_path_weight': 0.4,  # æœ€é«˜æƒé‡
        'load_balancing_weight': 0.1
    }

class WRENCHEnvironment:
    """åŸºäºWRENCHçš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ"""
    
    def __init__(self, platform_file: str, controller_host: str = "ControllerHost"):
        self.platform_file = platform_file
        self.controller_host = controller_host
        
        # è¯»å–å¹³å°é…ç½®
        with open(platform_file, 'r', encoding='utf-8') as f:
            self.platform_xml = f.read()
        
        self.sim = None
        self.workflow = None
        self.compute_service = None
        self.storage_service = None
        
        # è·å–è®¡ç®—èŠ‚ç‚¹ä¿¡æ¯
        self._setup_nodes()
        
        # çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´
        self.state_dim = 5 + len(self.compute_nodes) * 3  # ä»»åŠ¡ç‰¹å¾ + èŠ‚ç‚¹ç‰¹å¾
        self.action_dim = len(self.compute_nodes)
        
        print(f"WRENCHç¯å¢ƒåˆå§‹åŒ–å®Œæˆ: {len(self.compute_nodes)} è®¡ç®—èŠ‚ç‚¹, çŠ¶æ€ç»´åº¦: {self.state_dim}")
    
    def _setup_nodes(self):
        """è®¾ç½®è®¡ç®—èŠ‚ç‚¹ä¿¡æ¯"""
        # ä»å¹³å°XMLä¸­è§£æèŠ‚ç‚¹ä¿¡æ¯
        self.compute_nodes = ["ComputeHost1", "ComputeHost2", "ComputeHost3", "ComputeHost4"]
        self.node_capacities = {
            "ComputeHost1": 2.0,  # 2GHz
            "ComputeHost2": 3.0,  # 3GHz  
            "ComputeHost3": 2.5,  # 2.5GHz
            "ComputeHost4": 4.0   # 4GHz
        }
    
    def reset(self, num_tasks: int = 10) -> np.ndarray:
        """é‡ç½®ç¯å¢ƒå¹¶è¿”å›åˆå§‹çŠ¶æ€"""
        if self.sim:
            try:
                self.sim.terminate()
            except:
                pass
        
        # åˆ›å»ºæ–°çš„ä»¿çœŸ
        self.sim = wrench.Simulation()
        self.sim.start(self.platform_xml, self.controller_host)
        
        # åˆ›å»ºæœåŠ¡
        self.storage_service = self.sim.create_simple_storage_service("StorageHost", ["/storage"])
        
        # åˆ›å»ºè®¡ç®—æœåŠ¡
        compute_resources = {}
        for node in self.compute_nodes:
            compute_resources[node] = (4, 8_589_934_592)  # 4æ ¸, 8GBå†…å­˜
        
        self.compute_service = self.sim.create_bare_metal_compute_service(
            "ComputeHost1", compute_resources, "/scratch", {}, {}
        )
        
        # åˆ›å»ºå·¥ä½œæµ
        self.workflow = self.sim.create_workflow()
        self.tasks = []
        self.files = []
        
        # åˆ›å»ºä»»åŠ¡å’Œæ–‡ä»¶
        for i in range(num_tasks):
            # ä¸åŒç±»å‹çš„ä»»åŠ¡
            if i % 3 == 0:  # CPUå¯†é›†å‹
                flops = random.uniform(8e9, 15e9)
            elif i % 3 == 1:  # ä¸­ç­‰ä»»åŠ¡
                flops = random.uniform(3e9, 8e9)
            else:  # è½»é‡ä»»åŠ¡
                flops = random.uniform(1e9, 3e9)
            
            task = self.workflow.add_task(f"task_{i}", flops, 1, 1, 0)
            self.tasks.append(task)
            
            # åˆ›å»ºè¾“å‡ºæ–‡ä»¶
            if i < num_tasks - 1:
                output_file = self.sim.add_file(f"output_{i}", random.randint(1024, 10240))
                task.add_output_file(output_file)
                self.files.append(output_file)
        
        # åˆ›å»ºä»»åŠ¡ä¾èµ–å…³ç³»
        for i in range(1, min(num_tasks, len(self.files) + 1)):
            if i > 1 and random.random() < 0.3:  # 30%æ¦‚ç‡æœ‰ä¾èµ–
                dep_idx = random.randint(0, i-2)
                if dep_idx < len(self.files):
                    self.tasks[i].add_input_file(self.files[dep_idx])
        
        # ä¸ºæ‰€æœ‰æ–‡ä»¶åˆ›å»ºå‰¯æœ¬åœ¨å­˜å‚¨æœåŠ¡ä¸Š
        for file in self.files:
            self.storage_service.create_file_copy(file)
        
        # åˆå§‹åŒ–è°ƒåº¦çŠ¶æ€
        self.scheduled_tasks = set()
        self.task_completion_times = {}
        self.node_availability = {node: 0.0 for node in self.compute_nodes}
        self.current_time = 0.0
        
        # è¿”å›åˆå§‹çŠ¶æ€
        return self._get_state()
    
    def _get_state(self) -> np.ndarray:
        """è·å–å½“å‰çŠ¶æ€å‘é‡"""
        ready_tasks = self.workflow.get_ready_tasks()
        
        if not ready_tasks:
            # å¦‚æœæ²¡æœ‰å°±ç»ªä»»åŠ¡ï¼Œè¿”å›é›¶çŠ¶æ€
            return np.zeros(self.state_dim, dtype=np.float32)
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªå°±ç»ªä»»åŠ¡
        current_task = ready_tasks[0]
        
        # ä»»åŠ¡ç‰¹å¾
        task_features = [
            current_task.get_flops() / 1e9,  # æ ‡å‡†åŒ–åˆ°GFlops
            len(current_task.get_input_files()),
            current_task.get_number_of_children(),
            len(self.tasks),  # æ€»ä»»åŠ¡æ•°
            len(self.scheduled_tasks) / len(self.tasks)  # å®Œæˆè¿›åº¦
        ]
        
        # èŠ‚ç‚¹ç‰¹å¾
        node_features = []
        for node in self.compute_nodes:
            capacity = self.node_capacities[node]
            availability = self.node_availability[node]
            
            # è®¡ç®—ä¼°è®¡æ‰§è¡Œæ—¶é—´
            exec_time = current_task.get_flops() / (capacity * 1e9)
            
            node_features.extend([
                capacity / 4.0,  # æ ‡å‡†åŒ–å®¹é‡
                availability / 100.0,  # æ ‡å‡†åŒ–å¯ç”¨æ—¶é—´
                exec_time / 10.0   # æ ‡å‡†åŒ–æ‰§è¡Œæ—¶é—´
            ])
        
        state = np.array(task_features + node_features, dtype=np.float32)
        return state
    
    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict]:
        """æ‰§è¡ŒåŠ¨ä½œå¹¶è¿”å›æ–°çŠ¶æ€ã€å¥–åŠ±ã€æ˜¯å¦ç»“æŸã€é¢å¤–ä¿¡æ¯"""
        ready_tasks = self.workflow.get_ready_tasks()
        
        if not ready_tasks:
            # æ²¡æœ‰å°±ç»ªä»»åŠ¡ï¼Œæ£€æŸ¥æ˜¯å¦å®Œæˆ
            done = self.workflow.is_done()
            return self._get_state(), 0.0, done, {}
        
        # é€‰æ‹©è¦è°ƒåº¦çš„ä»»åŠ¡
        task_to_schedule = ready_tasks[0]
        
        # æ‰§è¡Œè°ƒåº¦åŠ¨ä½œ
        chosen_node = self.compute_nodes[action % len(self.compute_nodes)]
        
        # åˆ›å»ºä½œä¸šå¹¶æäº¤
        file_locations = {}
        for f in task_to_schedule.get_input_files():
            file_locations[f] = self.storage_service
        for f in task_to_schedule.get_output_files():
            file_locations[f] = self.storage_service
        
        job = self.sim.create_standard_job([task_to_schedule], file_locations)
        
        try:
            self.compute_service.submit_standard_job(job)
            
            # ç­‰å¾…ä½œä¸šå®Œæˆ
            while True:
                event = self.sim.wait_for_next_event()
                if event["event_type"] == "standard_job_completion":
                    completed_job = event["standard_job"]
                    if completed_job == job:
                        break
                elif event["event_type"] == "simulation_termination":
                    break
            
            # æ›´æ–°çŠ¶æ€
            self.scheduled_tasks.add(task_to_schedule.get_name())
            completion_time = self.sim.get_simulated_time()
            self.task_completion_times[task_to_schedule.get_name()] = completion_time
            self.current_time = completion_time
            
            # è®¡ç®—å¥–åŠ±
            reward = self._calculate_reward(task_to_schedule, chosen_node, completion_time)
            
        except Exception as e:
            print(f"è°ƒåº¦é”™è¯¯: {e}")
            reward = -10.0  # ä¸¥é‡æƒ©ç½š
        
        # æ£€æŸ¥æ˜¯å¦å®Œæˆ
        done = self.workflow.is_done()
        
        return self._get_state(), reward, done, {
            "task": task_to_schedule.get_name(),
            "node": chosen_node,
            "completion_time": completion_time
        }
    
    def _calculate_reward(self, task, chosen_node: str, completion_time: float) -> float:
        """
        è®¡ç®—å¯†é›†å¥–åŠ±å‡½æ•° (åŸºäºæŠ€æœ¯æŠ¥å‘Šçš„å¥–åŠ±è®¾è®¡)
        R_total = R_step + R_final
        """
        # ğŸ¯ å¯†é›†å¥–åŠ±è®¾è®¡ - R_step (ä¸­é—´å¥–åŠ±)
        
        # 1. æ•°æ®å±€éƒ¨æ€§å¥–åŠ±
        data_locality_reward = self._calculate_data_locality_reward(task, chosen_node)
        
        # 2. ç­‰å¾…æ—¶é—´æƒ©ç½š  
        waiting_time_penalty = self._calculate_waiting_time_penalty(task, chosen_node)
        
        # 3. å…³é”®è·¯å¾„å¥–åŠ±
        critical_path_reward = self._calculate_critical_path_reward(task, chosen_node)
        
        # 4. è´Ÿè½½å‡è¡¡å¥–åŠ±
        load_balancing_reward = self._calculate_load_balancing_reward(chosen_node)
        
        # ä½¿ç”¨è°ƒä¼˜åçš„æƒé‡ç»„åˆ
        config = getattr(self, 'config', {})
        data_locality_weight = config.get('data_locality_weight', 0.2)
        waiting_time_weight = config.get('waiting_time_weight', 0.1)
        critical_path_weight = config.get('critical_path_weight', 0.4)
        load_balancing_weight = config.get('load_balancing_weight', 0.1)
        
        r_step = (
            data_locality_weight * data_locality_reward +
            waiting_time_weight * (-waiting_time_penalty) +
            critical_path_weight * critical_path_reward +
            load_balancing_weight * load_balancing_reward
        )
        
        # R_final (æœ€ç»ˆå¥–åŠ±) - åœ¨å·¥ä½œæµç»“æŸæ—¶ç»™äºˆ
        r_final = -completion_time / 20.0  # æ ‡å‡†åŒ–çš„å®Œæˆæ—¶é—´å¥–åŠ±
        
        total_reward = r_step + r_final
        return total_reward
    
    def _calculate_data_locality_reward(self, task, chosen_node: str) -> float:
        """è®¡ç®—æ•°æ®å±€éƒ¨æ€§å¥–åŠ±"""
        # ä¿®å¤WRENCH APIå…¼å®¹æ€§ï¼šä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•è·å–è¾“å…¥æ–‡ä»¶
        try:
            if hasattr(task, 'get_input_files'):
                input_files = task.get_input_files()
            elif hasattr(task, 'input_files'):
                input_files = task.input_files
            else:
                input_files = []
            
            if not input_files:
                return 0.1  # æ²¡æœ‰è¾“å…¥æ–‡ä»¶çš„ä»»åŠ¡ç»™å°å¥–åŠ±
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦åœ¨æœ¬åœ° (ç®€åŒ–å‡è®¾)
            local_data_ratio = 0.8 if chosen_node == "ComputeHost1" else 0.5
            return local_data_ratio
            
        except Exception as e:
            # å¦‚æœå‡ºé”™ï¼Œè¿”å›é»˜è®¤å€¼
            return 0.3
    
    def _calculate_waiting_time_penalty(self, task, chosen_node: str) -> float:
        """è®¡ç®—ç­‰å¾…æ—¶é—´æƒ©ç½š"""
        node_availability = self.node_availability.get(chosen_node, 0.0)
        current_time = self.current_time
        waiting_time = max(0, node_availability - current_time)
        return waiting_time / 10.0  # æ ‡å‡†åŒ–
    
    def _calculate_critical_path_reward(self, task, chosen_node: str) -> float:
        """è®¡ç®—å…³é”®è·¯å¾„å¥–åŠ±"""
        # ä¿®å¤WRENCH APIå…¼å®¹æ€§ï¼šä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•è·å–åç»§ä»»åŠ¡
        try:
            # WRENCH Taskå¯¹è±¡ä½¿ç”¨ä¸åŒçš„æ–¹æ³•å
            if hasattr(task, 'get_children_tasks'):
                children = task.get_children_tasks()
            elif hasattr(task, 'children'):
                children = task.children
            else:
                # å¦‚æœæ²¡æœ‰ç›¸å…³æ–¹æ³•ï¼Œä½¿ç”¨å·¥ä½œæµçº§åˆ«çš„ä¿¡æ¯
                children = []
                for other_task in self.tasks:
                    if other_task != task:
                        # æ£€æŸ¥æ˜¯å¦æœ‰ä¾èµ–å…³ç³»ï¼ˆç®€åŒ–å®ç°ï¼‰
                        if hasattr(other_task, 'get_input_files'):
                            input_files = other_task.get_input_files()
                            output_files = task.get_output_files() if hasattr(task, 'get_output_files') else []
                            # å¦‚æœother_taskçš„è¾“å…¥åŒ…å«å½“å‰taskçš„è¾“å‡ºï¼Œåˆ™æ˜¯å­ä»»åŠ¡
                            if any(f in input_files for f in output_files):
                                children.append(other_task)
            
            num_children = len(children)
            if num_children > 0:
                return min(num_children / 3.0, 1.0)  # æ ‡å‡†åŒ–åˆ°[0,1]
            return 0.1
            
        except Exception as e:
            # å¦‚æœå‡ºé”™ï¼Œè¿”å›é»˜è®¤å€¼
            return 0.3  # ä¸­ç­‰é‡è¦æ€§
    
    def _calculate_load_balancing_reward(self, chosen_node: str) -> float:
        """è®¡ç®—è´Ÿè½½å‡è¡¡å¥–åŠ±"""
        # æ£€æŸ¥èŠ‚ç‚¹ä½¿ç”¨åˆ†å¸ƒ
        node_usage = {}
        for node in self.compute_nodes:
            node_usage[node] = sum(1 for t_name, t_node in getattr(self, 'task_node_mapping', {}).items() 
                                 if t_node == node)
        
        # è®¡ç®—ä½¿ç”¨æ–¹å·® (è¶Šå°è¶Šå¥½)
        usage_values = list(node_usage.values())
        if len(usage_values) > 1:
            usage_variance = np.var(usage_values)
            return max(0, 1.0 - usage_variance / 10.0)  # åæ¯”å¥–åŠ±
        return 0.5
    
    def get_final_makespan(self) -> float:
        """è·å–æœ€ç»ˆçš„makespan"""
        if not self.task_completion_times:
            return float('inf')
        return max(self.task_completion_times.values())
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.sim:
            try:
                self.sim.terminate()
            except:
                pass
            self.sim = None

class SimpleDQN(nn.Module):
    """ä¼˜åŒ–çš„DQNç½‘ç»œ - ä½¿ç”¨è°ƒä¼˜åçš„æœ€ä½³ç»“æ„"""
    
    def __init__(self, state_dim: int, action_dim: int, config: Dict):
        super().__init__()
        
        # ä½¿ç”¨è°ƒä¼˜åçš„ç½‘ç»œç»“æ„
        hidden_dim_1 = config.get('hidden_dim_1', 256)
        hidden_dim_2 = config.get('hidden_dim_2', 128)
        dropout_rate = config.get('dropout_rate', 0.2)
        
        print(f"ğŸ§  æ„å»ºä¼˜åŒ–çš„DQNç½‘ç»œ: [{state_dim}] -> [{hidden_dim_1}] -> [{hidden_dim_2}] -> [{action_dim}]")
        print(f"   Dropoutç‡: {dropout_rate}")
        
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim_1),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim_1, hidden_dim_2),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim_2, action_dim)
        )
    
    def forward(self, x):
        return self.network(x)

class DQNAgent:
    """ä¼˜åŒ–çš„DQNæ™ºèƒ½ä½“ - ä½¿ç”¨è°ƒä¼˜åçš„æœ€ä½³è¶…å‚æ•°"""
    
    def __init__(self, state_dim: int, action_dim: int, config: Dict):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # ä½¿ç”¨è°ƒä¼˜åçš„è¶…å‚æ•°
        learning_rate = config.get('learning_rate', 0.0005)
        self.gamma = config.get('gamma', 0.99)
        self.epsilon = config.get('epsilon_start', 1.0)
        self.epsilon_decay = config.get('epsilon_decay', 0.995)
        self.epsilon_min = config.get('epsilon_end', 0.01)
        self.batch_size = config.get('batch_size', 64)
        memory_size = config.get('memory_size', 2000)
        self.target_update_freq = config.get('target_update_freq', 100)
        
        print(f"ğŸ¤– åˆ›å»ºä¼˜åŒ–çš„DQNæ™ºèƒ½ä½“:")
        print(f"   å­¦ä¹ ç‡: {learning_rate}")
        print(f"   Gamma: {self.gamma}")
        print(f"   æ¢ç´¢å‚æ•°: Îµ={self.epsilon} -> {self.epsilon_min} (è¡°å‡={self.epsilon_decay})")
        print(f"   æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        print(f"   ç»éªŒå›æ”¾å¤§å°: {memory_size}")
        
        # ä½¿ç”¨ä¼˜åŒ–çš„ç½‘ç»œç»“æ„
        self.q_network = SimpleDQN(state_dim, action_dim, config).to(self.device)
        self.target_network = SimpleDQN(state_dim, action_dim, config).to(self.device)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)
        
        self.memory = deque(maxlen=memory_size)
        self.training_step = 0
        
        # å¤åˆ¶å‚æ•°åˆ°ç›®æ ‡ç½‘ç»œ
        self.target_network.load_state_dict(self.q_network.state_dict())
    
    def act(self, state: np.ndarray) -> int:
        """é€‰æ‹©åŠ¨ä½œ"""
        if np.random.random() < self.epsilon:
            return np.random.randint(0, self.q_network.network[-1].out_features)
        
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        q_values = self.q_network(state_tensor)
        return q_values.argmax().item()
    
    def remember(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒ"""
        self.memory.append((state, action, reward, next_state, done))
    
    def replay(self):
        """ç»éªŒå›æ”¾"""
        if len(self.memory) < self.batch_size:
            return
        
        batch = random.sample(self.memory, self.batch_size)
        states = torch.FloatTensor([e[0] for e in batch]).to(self.device)
        actions = torch.LongTensor([e[1] for e in batch]).to(self.device)
        rewards = torch.FloatTensor([e[2] for e in batch]).to(self.device)
        next_states = torch.FloatTensor([e[3] for e in batch]).to(self.device)
        dones = torch.BoolTensor([e[4] for e in batch]).to(self.device)
        
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        next_q_values = self.target_network(next_states).max(1)[0].detach()
        target_q_values = rewards + (self.gamma * next_q_values * ~dones)
        
        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)
        
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
        
        self.training_step += 1
    
    def update_target_network(self):
        """æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        self.target_network.load_state_dict(self.q_network.state_dict())

def train_drl_agent(config: Dict):
    """è®­ç»ƒDRLæ™ºèƒ½ä½“ - ä½¿ç”¨è°ƒä¼˜åçš„æœ€ä½³è¶…å‚æ•°"""
    print("ğŸš€ å¼€å§‹åŸºäºWRENCHçš„DRLæ™ºèƒ½ä½“è®­ç»ƒ (ä½¿ç”¨è°ƒä¼˜é…ç½®)...")
    
    # æ˜¾ç¤ºå…³é”®é…ç½®
    print(f"ğŸ“Š è®­ç»ƒé…ç½®:")
    print(f"   å­¦ä¹ ç‡: {config.get('learning_rate', 0.0005)}")
    print(f"   ç½‘ç»œç»“æ„: [{config.get('hidden_dim_1', 256)}, {config.get('hidden_dim_2', 128)}]")
    print(f"   æ‰¹æ¬¡å¤§å°: {config.get('batch_size', 64)}")
    print(f"   å…³é”®è·¯å¾„æƒé‡: {config.get('critical_path_weight', 0.4)}")
    
    # åˆ›å»ºç¯å¢ƒ
    platform_file = config.get('platform', {}).get('platform_file', 'configs/platform.xml')
    env = WRENCHEnvironment(platform_file)
    env.config = config  # ä¼ é€’é…ç½®ç»™ç¯å¢ƒï¼Œç”¨äºå¥–åŠ±è®¡ç®—
    
    # åˆ›å»ºæ™ºèƒ½ä½“ (ä½¿ç”¨è°ƒä¼˜é…ç½®)
    agent = DQNAgent(env.state_dim, env.action_dim, config)
    
    # è®­ç»ƒå‚æ•°
    episodes = config.get('drl', {}).get('episodes', 100)  # å¢åŠ è®­ç»ƒepisode
    max_steps = config.get('drl', {}).get('max_steps', 30)
    
    # è®­ç»ƒå¾ªç¯
    episode_rewards = []
    episode_makespans = []
    
    print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ {episodes} episodes...")
    
    for episode in range(episodes):
        state = env.reset(num_tasks=random.randint(8, 20))  # æ›´å¤šä»»åŠ¡å¢åŠ å¤æ‚æ€§
        total_reward = 0
        steps = 0
        
        while steps < max_steps:
            action = agent.act(state)
            next_state, reward, done, info = env.step(action)
            
            agent.remember(state, action, reward, next_state, done)
            state = next_state
            total_reward += reward
            steps += 1
            
            if done:
                break
        
        # ç»éªŒå›æ”¾
        agent.replay()
        
        # æŒ‰è°ƒä¼˜åçš„é¢‘ç‡æ›´æ–°ç›®æ ‡ç½‘ç»œ
        if episode % agent.target_update_freq == 0:
            agent.update_target_network()
        
        # è®°å½•æ€§èƒ½
        makespan = env.get_final_makespan()
        episode_rewards.append(total_reward)
        episode_makespans.append(makespan)
        
        # æ›´è¯¦ç»†çš„è¿›åº¦æŠ¥å‘Š
        if episode % 20 == 0 or episode < 10:
            avg_reward = np.mean(episode_rewards[-10:])
            avg_makespan = np.mean(episode_makespans[-10:])
            print(f"Episode {episode:3d}: å¥–åŠ±={avg_reward:6.2f}, Makespan={avg_makespan:6.2f}s, Îµ={agent.epsilon:.3f}, æ­¥æ•°={steps}")
    
    # ä¿å­˜æ¨¡å‹
    model_path = Path("models/wass_optimized_models.pth")
    model_path.parent.mkdir(exist_ok=True)
    
    try:
        checkpoint = torch.load(model_path, map_location="cpu", weights_only=False)
    except:
        checkpoint = {}
    
    checkpoint["drl_agent"] = agent.q_network.state_dict()
    checkpoint["drl_metadata"] = {
        "episodes": episodes,
        "final_epsilon": agent.epsilon,
        "avg_reward": np.mean(episode_rewards[-10:]),
        "avg_makespan": np.mean(episode_makespans[-10:]),
        "trained_at": time.strftime("%Y-%m-%d %H:%M:%S"),
        "hyperparameters": {
            "learning_rate": config.get('learning_rate'),
            "gamma": config.get('gamma'),
            "batch_size": config.get('batch_size'),
            "hidden_layers": [config.get('hidden_dim_1'), config.get('hidden_dim_2')],
            "reward_weights": {
                "data_locality": config.get('data_locality_weight'),
                "waiting_time": config.get('waiting_time_weight'),
                "critical_path": config.get('critical_path_weight'),
                "load_balancing": config.get('load_balancing_weight')
            }
        },
        "optimization_info": "ä½¿ç”¨è¶…å‚æ•°è°ƒä¼˜åçš„æœ€ä½³é…ç½®è®­ç»ƒ"
    }
    
    torch.save(checkpoint, model_path)
    print(f"âœ… ä¼˜åŒ–DRLæ¨¡å‹å·²ä¿å­˜åˆ° {model_path}")
    
    # æ˜¾ç¤ºè®­ç»ƒæ€»ç»“
    final_makespan = np.mean(episode_makespans[-10:])
    improvement = (episode_makespans[0] - final_makespan) / episode_makespans[0] * 100 if episode_makespans[0] > 0 else 0
    
    print(f"\nğŸ“Š è®­ç»ƒæ€»ç»“:")
    print(f"   æœ€ç»ˆå¹³å‡Makespan: {final_makespan:.2f}s")
    print(f"   ç›¸æ¯”åˆæœŸæ”¹å–„: {improvement:.1f}%")
    print(f"   æœ€ç»ˆæ¢ç´¢ç‡: {agent.epsilon:.3f}")
    print(f"   è®­ç»ƒæ­¥æ•°æ€»è®¡: {agent.training_step}")
    
    # æ¸…ç†
    env.cleanup()
    
    return {
        "final_performance": final_makespan,
        "improvement": improvement,
        "training_episodes": episodes,
        "model_path": str(model_path),
        "hyperparameters_used": config
    }

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ WASS-DRL ä¼˜åŒ–è®­ç»ƒè„šæœ¬")
    print("=" * 50)
    
    if len(sys.argv) != 2:
        print("ä½¿ç”¨æ–¹æ³•: python scripts/train_drl_wrench.py <config.yaml>")
        print("\nğŸ’¡ æç¤º: è„šæœ¬å·²é›†æˆè°ƒä¼˜åçš„æœ€ä½³è¶…å‚æ•°ï¼Œæ— éœ€é¢å¤–é…ç½®")
        print("   è‡ªåŠ¨ä½¿ç”¨ä»¥ä¸‹ä¼˜åŒ–é…ç½®:")
        print("   - å­¦ä¹ ç‡: 0.0005")
        print("   - ç½‘ç»œç»“æ„: [256, 128]")  
        print("   - æ‰¹æ¬¡å¤§å°: 64")
        print("   - å¯†é›†å¥–åŠ±å‡½æ•°")
        sys.exit(1)
    
    cfg_path = sys.argv[1]
    config = load_config(cfg_path)
    
    results = train_drl_agent(config)
    
    print(f"\nğŸ‰ DRLè®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ† æœ€ç»ˆæ€§èƒ½: {results['final_performance']:.2f}s")
    print(f"ğŸ“ˆ æ€§èƒ½æ”¹å–„: {results.get('improvement', 0):.1f}%")
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {results['model_path']}")
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥: è¿è¡Œå®Œæ•´å®éªŒéªŒè¯è°ƒä¼˜æ•ˆæœ")
    print("   python experiments/wrench_real_experiment.py")

if __name__ == "__main__":
    main()
