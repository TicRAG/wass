#!/usr/bin/env python3
"""
åŸºäºWRENCHçš„DRLæ™ºèƒ½ä½“è®­ç»ƒè„šæœ¬
é€šè¿‡çœŸå®çš„WRENCHä»¿çœŸç¯å¢ƒè®­ç»ƒæ·±åº¦å¼ºåŒ–å­¦ä¹ è°ƒåº¦å™¨
"""

import sys
import os
import json
import time
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
from pathlib import Path
from typing import Dict, List, Tuple, Any
import yaml

# ç¡®ä¿èƒ½å¯¼å…¥WRENCH
try:
    import wrench
except ImportError:
    print("Error: WRENCH not available. Please install wrench-python-api.")
    sys.exit(1)

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, str(parent_dir))

def load_config(cfg_path: str) -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(cfg_path, 'r', encoding='utf-8') as f:
        cfg = yaml.safe_load(f) or {}
    
    # Process includes
    if 'include' in cfg:
        base_dir = os.path.dirname(cfg_path)
        for include_file in cfg['include']:
            include_path = os.path.join(base_dir, include_file)
            if os.path.exists(include_path):
                with open(include_path, 'r', encoding='utf-8') as f:
                    include_cfg = yaml.safe_load(f) or {}
                    for key, value in include_cfg.items():
                        if key not in cfg:
                            cfg[key] = value
    return cfg

class WRENCHEnvironment:
    """åŸºäºWRENCHçš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ"""
    
    def __init__(self, platform_file: str, controller_host: str = "ControllerHost"):
        self.platform_file = platform_file
        self.controller_host = controller_host
        
        # è¯»å–å¹³å°é…ç½®
        with open(platform_file, 'r', encoding='utf-8') as f:
            self.platform_xml = f.read()
        
        self.sim = None
        self.workflow = None
        self.compute_service = None
        self.storage_service = None
        
        # è·å–è®¡ç®—èŠ‚ç‚¹ä¿¡æ¯
        self._setup_nodes()
        
        # çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´
        self.state_dim = 5 + len(self.compute_nodes) * 3  # ä»»åŠ¡ç‰¹å¾ + èŠ‚ç‚¹ç‰¹å¾
        self.action_dim = len(self.compute_nodes)
        
        print(f"WRENCHç¯å¢ƒåˆå§‹åŒ–å®Œæˆ: {len(self.compute_nodes)} è®¡ç®—èŠ‚ç‚¹, çŠ¶æ€ç»´åº¦: {self.state_dim}")
    
    def _setup_nodes(self):
        """è®¾ç½®è®¡ç®—èŠ‚ç‚¹ä¿¡æ¯"""
        # ä»å¹³å°XMLä¸­è§£æèŠ‚ç‚¹ä¿¡æ¯
        self.compute_nodes = ["ComputeHost1", "ComputeHost2", "ComputeHost3", "ComputeHost4"]
        self.node_capacities = {
            "ComputeHost1": 2.0,  # 2GHz
            "ComputeHost2": 3.0,  # 3GHz  
            "ComputeHost3": 2.5,  # 2.5GHz
            "ComputeHost4": 4.0   # 4GHz
        }
    
    def reset(self, num_tasks: int = 10) -> np.ndarray:
        """é‡ç½®ç¯å¢ƒå¹¶è¿”å›åˆå§‹çŠ¶æ€"""
        if self.sim:
            try:
                self.sim.terminate()
            except:
                pass
        
        # åˆ›å»ºæ–°çš„ä»¿çœŸ
        self.sim = wrench.Simulation()
        self.sim.start(self.platform_xml, self.controller_host)
        
        # åˆ›å»ºæœåŠ¡
        self.storage_service = self.sim.create_simple_storage_service("StorageHost", ["/storage"])
        
        # åˆ›å»ºè®¡ç®—æœåŠ¡
        compute_resources = {}
        for node in self.compute_nodes:
            compute_resources[node] = (4, 8_589_934_592)  # 4æ ¸, 8GBå†…å­˜
        
        self.compute_service = self.sim.create_bare_metal_compute_service(
            "ComputeHost1", compute_resources, "/scratch", {}, {}
        )
        
        # åˆ›å»ºå·¥ä½œæµ
        self.workflow = self.sim.create_workflow()
        self.tasks = []
        self.files = []
        
        # åˆ›å»ºä»»åŠ¡å’Œæ–‡ä»¶
        for i in range(num_tasks):
            # ä¸åŒç±»å‹çš„ä»»åŠ¡
            if i % 3 == 0:  # CPUå¯†é›†å‹
                flops = random.uniform(8e9, 15e9)
            elif i % 3 == 1:  # ä¸­ç­‰ä»»åŠ¡
                flops = random.uniform(3e9, 8e9)
            else:  # è½»é‡ä»»åŠ¡
                flops = random.uniform(1e9, 3e9)
            
            task = self.workflow.add_task(f"task_{i}", flops, 1, 1, 0)
            self.tasks.append(task)
            
            # åˆ›å»ºè¾“å‡ºæ–‡ä»¶
            if i < num_tasks - 1:
                output_file = self.sim.add_file(f"output_{i}", random.randint(1024, 10240))
                task.add_output_file(output_file)
                self.files.append(output_file)
        
        # åˆ›å»ºä»»åŠ¡ä¾èµ–å…³ç³»
        for i in range(1, min(num_tasks, len(self.files) + 1)):
            if i > 1 and random.random() < 0.3:  # 30%æ¦‚ç‡æœ‰ä¾èµ–
                dep_idx = random.randint(0, i-2)
                if dep_idx < len(self.files):
                    self.tasks[i].add_input_file(self.files[dep_idx])
        
        # ä¸ºæ‰€æœ‰æ–‡ä»¶åˆ›å»ºå‰¯æœ¬åœ¨å­˜å‚¨æœåŠ¡ä¸Š
        for file in self.files:
            self.storage_service.create_file_copy(file)
        
        # åˆå§‹åŒ–è°ƒåº¦çŠ¶æ€
        self.scheduled_tasks = set()
        self.task_completion_times = {}
        self.node_availability = {node: 0.0 for node in self.compute_nodes}
        self.current_time = 0.0
        
        # è¿”å›åˆå§‹çŠ¶æ€
        return self._get_state()
    
    def _get_state(self) -> np.ndarray:
        """è·å–å½“å‰çŠ¶æ€å‘é‡"""
        ready_tasks = self.workflow.get_ready_tasks()
        
        if not ready_tasks:
            # å¦‚æœæ²¡æœ‰å°±ç»ªä»»åŠ¡ï¼Œè¿”å›é›¶çŠ¶æ€
            return np.zeros(self.state_dim, dtype=np.float32)
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªå°±ç»ªä»»åŠ¡
        current_task = ready_tasks[0]
        
        # ä»»åŠ¡ç‰¹å¾
        task_features = [
            current_task.get_flops() / 1e9,  # æ ‡å‡†åŒ–åˆ°GFlops
            len(current_task.get_input_files()),
            current_task.get_number_of_children(),
            len(self.tasks),  # æ€»ä»»åŠ¡æ•°
            len(self.scheduled_tasks) / len(self.tasks)  # å®Œæˆè¿›åº¦
        ]
        
        # èŠ‚ç‚¹ç‰¹å¾
        node_features = []
        for node in self.compute_nodes:
            capacity = self.node_capacities[node]
            availability = self.node_availability[node]
            
            # è®¡ç®—ä¼°è®¡æ‰§è¡Œæ—¶é—´
            exec_time = current_task.get_flops() / (capacity * 1e9)
            
            node_features.extend([
                capacity / 4.0,  # æ ‡å‡†åŒ–å®¹é‡
                availability / 100.0,  # æ ‡å‡†åŒ–å¯ç”¨æ—¶é—´
                exec_time / 10.0   # æ ‡å‡†åŒ–æ‰§è¡Œæ—¶é—´
            ])
        
        state = np.array(task_features + node_features, dtype=np.float32)
        return state
    
    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict]:
        """æ‰§è¡ŒåŠ¨ä½œå¹¶è¿”å›æ–°çŠ¶æ€ã€å¥–åŠ±ã€æ˜¯å¦ç»“æŸã€é¢å¤–ä¿¡æ¯"""
        ready_tasks = self.workflow.get_ready_tasks()
        
        if not ready_tasks:
            # æ²¡æœ‰å°±ç»ªä»»åŠ¡ï¼Œæ£€æŸ¥æ˜¯å¦å®Œæˆ
            done = self.workflow.is_done()
            return self._get_state(), 0.0, done, {}
        
        # é€‰æ‹©è¦è°ƒåº¦çš„ä»»åŠ¡
        task_to_schedule = ready_tasks[0]
        
        # æ‰§è¡Œè°ƒåº¦åŠ¨ä½œ
        chosen_node = self.compute_nodes[action % len(self.compute_nodes)]
        
        # åˆ›å»ºä½œä¸šå¹¶æäº¤
        file_locations = {}
        for f in task_to_schedule.get_input_files():
            file_locations[f] = self.storage_service
        for f in task_to_schedule.get_output_files():
            file_locations[f] = self.storage_service
        
        job = self.sim.create_standard_job([task_to_schedule], file_locations)
        
        try:
            self.compute_service.submit_standard_job(job)
            
            # ç­‰å¾…ä½œä¸šå®Œæˆ
            while True:
                event = self.sim.wait_for_next_event()
                if event["event_type"] == "standard_job_completion":
                    completed_job = event["standard_job"]
                    if completed_job == job:
                        break
                elif event["event_type"] == "simulation_termination":
                    break
            
            # æ›´æ–°çŠ¶æ€
            self.scheduled_tasks.add(task_to_schedule.get_name())
            completion_time = self.sim.get_simulated_time()
            self.task_completion_times[task_to_schedule.get_name()] = completion_time
            self.current_time = completion_time
            
            # è®¡ç®—å¥–åŠ±
            reward = self._calculate_reward(task_to_schedule, chosen_node, completion_time)
            
        except Exception as e:
            print(f"è°ƒåº¦é”™è¯¯: {e}")
            reward = -10.0  # ä¸¥é‡æƒ©ç½š
        
        # æ£€æŸ¥æ˜¯å¦å®Œæˆ
        done = self.workflow.is_done()
        
        return self._get_state(), reward, done, {
            "task": task_to_schedule.get_name(),
            "node": chosen_node,
            "completion_time": completion_time
        }
    
    def _calculate_reward(self, task, chosen_node: str, completion_time: float) -> float:
        """è®¡ç®—å¥–åŠ±å‡½æ•°"""
        # åŸºç¡€å¥–åŠ±ï¼šè´Ÿçš„å®Œæˆæ—¶é—´ï¼ˆè¶Šå¿«è¶Šå¥½ï¼‰
        base_reward = -completion_time / 10.0
        
        # èŠ‚ç‚¹æ•ˆç‡å¥–åŠ±
        task_flops = task.get_flops()
        node_capacity = self.node_capacities[chosen_node]
        efficiency = node_capacity / 4.0  # æ ‡å‡†åŒ–åˆ°æœ€é«˜æ€§èƒ½èŠ‚ç‚¹
        efficiency_bonus = efficiency * 2.0
        
        # è´Ÿè½½å‡è¡¡å¥–åŠ±
        node_usage = sum(1 for t_name, t_node in getattr(self, 'task_node_mapping', {}).items() 
                        if t_node == chosen_node)
        balance_penalty = node_usage * 0.5
        
        total_reward = base_reward + efficiency_bonus - balance_penalty
        return total_reward
    
    def get_final_makespan(self) -> float:
        """è·å–æœ€ç»ˆçš„makespan"""
        if not self.task_completion_times:
            return float('inf')
        return max(self.task_completion_times.values())
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.sim:
            try:
                self.sim.terminate()
            except:
                pass
            self.sim = None

class SimpleDQN(nn.Module):
    """ç®€å•çš„DQNç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )
    
    def forward(self, x):
        return self.network(x)

class DQNAgent:
    """DQNæ™ºèƒ½ä½“"""
    
    def __init__(self, state_dim: int, action_dim: int, lr: float = 1e-3):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        self.q_network = SimpleDQN(state_dim, action_dim).to(self.device)
        self.target_network = SimpleDQN(state_dim, action_dim).to(self.device)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)
        
        self.memory = deque(maxlen=10000)
        self.epsilon = 1.0
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.1
        self.gamma = 0.99
        self.batch_size = 32
        
        # å¤åˆ¶å‚æ•°åˆ°ç›®æ ‡ç½‘ç»œ
        self.target_network.load_state_dict(self.q_network.state_dict())
    
    def act(self, state: np.ndarray) -> int:
        """é€‰æ‹©åŠ¨ä½œ"""
        if np.random.random() < self.epsilon:
            return np.random.randint(0, self.q_network.network[-1].out_features)
        
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        q_values = self.q_network(state_tensor)
        return q_values.argmax().item()
    
    def remember(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒ"""
        self.memory.append((state, action, reward, next_state, done))
    
    def replay(self):
        """ç»éªŒå›æ”¾"""
        if len(self.memory) < self.batch_size:
            return
        
        batch = random.sample(self.memory, self.batch_size)
        states = torch.FloatTensor([e[0] for e in batch]).to(self.device)
        actions = torch.LongTensor([e[1] for e in batch]).to(self.device)
        rewards = torch.FloatTensor([e[2] for e in batch]).to(self.device)
        next_states = torch.FloatTensor([e[3] for e in batch]).to(self.device)
        dones = torch.BoolTensor([e[4] for e in batch]).to(self.device)
        
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        next_q_values = self.target_network(next_states).max(1)[0].detach()
        target_q_values = rewards + (self.gamma * next_q_values * ~dones)
        
        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)
        
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def update_target_network(self):
        """æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        self.target_network.load_state_dict(self.q_network.state_dict())

def train_drl_agent(config: Dict):
    """è®­ç»ƒDRLæ™ºèƒ½ä½“"""
    print("ğŸš€ å¼€å§‹åŸºäºWRENCHçš„DRLæ™ºèƒ½ä½“è®­ç»ƒ...")
    
    # åˆ›å»ºç¯å¢ƒ
    platform_file = config['platform']['platform_file']
    env = WRENCHEnvironment(platform_file)
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    agent = DQNAgent(env.state_dim, env.action_dim)
    
    # è®­ç»ƒå‚æ•°
    episodes = config.get('drl', {}).get('episodes', 50)
    max_steps = config.get('drl', {}).get('max_steps', 20)
    
    # è®­ç»ƒå¾ªç¯
    episode_rewards = []
    episode_makespans = []
    
    for episode in range(episodes):
        state = env.reset(num_tasks=random.randint(5, 15))
        total_reward = 0
        steps = 0
        
        while steps < max_steps:
            action = agent.act(state)
            next_state, reward, done, info = env.step(action)
            
            agent.remember(state, action, reward, next_state, done)
            state = next_state
            total_reward += reward
            steps += 1
            
            if done:
                break
        
        # ç»éªŒå›æ”¾
        agent.replay()
        
        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        if episode % 10 == 0:
            agent.update_target_network()
        
        # è®°å½•æ€§èƒ½
        makespan = env.get_final_makespan()
        episode_rewards.append(total_reward)
        episode_makespans.append(makespan)
        
        if episode % 10 == 0:
            avg_reward = np.mean(episode_rewards[-10:])
            avg_makespan = np.mean(episode_makespans[-10:])
            print(f"Episode {episode}: å¹³å‡å¥–åŠ±={avg_reward:.2f}, å¹³å‡Makespan={avg_makespan:.2f}s, Îµ={agent.epsilon:.3f}")
    
    # ä¿å­˜æ¨¡å‹
    model_path = Path("models/wass_models.pth")
    model_path.parent.mkdir(exist_ok=True)
    
    try:
        checkpoint = torch.load(model_path, map_location="cpu", weights_only=False)
    except:
        checkpoint = {}
    
    checkpoint["drl_agent"] = agent.q_network.state_dict()
    checkpoint["drl_metadata"] = {
        "episodes": episodes,
        "final_epsilon": agent.epsilon,
        "avg_reward": np.mean(episode_rewards[-10:]),
        "avg_makespan": np.mean(episode_makespans[-10:]),
        "trained_at": time.strftime("%Y-%m-%d %H:%M:%S")
    }
    
    torch.save(checkpoint, model_path)
    print(f"âœ… DRLæ¨¡å‹å·²ä¿å­˜åˆ° {model_path}")
    
    # æ¸…ç†
    env.cleanup()
    
    return {
        "final_performance": np.mean(episode_makespans[-10:]),
        "training_episodes": episodes,
        "model_path": str(model_path)
    }

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) != 2:
        print("Usage: python scripts/train_drl_wrench.py <config.yaml>")
        sys.exit(1)
    
    cfg_path = sys.argv[1]
    config = load_config(cfg_path)
    
    results = train_drl_agent(config)
    print(f"\nğŸ‰ DRLè®­ç»ƒå®Œæˆï¼æœ€ç»ˆæ€§èƒ½: {results['final_performance']:.2f}s")

if __name__ == "__main__":
    main()
