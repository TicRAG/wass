#!/usr/bin/env python3
"""
ä½¿ç”¨è°ƒä¼˜åçš„æœ€ä½³è¶…å‚æ•°é‡æ–°è®­ç»ƒWASS-DRLæ¨¡å‹

åŸºäºè¶…å‚æ•°è°ƒä¼˜ç»“æœï¼Œä½¿ç”¨æœ€ä¼˜é…ç½®è¿›è¡Œæ­£å¼è®­ç»ƒ
"""

import os
import yaml
import json
import torch
import numpy as np
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/data/workspace/wass/src')
sys.path.append('/data/workspace/wass/scripts')

from improved_drl_trainer import ImprovedDRLTrainer, DRLSchedulingEnvironment


def load_optimized_config():
    """åŠ è½½è°ƒä¼˜åçš„æœ€ä½³è¶…å‚æ•°é…ç½®"""
    config_path = "/data/workspace/wass/results/local_hyperparameter_tuning/best_hyperparameters_for_training.yaml"
    
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    print("ğŸ“Š ä½¿ç”¨è°ƒä¼˜åçš„æœ€ä½³é…ç½®:")
    print(f"  å­¦ä¹ ç‡: {config['training']['learning_rate']}")
    print(f"  Gamma: {config['training']['gamma']}")
    print(f"  ç½‘ç»œç»“æ„: {config['model']['hidden_layers']}")
    print(f"  æ‰¹æ¬¡å¤§å°: {config['training']['batch_size']}")
    print(f"  å¥–åŠ±æƒé‡: {config['reward_weights']}")
    print(f"  è°ƒä¼˜å¾—åˆ†: {config['tuning_metadata']['best_score']:.4f}")
    
    return config


def train_optimized_model():
    """ä½¿ç”¨æœ€ä¼˜è¶…å‚æ•°è®­ç»ƒæ¨¡å‹"""
    print("ğŸš€ å¼€å§‹ä½¿ç”¨è°ƒä¼˜åé…ç½®è®­ç»ƒWASS-DRLæ¨¡å‹...")
    
    # åŠ è½½æœ€ä¼˜é…ç½®
    config = load_optimized_config()
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    # åˆ›å»ºç¯å¢ƒ
    env = DRLSchedulingEnvironment()
    
    # ä½¿ç”¨æœ€ä¼˜è¶…å‚æ•°åˆ›å»ºè®­ç»ƒå™¨
    trainer = ImprovedDRLTrainer(
        state_dim=env.get_state_dimension(),
        action_dim=env.get_action_dimension(),
        hidden_layers=config['model']['hidden_layers'],
        learning_rate=config['training']['learning_rate'],
        gamma=config['training']['gamma'],
        epsilon_start=config['training']['epsilon_start'],
        epsilon_end=config['training']['epsilon_end'],
        epsilon_decay=config['training']['epsilon_decay'],
        dropout_rate=config['model']['dropout_rate'],
        batch_size=config['training']['batch_size'],
        memory_size=config['training']['memory_size'],
        target_update_freq=config['training']['target_update_freq'],
        data_locality_weight=config['reward_weights']['data_locality_weight'],
        waiting_time_weight=config['reward_weights']['waiting_time_weight'],
        critical_path_weight=config['reward_weights']['critical_path_weight'],
        load_balancing_weight=config['reward_weights']['load_balancing_weight']
    )
    
    print("\nğŸ¯ å¼€å§‹è®­ç»ƒ (ä½¿ç”¨å¯†é›†å¥–åŠ±å‡½æ•°)...")
    
    # è¿›è¡Œå®Œæ•´è®­ç»ƒ (æ›´å¤šepisodes)
    training_metrics = trainer.train(
        episodes=500,  # å¢åŠ è®­ç»ƒepisodes
        max_steps_per_episode=300,
        verbose=True,
        save_interval=50
    )
    
    # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
    model_save_path = "/data/workspace/wass/models/wass_optimized_models.pth"
    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
    
    trainer.save_model(model_save_path)
    
    print(f"\nâœ… æ¨¡å‹è®­ç»ƒå®Œæˆå¹¶ä¿å­˜åˆ°: {model_save_path}")
    
    # ä¿å­˜è®­ç»ƒæŠ¥å‘Š
    training_report = {
        'hyperparameters': config,
        'training_metrics': training_metrics,
        'model_path': model_save_path,
        'training_completed': True
    }
    
    report_path = "/data/workspace/wass/results/local_hyperparameter_tuning/optimized_training_report.json"
    with open(report_path, 'w') as f:
        json.dump(training_report, f, indent=2, default=str)
    
    print(f"ğŸ“Š è®­ç»ƒæŠ¥å‘Šä¿å­˜åˆ°: {report_path}")
    
    return training_metrics


def main():
    """ä¸»å‡½æ•°"""
    try:
        # è®­ç»ƒä¼˜åŒ–æ¨¡å‹
        metrics = train_optimized_model()
        
        print("\nğŸ‰ ä¼˜åŒ–è®­ç»ƒå®Œæˆ!")
        print("ğŸ’¡ ä¸‹ä¸€æ­¥: è¿è¡Œå®Œæ•´å®éªŒéªŒè¯è°ƒä¼˜æ•ˆæœ")
        print("   python experiments/wrench_real_experiment.py")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
