#!/usr/bin/env python3
"""
å®Œå…¨é‡å†™çš„æ€§èƒ½é¢„æµ‹å™¨è®­ç»ƒè„šæœ¬
ç¡®ä¿è®­ç»ƒå’Œé¢„æµ‹æ—¶ä½¿ç”¨å®Œå…¨ç›¸åŒçš„ç‰¹å¾ç”Ÿæˆé€»è¾‘
"""

import sys
import os
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
from typing import List, Dict, Any
import pickle

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)  # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
sys.path.insert(0, os.path.join(project_root, 'src'))  # æ·»åŠ srcç›®å½•

try:
    from ai_schedulers import WASSRAGScheduler, SchedulingState
    from interfaces import PerformancePredictor
    print("âœ… Successfully imported all required modules")
except ImportError as e:
    print(f"âŒ Import error: {e}")
    sys.exit(1)

def create_consistent_training_data(num_samples: int = 5000) -> List[Dict[str, Any]]:
    """
    ç”Ÿæˆä¸ai_schedulers.pyå®Œå…¨ä¸€è‡´çš„è®­ç»ƒæ•°æ®
    """
    print(f"ğŸ”§ Generating {num_samples} training samples with consistent features...")

    # åˆ›å»ºä¸´æ—¶è°ƒåº¦å™¨å®ä¾‹æ¥å¤ç”¨ç‰¹å¾ç¼–ç é€»è¾‘
    temp_scheduler = WASSRAGScheduler()
    training_data = []
    makespan_values = []

    for i in range(num_samples):
        # 1. åˆ›å»ºéšæœºè°ƒåº¦åœºæ™¯
        num_nodes = np.random.randint(2, 11)  # 2-10ä¸ªèŠ‚ç‚¹
        
        nodes = {f"node_{j}": {
            "cpu_capacity": round(np.random.uniform(2.0, 8.0), 2),
            "memory_capacity": round(np.random.uniform(8.0, 64.0), 2), 
            "current_load": round(np.random.uniform(0.1, 0.9), 2),
        } for j in range(num_nodes)}

        # ä»»åŠ¡ä¿¡æ¯
        task_info = {
            "id": f"task_{i}",
            "flops": float(np.random.uniform(0.5e9, 15e9)),  # 0.5-15 GFlops
            "memory": round(np.random.uniform(1.0, 16.0), 2),
            "dependencies": []
        }

        # åˆ›å»ºè°ƒåº¦çŠ¶æ€
        state = SchedulingState(
            workflow_graph={"tasks": [task_info], "task_requirements": {f"task_{i}": task_info}},
            cluster_state={"nodes": nodes},
            pending_tasks=[f"task_{i}"],
            current_task=f"task_{i}",
            available_nodes=list(nodes.keys()),
            timestamp=0.0
        )

        # 2. ä½¿ç”¨è°ƒåº¦å™¨çš„æ–¹æ³•ç”ŸæˆçŠ¶æ€åµŒå…¥
        state_embedding = temp_scheduler._extract_simple_features_fallback(state)
        
        # 3. ä¸ºæ¯ä¸ªèŠ‚ç‚¹ç”Ÿæˆè®­ç»ƒæ ·æœ¬
        for node_name, node_details in nodes.items():
            # å…³é”®ï¼šä½¿ç”¨ä¸é¢„æµ‹æ—¶å®Œå…¨ç›¸åŒçš„_encode_actionå‡½æ•°
            action_embedding = temp_scheduler._encode_action(node_name, state)
            
            # æ¨¡æ‹Ÿä¸Šä¸‹æ–‡åµŒå…¥
            context_embedding = torch.randn(32, device=temp_scheduler.device)
            
            # æ‹¼æ¥96ç»´ç‰¹å¾
            combined_features = torch.cat([
                state_embedding,
                action_embedding,
                context_embedding
            ]).cpu().numpy()
            
            # 4. è®¡ç®—çœŸå®æ‰§è¡Œæ—¶é—´æ ‡ç­¾
            task_cpu_gflops = task_info["flops"] / 1e9
            node_cpu_cap = node_details["cpu_capacity"]
            node_load = node_details["current_load"]
            
            available_cpu = node_cpu_cap * (1.0 - node_load)
            
            # åŸºç¡€æ‰§è¡Œæ—¶é—´
            base_time = task_cpu_gflops / max(available_cpu, 0.1)
            
            # æ·»åŠ å„ç§ç°å®å› ç´ 
            mem_penalty = max(0, task_info["memory"] - node_details["memory_capacity"]) * 0.5
            load_penalty = node_load * 2.0
            random_noise = np.random.uniform(-0.5, 0.5)
            
            execution_time = base_time + mem_penalty + load_penalty + random_noise
            execution_time = max(1.0, min(180.0, execution_time))  # çº¦æŸèŒƒå›´
            
            makespan_values.append(execution_time)
            
            training_data.append({
                "features": combined_features.tolist(),
                "makespan": execution_time
            })
    
    # æ‰“å°ç»Ÿè®¡
    makespan_array = np.array(makespan_values)
    print(f"ğŸ“Š Execution time distribution:")
    print(f"   Mean: {np.mean(makespan_array):.2f}s")
    print(f"   Std:  {np.std(makespan_array):.2f}s")
    print(f"   Range: [{np.min(makespan_array):.2f}, {np.max(makespan_array):.2f}]s")
    
    return training_data

def train_consistent_model(training_data: List[Dict[str, Any]], epochs: int = 200):
    """
    è®­ç»ƒæ€§èƒ½é¢„æµ‹å™¨
    """
    print(f"ğŸš€ Training model with {len(training_data)} samples...")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"   Using device: {device}")
    
    # å‡†å¤‡æ•°æ®
    X = np.array([sample["features"] for sample in training_data])
    y = np.array([sample["makespan"] for sample in training_data])
    
    # å½’ä¸€åŒ–æ ‡ç­¾
    y_mean, y_std = np.mean(y), np.std(y)
    y_normalized = (y - y_mean) / (y_std + 1e-8)
    
    print(f"ğŸ“ˆ Training statistics:")
    print(f"   Original y: mean={y_mean:.2f}, std={y_std:.2f}")
    print(f"   Normalized y range: [{np.min(y_normalized):.3f}, {np.max(y_normalized):.3f}]")
    
    # è½¬æ¢ä¸ºPyTorch
    X_tensor = torch.FloatTensor(X).to(device)
    y_tensor = torch.FloatTensor(y_normalized).to(device)
    
    dataset = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset, batch_size=64, shuffle=True)
    
    # æ¨¡å‹å’Œä¼˜åŒ–å™¨
    model = PerformancePredictor(input_dim=96, hidden_dim=128).to(device)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=20, factor=0.5)
    
    # è®­ç»ƒå¾ªç¯
    best_loss = float('inf')
    patience_counter = 0
    
    for epoch in range(epochs):
        model.train()
        total_loss = 0.0
        
        for batch_X, batch_y in dataloader:
            optimizer.zero_grad()
            predictions = model(batch_X).squeeze()
            loss = criterion(predictions, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        avg_loss = total_loss / len(dataloader)
        scheduler.step(avg_loss)
        
        # æ—©åœå’Œæ—¥å¿—
        if avg_loss < best_loss:
            best_loss = avg_loss
            patience_counter = 0
        else:
            patience_counter += 1
        
        if epoch % 20 == 0:
            print(f"   Epoch {epoch:3d}: loss={avg_loss:.6f}, best={best_loss:.6f}")
        
        if patience_counter >= 50:
            print(f"   Early stopping at epoch {epoch}")
            break
    
    print(f"âœ… Training completed. Final loss: {best_loss:.6f}")
    
    # ä¿å­˜æ¨¡å‹å’Œå½’ä¸€åŒ–å‚æ•°
    model_path = os.path.join(project_root, "models", "wass_models.pth")
    os.makedirs(os.path.dirname(model_path), exist_ok=True)
    
    # åŠ è½½å·²æœ‰æ¨¡å‹ç»„ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    checkpoint = {}
    if os.path.exists(model_path):
        try:
            checkpoint = torch.load(model_path, map_location=device)
            print("ğŸ“¦ Loaded existing model components")
        except:
            print("ğŸ“¦ Creating new model checkpoint")
    
    # ä¿å­˜æ‰€æœ‰ç»„ä»¶
    checkpoint.update({
        'performance_predictor_state_dict': model.state_dict(),
        'y_mean': y_mean,
        'y_std': y_std,
        'training_samples': len(training_data),
        'final_loss': best_loss
    })
    
    torch.save(checkpoint, model_path)
    print(f"ğŸ’¾ Model saved to {model_path}")
    print(f"ğŸ“Š Normalization: mean={y_mean:.2f}, std={y_std:.2f}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”„ Retraining Performance Predictor with Consistent Features")
    print("=" * 60)
    
    # 1. ç”Ÿæˆä¸€è‡´çš„è®­ç»ƒæ•°æ®
    training_data = create_consistent_training_data(num_samples=5000)
    
    # 2. è®­ç»ƒæ¨¡å‹
    train_consistent_model(training_data, epochs=200)
    
    print("\nâœ… Retraining completed successfully!")
    print("ğŸ§ª Run 'python test_predictions.py' to validate improvements")

if __name__ == "__main__":
    main()
