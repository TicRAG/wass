#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ”¹è¿›çš„WASS-DRLè®­ç»ƒå™¨ï¼Œä»WASS-Heuristicå’ŒHEFTè°ƒåº¦å™¨ä¸­å­¦ä¹ 
ä½¿ç”¨çœŸå®WRENCHç¯å¢ƒè¿›è¡Œè®­ç»ƒï¼Œè€Œä¸æ˜¯æ¨¡æ‹Ÿç¯å¢ƒ
"""

import os
import sys
import json
import time
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque, namedtuple
from pathlib import Path
from typing import Dict, List, Tuple, Any
import yaml

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

from src.drl.reward import compute_step_reward, compute_final_reward, StepContext, EpisodeStats
from src.reward_fix import RewardFix

# ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
Path('results').mkdir(exist_ok=True)

class TeacherGuidedDQN(nn.Module):
    """æ•™å¸ˆå¼•å¯¼çš„DQNç½‘ç»œ"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dims: List[int] = None):
        super().__init__()
        
        if hidden_dims is None:
            hidden_dims = [256, 128, 64]
        
        layers = []
        current_dim = state_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(current_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.1)
            ])
            current_dim = hidden_dim
        
        layers.append(nn.Linear(current_dim, action_dim))
        
        self.network = nn.Sequential(*layers)
        
        # ä½¿ç”¨Xavieråˆå§‹åŒ–
        for layer in self.network:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.zeros_(layer.bias)
    
    def forward(self, x):
        return self.network(x)

class TeacherGuidedDQNAgent:
    """æ•™å¸ˆå¼•å¯¼çš„DQNæ™ºèƒ½ä½“"""
    
    def __init__(self,
                 state_dim: int,
                 action_dim: int,
                 learning_rate: float = 1e-3,
                 epsilon_start: float = 1.0,
                 epsilon_end: float = 0.05,
                 epsilon_decay: float = 0.995,
                 gamma: float = 0.95,
                 memory_size: int = 10000,
                 batch_size: int = 64,
                 target_update_freq: int = 100,
                 device: str = None):
        
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.batch_size = batch_size
        self.target_update_freq = target_update_freq
        
        # ç½‘ç»œ
        self.q_network = TeacherGuidedDQN(state_dim, action_dim).to(self.device)
        self.target_network = TeacherGuidedDQN(state_dim, action_dim).to(self.device)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)
        
        # ç»éªŒå›æ”¾
        self.memory = deque(maxlen=memory_size)
        self.experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])
        
        # æ¢ç´¢ç­–ç•¥
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
        
        # è®­ç»ƒç»Ÿè®¡
        self.training_step = 0
        self.update_target()
        
        # æ•™å¸ˆæŒ‡å¯¼
        self.teacher_actions = []
        self.teacher_confidence = 0.8  # æ•™å¸ˆæŒ‡å¯¼çš„ç½®ä¿¡åº¦
    
    def update_target(self):
        """æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        self.target_network.load_state_dict(self.q_network.state_dict())
    
    def remember(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒ"""
        experience = self.experience(state, action, reward, next_state, done)
        self.memory.append(experience)
    
    def act(self, state, training=True, teacher_action=None):
        """é€‰æ‹©åŠ¨ä½œï¼Œæ”¯æŒæ•™å¸ˆæŒ‡å¯¼"""
        if training and teacher_action is not None and random.random() < self.teacher_confidence:
            # æ•™å¸ˆæŒ‡å¯¼æ¨¡å¼
            return teacher_action
        
        if training and random.random() < self.epsilon:
            # æ¢ç´¢æ¨¡å¼
            return random.randint(0, self.action_dim - 1)
        
        # åˆ©ç”¨æ¨¡å¼
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            q_values = self.q_network(state_tensor)
            return q_values.argmax().item()
    
    def replay(self):
        """ç»éªŒå›æ”¾è®­ç»ƒ"""
        if len(self.memory) < self.batch_size:
            return None
        
        # é‡‡æ ·ç»éªŒ
        batch = random.sample(self.memory, self.batch_size)
        states = torch.FloatTensor([e.state for e in batch]).to(self.device)
        actions = torch.LongTensor([e.action for e in batch]).to(self.device)
        rewards = torch.FloatTensor([e.reward for e in batch]).to(self.device)
        next_states = torch.FloatTensor([e.next_state for e in batch]).to(self.device)
        dones = torch.BoolTensor([e.done for e in batch]).to(self.device)
        
        # è®¡ç®—Qå€¼
        current_q = self.q_network(states).gather(1, actions.unsqueeze(1))
        next_q = self.target_network(next_states).max(1)[0].detach()
        target_q = rewards + (self.gamma * next_q * ~dones)
        
        # è®¡ç®—æŸå¤±
        loss = nn.MSELoss()(current_q.squeeze(), target_q)
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)
        
        self.optimizer.step()
        
        # æ›´æ–°æ¢ç´¢ç‡
        if self.epsilon > self.epsilon_end:
            self.epsilon *= self.epsilon_decay
        
        # å®šæœŸæ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.training_step += 1
        if self.training_step % self.target_update_freq == 0:
            self.update_target()
        
        return loss.item()

class WRENCHBasedDRLTrainer:
    """åŸºäºWRENCHç¯å¢ƒçš„DRLè®­ç»ƒå™¨ï¼Œä»ä¼˜ç§€æ•™å¸ˆè°ƒåº¦å™¨ä¸­å­¦ä¹ """
    
    def __init__(self, config_path: str):
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.reward_fix = RewardFix()
        self.agent = None
        self.training_history = []
        self.best_makespan = float('inf')
        
        # é…ç½®å‚æ•°
        self.drl_cfg = self.config.get('drl', {})
        self.checkpoint_cfg = self.config.get('checkpoint', {})
        self.logging_cfg = self.config.get('logging', {})
        
        # åˆ›å»ºç›®å½•
        Path(self.checkpoint_cfg.get('dir', 'models/checkpoints/')).mkdir(parents=True, exist_ok=True)
        Path(self.logging_cfg.get('metrics_file', 'results/training_metrics.jsonl')).parent.mkdir(parents=True, exist_ok=True)
    
    def create_wrench_environment(self):
        """åˆ›å»ºWRENCHè®­ç»ƒç¯å¢ƒ"""
        try:
            import wrench
            
            # åˆ›å»ºä»¿çœŸ
            simulation = wrench.Simulation()
            
            # åˆ›å»ºå¹³å°
            platform = simulation.create_platform([
                wrench.Host("ComputeHost1", "100Gf", ["100Gf", "100GB"]),
                wrench.Host("ComputeHost2", "150Gf", ["150Gf", "150GB"]),
                wrench.Host("ComputeHost3", "200Gf", ["200Gf", "200GB"]),
                wrench.Host("ComputeHost4", "250Gf", ["250Gf", "250GB"])
            ])
            
            # åˆ›å»ºè®¡ç®—æœåŠ¡
            compute_service = simulation.create_bare_metal_compute_service(
                "ComputeService",
                platform.get_hosts(),
                {}
            )
            
            # åˆ›å»ºå·¥ä½œæµ
            workflow = simulation.create_workflow("training_workflow")
            
            # åˆ›å»ºä»»åŠ¡å›¾ï¼ˆæ¨¡æ‹ŸçœŸå®å·¥ä½œæµï¼‰
            tasks = []
            for i in range(20):  # åˆ›å»º20ä¸ªä»»åŠ¡
                task = workflow.add_task(f"task_{i}", random.uniform(1e9, 1e10))
                tasks.append(task)
            
            # æ·»åŠ ä¾èµ–å…³ç³»
            for i in range(1, 20):
                # æ¯ä¸ªä»»åŠ¡ä¾èµ–å‰é¢çš„1-2ä¸ªä»»åŠ¡
                num_deps = min(random.randint(1, 2), i)
                for j in range(max(0, i-num_deps), i):
                    workflow.add_control_dependency(tasks[j], tasks[i])
            
            return simulation, platform, compute_service, workflow, tasks
            
        except Exception as e:
            print(f"åˆ›å»ºWRENCHç¯å¢ƒå¤±è´¥: {e}")
            return None, None, None, None, None
    
    def extract_state_features(self, task, available_nodes, node_capacities, node_loads, workflow):
        """ä»WRENCHç¯å¢ƒä¸­æå–çŠ¶æ€ç‰¹å¾"""
        features = []
        
        # ä»»åŠ¡ç‰¹å¾
        features.extend([
            task.get_flops() / 1e10,  # å½’ä¸€åŒ–è®¡ç®—é‡
            len(task.get_parents()),   # çˆ¶ä»»åŠ¡æ•°é‡
            len(task.get_children()),  # å­ä»»åŠ¡æ•°é‡
            1.0 if len(task.get_parents()) == 0 else 0.0,  # æ˜¯å¦æ˜¯å…¥å£ä»»åŠ¡
            1.0 if len(task.get_children()) == 0 else 0.0,  # æ˜¯å¦æ˜¯å‡ºå£ä»»åŠ¡
        ])
        
        # èŠ‚ç‚¹ç‰¹å¾
        for node in available_nodes:
            capacity = node_capacities.get(node, 1.0)
            load = node_loads.get(node, 0.0)
            features.extend([
                capacity / 250.0,  # å½’ä¸€åŒ–å®¹é‡
                load / 100.0,     # å½’ä¸€åŒ–è´Ÿè½½
            ])
        
        # ç¯å¢ƒç‰¹å¾
        total_tasks = len(workflow.get_tasks())
        completed_tasks = sum(1 for t in workflow.get_tasks() if t.get_state() == wrench.TaskState.COMPLETED)
        features.extend([
            completed_tasks / total_tasks,  # å·¥ä½œæµè¿›åº¦
            len(available_nodes) / 4.0,    # å¯ç”¨èŠ‚ç‚¹æ¯”ä¾‹
        ])
        
        return np.array(features, dtype=np.float32)
    
    def get_teacher_action(self, task, available_nodes, node_capacities, node_loads, teacher_type="HEFT"):
        """è·å–æ•™å¸ˆè°ƒåº¦å™¨çš„åŠ¨ä½œ"""
        if teacher_type == "HEFT":
            # HEFTç­–ç•¥ï¼šé€‰æ‹©èƒ½æœ€æ—©å®Œæˆä»»åŠ¡çš„èŠ‚ç‚¹
            best_node = None
            best_finish_time = float('inf')
            
            for node in available_nodes:
                capacity = node_capacities.get(node, 1.0)
                load = node_loads.get(node, 0.0)
                exec_time = task.get_flops() / (capacity * 1e9)
                finish_time = load + exec_time
                
                if finish_time < best_finish_time:
                    best_finish_time = finish_time
                    best_node = node
            
            return available_nodes.index(best_node) if best_node else 0
        
        elif teacher_type == "WASS-Heuristic":
            # WASS-Heuristicç­–ç•¥ï¼šè€ƒè™‘æ•°æ®å±€éƒ¨æ€§
            best_node = None
            best_score = float('inf')
            
            for node in available_nodes:
                # è®¡ç®—EFT
                capacity = node_capacities.get(node, 1.0)
                load = node_loads.get(node, 0.0)
                exec_time = task.get_flops() / (capacity * 1e9)
                eft = load + exec_time
                
                # ç®€åŒ–çš„DRTè®¡ç®—ï¼ˆæ¨¡æ‹Ÿæ•°æ®å±€éƒ¨æ€§ï¼‰
                drt = 0.0
                for parent in task.get_parents():
                    # å‡è®¾çˆ¶ä»»åŠ¡å¯èƒ½åœ¨ä»»ä½•èŠ‚ç‚¹ä¸Šæ‰§è¡Œ
                    if random.random() > 0.5:  # 50%æ¦‚ç‡éœ€è¦æ•°æ®ä¼ è¾“
                        file_size = task.get_flops() * 0.1  # å‡è®¾æ•°æ®å¤§å°
                        network_bandwidth = 1e9  # 1GB/s
                        drt += file_size / network_bandwidth
                
                # WASSè¯„åˆ†
                w = 0.5  # æ•°æ®å±€éƒ¨æ€§æƒé‡
                score = (1 - w) * eft + w * drt
                
                if score < best_score:
                    best_score = score
                    best_node = node
            
            return available_nodes.index(best_node) if best_node else 0
        
        else:
            # é»˜è®¤éšæœºé€‰æ‹©
            return random.randint(0, len(available_nodes) - 1)
    
    def train_episode(self, teacher_type="HEFT"):
        """è®­ç»ƒä¸€ä¸ªepisodeï¼Œä½¿ç”¨æ•™å¸ˆæŒ‡å¯¼"""
        simulation, platform, compute_service, workflow, tasks = self.create_wrench_environment()
        if simulation is None:
            return None
        
        # åˆå§‹åŒ–èŠ‚ç‚¹çŠ¶æ€
        available_nodes = ["ComputeHost1", "ComputeHost2", "ComputeHost3", "ComputeHost4"]
        node_capacities = {
            "ComputeHost1": 100.0,
            "ComputeHost2": 150.0,
            "ComputeHost3": 200.0,
            "ComputeHost4": 250.0
        }
        node_loads = {node: 0.0 for node in available_nodes}
        
        # è®­ç»ƒç»Ÿè®¡
        step_rewards = []
        total_makespan = 0.0
        step_count = 0
        
        # è°ƒè¯•æ—¥å¿—
        reward_debug_path = self.logging_cfg.get('reward_debug', 'results/reward_debug.log')
        debug_file = None
        try:
            debug_file = open(reward_debug_path, 'a')
        except Exception:
            debug_file = None
        
        # æ¨¡æ‹Ÿè°ƒåº¦è¿‡ç¨‹
        ready_tasks = [t for t in tasks if len(t.get_parents()) == 0]
        completed_tasks = set()
        
        while ready_tasks and step_count < 100:  # é™åˆ¶æœ€å¤§æ­¥æ•°
            # é€‰æ‹©å½“å‰ä»»åŠ¡ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
            current_task = ready_tasks[0]
            
            # æå–çŠ¶æ€ç‰¹å¾
            state = self.extract_state_features(
                current_task, available_nodes, node_capacities, node_loads, workflow
            )
            
            # è·å–æ•™å¸ˆåŠ¨ä½œ
            teacher_action = self.get_teacher_action(
                current_task, available_nodes, node_capacities, node_loads, teacher_type
            )
            
            # æ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œ
            action = self.agent.act(state, training=True, teacher_action=teacher_action)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            chosen_node = available_nodes[action]
            
            # è®¡ç®—æ‰§è¡Œæ—¶é—´
            capacity = node_capacities[chosen_node]
            exec_time = current_task.get_flops() / (capacity * 1e9)
            
            # æ›´æ–°èŠ‚ç‚¹è´Ÿè½½
            node_loads[chosen_node] += exec_time
            
            # è®¡ç®—å¥–åŠ±
            try:
                # æ„é€ StepContextç”¨äºè®¡ç®—å¥–åŠ±
                ctx = StepContext(
                    completed_critical_path_tasks=len(completed_tasks),
                    total_critical_path_tasks=len(tasks),
                    node_busy_times=node_loads,
                    ready_task_count=len(ready_tasks) - 1,
                    total_nodes=len(available_nodes),
                    avg_queue_wait=np.mean(list(node_loads.values())),
                    queue_wait_baseline=0.0
                )
                step_reward, _metrics = compute_step_reward(ctx, debug_writer=debug_file)
            except Exception:
                step_reward = 0.0
            
            step_rewards.append(step_reward)
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            completed_tasks.add(current_task)
            ready_tasks.remove(current_task)
            
            # æ›´æ–°å°±ç»ªä»»åŠ¡åˆ—è¡¨
            for child in current_task.get_children():
                if all(parent in completed_tasks for parent in child.get_parents()):
                    if child not in ready_tasks:
                        ready_tasks.append(child)
            
            # æ›´æ–°æ—¶é—´
            total_makespan = max(node_loads.values())
            step_count += 1
            
            # æå–ä¸‹ä¸€çŠ¶æ€
            if ready_tasks:
                next_task = ready_tasks[0]
                next_state = self.extract_state_features(
                    next_task, available_nodes, node_capacities, node_loads, workflow
                )
            else:
                next_state = np.zeros_like(state)
            
            # å­˜å‚¨ç»éªŒ
            done = len(ready_tasks) == 0
            self.agent.remember(state, action, step_reward, next_state, done)
            
            # è®­ç»ƒæ™ºèƒ½ä½“
            if step_count % 4 == 0:
                loss = self.agent.replay()
            
            if done:
                break
        
        # è®¡ç®—æœ€ç»ˆå¥–åŠ±
        final_reward = compute_final_reward(EpisodeStats(makespan=total_makespan))
        
        # è®¡ç®—å¹³å‡å¥–åŠ±
        avg_step_reward = np.mean(step_rewards) if step_rewards else 0.0
        total_reward = avg_step_reward + final_reward
        
        # è®°å½•è°ƒè¯•ä¿¡æ¯
        if debug_file:
            try:
                debug_file.write(f"FINAL\tmakespan={total_makespan:.4f}\tavg_step_reward={avg_step_reward:.4f}\tfinal_reward={final_reward:.4f}\ttotal_reward={total_reward:.4f}\tteacher={teacher_type}\n")
                debug_file.close()
            except Exception:
                pass
        
        return {
            'total_reward': total_reward,
            'avg_step_reward': avg_step_reward,
            'final_reward': final_reward,
            'makespan': total_makespan,
            'step_count': step_count,
            'epsilon': self.agent.epsilon,
            'teacher_type': teacher_type
        }
    
    def train(self, episodes: int = 1000):
        """è®­ç»ƒDRLæ™ºèƒ½ä½“ï¼Œä½¿ç”¨æ•™å¸ˆæŒ‡å¯¼"""
        print(f"ğŸš€ å¼€å§‹åŸºäºWRENCHçš„æ•™å¸ˆå¼•å¯¼DRLè®­ç»ƒ: {episodes} episodes")
        
        # åˆå§‹åŒ–æ™ºèƒ½ä½“
        state_dim = 5 + 4 * 2 + 2  # ä»»åŠ¡ç‰¹å¾ + èŠ‚ç‚¹ç‰¹å¾ + ç¯å¢ƒç‰¹å¾
        action_dim = 4  # 4ä¸ªèŠ‚ç‚¹
        
        self.agent = TeacherGuidedDQNAgent(
            state_dim=state_dim,
            action_dim=action_dim,
            **self.config.get('drl', {})
        )
        
        # è®­ç»ƒå¾ªç¯
        best_makespan = float('inf')
        recent_rewards = deque(maxlen=100)
        
        log_interval = self.drl_cfg.get('log_interval', 50)
        eval_interval = self.drl_cfg.get('eval_interval', 100)
        checkpoint_interval = self.drl_cfg.get('checkpoint_interval', 100)
        
        for episode in range(episodes):
            # åŠ¨æ€é€‰æ‹©æ•™å¸ˆç±»å‹
            if episode < episodes // 2:
                teacher_type = "HEFT"
            else:
                teacher_type = "WASS-Heuristic"
            
            # è®­ç»ƒä¸€ä¸ªepisode
            result = self.train_episode(teacher_type)
            if result is None:
                continue
            
            # è®°å½•ç»“æœ
            self.training_history.append(result)
            recent_rewards.append(result['total_reward'])
            
            # æ›´æ–°æœ€ä½³makespan
            if result['makespan'] < best_makespan:
                best_makespan = result['makespan']
                self.save_model('best_model.pth')
            
            # å®šæœŸæ‰“å°æ—¥å¿—
            if (episode + 1) % log_interval == 0:
                avg_reward = np.mean(recent_rewards)
                print(f"Episode {episode + 1}: å¹³å‡å¥–åŠ±={avg_reward:.3f}, Makespan={result['makespan']:.2f}, Îµ={result['epsilon']:.3f}, æ•™å¸ˆ={teacher_type}")
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (episode + 1) % checkpoint_interval == 0:
                self.save_model(f'checkpoint_episode_{episode + 1}.pth')
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_model('wass_drl_teacher_guided.pth')
        
        print(f"âœ… DRLè®­ç»ƒå®Œæˆ!")
        print(f"   æœ€ä½³Makespan: {best_makespan:.2f}s")
        print(f"   æœ€ç»ˆEpsilon: {self.agent.epsilon:.3f}")
        
        return {
            'best_makespan': best_makespan,
            'training_history': self.training_history
        }
    
    def save_model(self, filename):
        """ä¿å­˜æ¨¡å‹"""
        model_path = Path(self.checkpoint_cfg.get('dir', 'models/checkpoints/')) / filename
        torch.save({
            'q_network_state_dict': self.agent.q_network.state_dict(),
            'target_network_state_dict': self.agent.target_network.state_dict(),
            'optimizer_state_dict': self.agent.optimizer.state_dict(),
            'training_step': self.agent.training_step,
            'epsilon': self.agent.epsilon,
            'training_history': self.training_history
        }, model_path)
        print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜: {model_path}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='WASS-DRLæ•™å¸ˆå¼•å¯¼è®­ç»ƒ')
    parser.add_argument('--config', type=str, default='configs/experiment.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--episodes', type=int, default=1000, help='è®­ç»ƒè½®æ•°')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = WRENCHBasedDRLTrainer(args.config)
    
    # å¼€å§‹è®­ç»ƒ
    results = trainer.train(args.episodes)
    
    print("ğŸ‰ è®­ç»ƒå®Œæˆ! æ¨¡å‹å’Œç»“æœå·²ä¿å­˜åˆ° models")

if __name__ == '__main__':
    main()