# scripts/train_predictor.py
import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import numpy as np
import pandas as pd
from pathlib import Path
import joblib
import json
from sklearn.preprocessing import StandardScaler

# --- Add project root to sys.path ---
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)
# ------------------------------------

# --- æ–°çš„ã€æ›´ç®€å•çš„é¢„æµ‹å™¨æ¨¡å‹ ---
class SimplePredictor(nn.Module):
    """ä¸€ä¸ªç®€å•çš„MLPï¼Œè¾“å…¥æ˜¯æ‰‹å·¥è®¾è®¡çš„ç»Ÿè®¡ç‰¹å¾ã€‚"""
    def __init__(self, input_dim: int):
        super(SimplePredictor, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )
    def forward(self, x):
        return self.model(x)

def extract_statistical_features(workflow_file: str) -> list:
    """ä»å·¥ä½œæµJSONæ–‡ä»¶ä¸­æå–ä¸€ç»„ç»Ÿè®¡ç‰¹å¾ã€‚"""
    with open(workflow_file, 'r') as f:
        data = json.load(f)
    
    tasks = data['workflow']['tasks']
    if not tasks:
        return [0.0] * 5 # è¿”å›é»˜è®¤å€¼

    num_tasks = len(tasks)
    total_flops = sum(float(t.get('flops', 0.0)) for t in tasks)
    total_memory = sum(float(t.get('memory', 0.0)) for t in tasks)
    avg_flops = total_flops / num_tasks if num_tasks > 0 else 0.0
    
    # ä¼°ç®—å·¥ä½œæµæ·±åº¦ï¼ˆå…³é”®è·¯å¾„é•¿åº¦ï¼‰
    task_depth = {}
    for task in tasks:
        if not task.get('dependencies'):
            task_depth[task['id']] = 1
        else:
            max_parent_depth = 0
            for parent_id in task.get('dependencies'):
                if parent_id in task_depth:
                    max_parent_depth = max(max_parent_depth, task_depth[parent_id])
            task_depth[task['id']] = max_parent_depth + 1
            
    critical_path_length = max(task_depth.values()) if task_depth else 0

    return [num_tasks, total_flops, total_memory, avg_flops, critical_path_length]

# --- é…ç½® ---
KB_METADATA_PATH = "data/knowledge_base/workflow_metadata.csv"
WORKFLOW_DIR = "data/workflows"
MODEL_SAVE_DIR = "models/saved_models"
PREDICTOR_MODEL_PATH = os.path.join(MODEL_SAVE_DIR, "performance_predictor.pth")
FEATURE_SCALER_PATH = os.path.join(MODEL_SAVE_DIR, "feature_scaler.joblib")
MAKESPAN_SCALER_PATH = os.path.join(MODEL_SAVE_DIR, "makespan_scaler.joblib")

EPOCHS = 250
BATCH_SIZE = 8
LEARNING_RATE = 0.001

def main():
    print("ğŸš€ [Phase 2.2] Starting Performance Predictor Training (Statistical Features Version)...")
    
    Path(MODEL_SAVE_DIR).mkdir(parents=True, exist_ok=True)

    # 1. åŠ è½½å…ƒæ•°æ®
    print("\n[Step 1/5] Loading Knowledge Base metadata...")
    metadata = pd.read_csv(KB_METADATA_PATH)
    print(f"âœ… Loaded metadata for {len(metadata)} workflows.")

    # 2. æå–ç»Ÿè®¡ç‰¹å¾
    print("\n[Step 2/5] Extracting statistical features from JSON files...")
    features_list = []
    for filename in metadata['workflow_file']:
        # æ„é€ å®Œæ•´è·¯å¾„
        filepath = os.path.join(WORKFLOW_DIR, filename)
        if os.path.exists(filepath):
            features_list.append(extract_statistical_features(filepath))
        else:
            print(f"  [Warning] File not found, skipping: {filepath}")

    if not features_list:
        print("âŒ No features were extracted. Aborting.")
        return

    # 3. å½’ä¸€åŒ–è¾“å…¥å’Œè¾“å‡º
    print("\n[Step 3/5] Normalizing input features and output makespans...")
    feature_scaler = StandardScaler()
    makespan_scaler = StandardScaler()

    X_scaled = feature_scaler.fit_transform(features_list)
    y_scaled = makespan_scaler.fit_transform(metadata['makespan'].values.reshape(-1, 1))
    
    X_train = torch.tensor(X_scaled, dtype=torch.float32)
    y_train = torch.tensor(y_scaled, dtype=torch.float32)
    print("âœ… Features and targets normalized.")
    
    # 4. åˆå§‹åŒ–æ¨¡å‹
    input_dim = X_train.shape[1]
    model = SimplePredictor(input_dim=input_dim)
    loss_function = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    print(f"âœ… Model initialized with {input_dim} input features.")
    
    # 5. è®­ç»ƒ
    print("\n[Step 4/5] Starting training...")
    dataset = TensorDataset(X_train, y_train)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
    
    model.train()
    for epoch in range(EPOCHS):
        for batch_X, batch_y in dataloader:
            optimizer.zero_grad()
            predictions = model(batch_X)
            loss = loss_function(predictions, batch_y)
            loss.backward()
            optimizer.step()
            
        if (epoch + 1) % 25 == 0:
            # æ‰“å°ä¸€äº›æ ·æœ¬çš„é¢„æµ‹ç»“æœæ¥ç›‘æ§è®­ç»ƒè¿‡ç¨‹
            with torch.no_grad():
                sample_preds_scaled = model(X_train[:5])
                sample_preds_real = makespan_scaler.inverse_transform(sample_preds_scaled.numpy())
                sample_targets_real = makespan_scaler.inverse_transform(y_train[:5].numpy())
                errors = np.mean(np.abs(sample_preds_real - sample_targets_real) / sample_targets_real) * 100
                print(f"  Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item():.6f}, Avg Prediction Error (on 5 samples): {errors:.2f}%")

    print("âœ… Training finished.")

    # 6. ä¿å­˜æ‰€æœ‰ä¸œè¥¿
    print("\n[Step 5/5] Saving model and scalers...")
    torch.save(model.state_dict(), PREDICTOR_MODEL_PATH)
    joblib.dump(feature_scaler, FEATURE_SCALER_PATH)
    joblib.dump(makespan_scaler, MAKESPAN_SCALER_PATH)

    print(f"ğŸ’¾ Predictor model saved to: {PREDICTOR_MODEL_PATH}")
    print(f"ğŸ’¾ Feature scaler saved to: {FEATURE_SCALER_PATH}")
    print(f"ğŸ’¾ Makespan scaler saved to: {MAKESPAN_SCALER_PATH}")
    print("\nğŸ‰ [Phase 2.2] Completed Successfully! ğŸ‰")

if __name__ == "__main__":
    main()