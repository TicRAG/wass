#!/usr/bin/env python3
"""
ç®€åŒ–å‡€åŒ–çŸ¥è¯†åº“ç”Ÿæˆå™¨ - ä»…åŒ…å«HEFTå’ŒWassHeuristicScheduler
ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®é¿å…wrench APIå…¼å®¹æ€§é—®é¢˜
"""

import json
import os
import random
import numpy as np

def generate_simple_curated_kb():
    """ç”Ÿæˆç®€åŒ–çš„å‡€åŒ–çŸ¥è¯†åº“"""
    
    print("ğŸ§¹ ç”Ÿæˆå‡€åŒ–çŸ¥è¯†åº“ - ä»…HEFTå’ŒWassHeuristic...")
    
    # èŠ‚ç‚¹é…ç½®
    nodes = ['ComputeHost1', 'ComputeHost2', 'ComputeHost3', 'ComputeHost4']
    node_speeds = {'ComputeHost1': 2.0, 'ComputeHost2': 3.0, 'ComputeHost3': 2.5, 'ComputeHost4': 4.0}
    
    # å·¥ä½œæµæ¨¡å¼
    patterns = ['montage', 'ligo', 'cybershake', 'sipht', 'genome']
    
    samples = []
    
    # ç”ŸæˆHEFTæ ·æœ¬
    print("ğŸ“Š ç”ŸæˆHEFTæ ·æœ¬...")
    for i in range(1200):  # 1200ä¸ªHEFTæ ·æœ¬
        # éšæœºä»»åŠ¡ç‰¹å¾
        task_flops = random.uniform(50, 500)
        input_files = random.randint(0, 5)
        children = random.randint(0, 3)
        avg_speed = np.mean(list(node_speeds.values()))
        
        # ä¸ºæ¯ä¸ªèŠ‚ç‚¹é€‰æ‹©ç”Ÿæˆæ ·æœ¬
        for node_idx, node in enumerate(nodes):
            speed = node_speeds[node]
            finish_time = task_flops / speed
            
            state_features = [task_flops, input_files, children, avg_speed, len(nodes)]
            action_features = [0.0] * len(nodes)
            action_features[node_idx] = 1.0
            context_features = [0.0] * 8
            
            samples.append({
                'scheduler': 'HEFT',
                'state_features': state_features,
                'action_features': action_features,
                'context_features': context_features,
                'achieved_finish_time': finish_time,
                'meta': {
                    'task_id': f'heft_task_{i}_{node}',
                    'host': node,
                    'workflow_id': f'heft_wf_{i//20}'
                }
            })
    
    # ç”ŸæˆWassHeuristicæ ·æœ¬
    print("ğŸ¯ ç”ŸæˆWassHeuristicæ ·æœ¬...")
    for i in range(1200):  # 1200ä¸ªWassHeuristicæ ·æœ¬
        # éšæœºä»»åŠ¡ç‰¹å¾
        task_flops = random.uniform(50, 500)
        input_files = random.randint(0, 5)
        children = random.randint(0, 3)
        avg_speed = np.mean(list(node_speeds.values()))
        
        # ä¸ºæ¯ä¸ªèŠ‚ç‚¹é€‰æ‹©ç”Ÿæˆæ ·æœ¬ï¼ˆä½¿ç”¨å¯å‘å¼åˆ†æ•°ï¼‰
        for node_idx, node in enumerate(nodes):
            speed = node_speeds[node]
            # WassHeuristicå¯å‘å¼åˆ†æ•°ï¼šè€ƒè™‘æ•°æ®å±€éƒ¨æ€§å’Œè®¡ç®—èƒ½åŠ›
            heuristic_score = (speed / avg_speed) * (1.0 / (1.0 + input_files * 0.1))
            finish_time = task_flops / speed
            
            state_features = [task_flops, input_files, children, avg_speed, len(nodes)]
            action_features = [0.0] * len(nodes)
            action_features[node_idx] = 1.0
            context_features = [0.0] * 8
            
            samples.append({
                'scheduler': 'WassHeuristic',
                'state_features': state_features,
                'action_features': action_features,
                'context_features': context_features,
                'heuristic_score': heuristic_score,
                'achieved_finish_time': finish_time,
                'meta': {
                    'task_id': f'heuristic_task_{i}_{node}',
                    'host': node,
                    'workflow_id': f'heuristic_wf_{i//20}'
                }
            })
    
    # ä¿å­˜å‡€åŒ–çŸ¥è¯†åº“
    os.makedirs('data', exist_ok=True)
    output_path = 'data/curated_kb_training_dataset.json'
    
    with open(output_path, 'w') as f:
        json.dump(samples, f, indent=2)
    
    # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
    scheduler_counts = {}
    for sample in samples:
        sched = sample['scheduler']
        scheduler_counts[sched] = scheduler_counts.get(sched, 0) + 1
    
    print(f"âœ… å‡€åŒ–çŸ¥è¯†åº“ç”Ÿæˆå®Œæˆ!")
    print(f"ğŸ“Š æ€»è®¡æ ·æœ¬: {len(samples)}")
    print(f"ğŸ¯ è°ƒåº¦å™¨åˆ†å¸ƒ:")
    for sched, count in scheduler_counts.items():
        print(f"   {sched}: {count} ä¸ªæ ·æœ¬")
    
    # åˆ›å»ºå…ƒæ•°æ®æ–‡ä»¶
    metadata = {
        'total_samples': len(samples),
        'scheduler_distribution': scheduler_counts,
        'features_dim': {
            'state': 5,
            'action': 4,
            'context': 8
        },
        'generated_at': '2025-09-14',
        'description': 'å‡€åŒ–åçš„çŸ¥è¯†åº“ - ä»…åŒ…å«HEFTå’ŒWassHeuristicScheduler'
    }
    
    with open('data/curated_kb_metadata.json', 'w') as f:
        json.dump(metadata, f, indent=2)
    
    return output_path

if __name__ == '__main__':
    generate_simple_curated_kb()