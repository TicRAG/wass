#!/usr/bin/env python3
"""
WASS-RAG é˜¶æ®µä¸‰ï¼šDRL ä»£ç†è®­ç»ƒè„šæœ¬

è¯¥è„šæœ¬å®ç°äº†è®ºæ–‡ä¸­æè¿°çš„ DRL è®­ç»ƒå¾ªç¯ã€‚å®ƒåŒ…å«ï¼š
1. ä¸€ä¸ªè‡ªå®šä¹‰çš„ Gym ç¯å¢ƒ (WassEnv)ï¼Œå°†æˆ‘ä»¬çš„è°ƒåº¦é—®é¢˜åŒ…è£…èµ·æ¥ã€‚
2. ä¸€ä¸ªå¥–åŠ±å‡½æ•°ï¼Œåˆ©ç”¨é˜¶æ®µäºŒè®­ç»ƒå¥½çš„ PerformancePredictor ä½œä¸ºâ€œæ•™å¸ˆâ€æ¥æä¾›å¥–åŠ±ã€‚
3. ä½¿ç”¨ Stable-Baselines3 åº“ä¸­çš„ PPO ç®—æ³•æ¥è®­ç»ƒ PolicyNetworkã€‚
"""

import sys
import os
from pathlib import Path
import numpy as np
import torch
import gymnasium as gym
from gymnasium import spaces
from stable_baselines3 import PPO
from stable_baselines3.common.env_checker import check_env

# --- é¡¹ç›®è·¯å¾„è®¾ç½® ---
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, str(parent_dir))
sys.path.insert(0, os.path.join(parent_dir, 'src'))

# --- å¯¼å…¥æˆ‘ä»¬è‡ªå·±çš„æ¨¡å— ---
from experiments.real_experiment_framework import WassExperimentRunner
from src.ai_schedulers import WASSRAGScheduler, SchedulingState, PolicyNetwork

class WassEnv(gym.Env):
    """
    ä¸€ä¸ªå°† WASS è°ƒåº¦é—®é¢˜åŒ…è£…ä¸ºä¸ Stable-Baselines3 å…¼å®¹çš„è‡ªå®šä¹‰ Gym ç¯å¢ƒã€‚
    """
    metadata = {"render_modes": []}

    def __init__(self, config_dict):
        super().__init__()
        
        # 1. åˆå§‹åŒ–ä»¿çœŸå™¨å’Œâ€œæ•™å¸ˆâ€æ¨¡å‹
        self.sim_runner = WassExperimentRunner(config_dict)
        self.teacher_model = WASSRAGScheduler(model_path="models/wass_models.pth")
        
        # 2. å®šä¹‰åŠ¨ä½œç©ºé—´å’Œè§‚å¯Ÿç©ºé—´
        # åŠ¨ä½œï¼šé€‰æ‹©ä¸€ä¸ªèŠ‚ç‚¹çš„ç´¢å¼•ã€‚å‡è®¾æœ€å¤šæœ‰32ä¸ªèŠ‚ç‚¹ã€‚
        self.action_space = spaces.Discrete(32) 
        # è§‚å¯Ÿï¼šçŠ¶æ€ç‰¹å¾ + åŠ¨ä½œç‰¹å¾ã€‚å‡è®¾ state=32, action=32 -> 64ç»´
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(64,), dtype=np.float32)
        
        self.current_simulation_config = None

    def reset(self, seed=None, options=None):
        """å¼€å§‹ä¸€è½®æ–°çš„ä»¿çœŸï¼ˆä¸€ä¸ª episodeï¼‰"""
        super().reset(seed=seed)

        # éšæœºé€‰æ‹©ä¸€ä¸ªåœºæ™¯é…ç½®
        task_count = np.random.choice(self.sim_runner.config.workflow_sizes)
        cluster_size = np.random.choice(self.sim_runner.config.cluster_sizes)
        
        # åˆ›å»ºå·¥ä½œæµå’Œé›†ç¾¤
        self.workflow, self.cluster = self.sim_runner._generate_scenario(task_count, cluster_size, seed if seed is not None else int.from_bytes(os.urandom(4), 'little'))
        self.nodes = list(self.cluster.keys())
        
        # é‡ç½®ä»¿çœŸçŠ¶æ€
        self.pending_tasks = {t['id'] for t in self.workflow['tasks']}
        self.task_finish_times = {}
        self.node_available_times = {node: 0.0 for node in self.nodes}
        self.task_placements = {}
        
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªè¦è°ƒåº¦çš„ä»»åŠ¡
        observation, info = self._get_next_observation()
        return observation, info

    def step(self, action):
        """æ‰§è¡Œä¸€ä¸ªåŠ¨ä½œå¹¶æ¨è¿›ç¯å¢ƒ"""
        # 1. è§£æåŠ¨ä½œ
        # æ£€æŸ¥åŠ¨ä½œæ˜¯å¦æœ‰æ•ˆï¼ˆé€‰æ‹©çš„èŠ‚ç‚¹æ˜¯å¦å­˜åœ¨ï¼‰
        if action >= len(self.nodes):
            # æ— æ•ˆåŠ¨ä½œï¼Œç»™äºˆæƒ©ç½šå¹¶ç»“æŸ
            return self._get_next_observation()[0], -100.0, True, False, {"error": "Invalid action"}
        
        chosen_node = self.nodes[action]

        # 2. è®¡ç®—å¥–åŠ±ï¼ˆæ ¸å¿ƒï¼‰
        # ä½¿ç”¨â€œæ•™å¸ˆâ€æ¨¡å‹æ¥é¢„æµ‹è¿™ä¸ªå†³ç­–çš„å¥½å
        action_embedding = self.teacher_model._encode_action(chosen_node, self.current_state)
        predicted_finish_time = self.teacher_model._predict_performance(self.current_state_embedding, action_embedding, {})
        
        # å¥–åŠ±å‡½æ•°ï¼šå®Œæˆæ—¶é—´è¶ŠçŸ­ï¼Œå¥–åŠ±è¶Šé«˜ã€‚æˆ‘ä»¬ä½¿ç”¨ 1/time çš„å½¢å¼ã€‚
        reward = 1.0 / (predicted_finish_time + 1e-6)

        # 3. æ›´æ–°ä»¿çœŸçŠ¶æ€
        task_to_schedule = self.current_task_obj
        est = self.current_state.cluster_state['earliest_start_times'][chosen_node]
        exec_time = task_to_schedule['flops'] / (self.cluster[chosen_node]['cpu_capacity'] * 1e9)
        finish_time = est + exec_time
        
        self.task_finish_times[task_to_schedule['id']] = finish_time
        self.node_available_times[chosen_node] = finish_time
        self.task_placements[task_to_schedule['id']] = chosen_node
        self.pending_tasks.remove(task_to_schedule['id'])

        # 4. è·å–ä¸‹ä¸€ä¸ªè§‚å¯Ÿ
        observation, info = self._get_next_observation()
        
        # 5. æ£€æŸ¥æ˜¯å¦ç»“æŸ
        terminated = len(self.pending_tasks) == 0
        truncated = False # æˆ‘ä»¬ä¸è®¾ç½®æ—¶é—´æ­¥æˆªæ–­
        
        return observation, reward, terminated, truncated, info

    def _get_next_observation(self):
        """æ‰¾åˆ°ä¸‹ä¸€ä¸ªå°±ç»ªçš„ä»»åŠ¡å¹¶ä¸ºå…¶æ„å»ºè§‚å¯Ÿå‘é‡"""
        if not self.pending_tasks:
            return np.zeros(self.observation_space.shape), {"is_success": True}

        # æ‰¾åˆ°ä¸‹ä¸€ä¸ªå°±ç»ªçš„ä»»åŠ¡
        ready_tasks = [
            task for task_id in sorted(list(self.pending_tasks))
            if all(dep in self.task_finish_times for dep in (task := next(t for t in self.workflow['tasks'] if t['id'] == task_id))['dependencies'])
        ]
        
        if not ready_tasks:
            # å¦‚æœæ²¡æœ‰å°±ç»ªä»»åŠ¡ï¼Œè¯´æ˜å·¥ä½œæµç»“æŸæˆ–å¡æ­»
            return np.zeros(self.observation_space.shape), {"error": "No ready tasks"}

        self.current_task_obj = ready_tasks[0]
        
        # æ„å»ºå½“å‰çŠ¶æ€
        current_time = min(self.node_available_times.values())
        earliest_start_times = {}
        for node in self.nodes:
            deps = self.current_task_obj.get('dependencies', [])
            data_ready = max([self.task_finish_times.get(d, 0) for d in deps], default=0)
            earliest_start_times[node] = max(self.node_available_times[node], data_ready)

        self.current_state = SchedulingState(
            workflow_graph=self.workflow,
            cluster_state={"nodes": self.cluster, "earliest_start_times": earliest_start_times},
            pending_tasks=list(self.pending_tasks),
            current_task=self.current_task_obj['id'],
            available_nodes=self.nodes,
            timestamp=current_time
        )
        
        # ç”ŸæˆçŠ¶æ€ç‰¹å¾
        self.current_state_embedding = self.teacher_model._extract_simple_features_fallback(self.current_state)
        
        # å¯¹äºPPOï¼Œè§‚å¯Ÿé€šå¸¸æ˜¯çŠ¶æ€æœ¬èº«
        # è¿™é‡Œçš„è§‚å¯Ÿç©ºé—´è¢«ç®€åŒ–äº†ï¼Œå®é™…åº”ç”¨ä¸­å¯ä»¥æ›´å¤æ‚
        # ä¸ºäº†ä¸ PolicyNetwork çš„è¾“å…¥åŒ¹é…ï¼Œæˆ‘ä»¬ç”¨ state_embedding å’Œä¸€ä¸ªé›¶å‘é‡æ‹¼æ¥
        obs = np.concatenate([
            self.current_state_embedding.cpu().numpy(),
            np.zeros(32) # å ä½ç¬¦
        ])
        
        return obs.astype(np.float32), {}

def main():
    print("ğŸš€ WASS-RAG Stage 3: DRL Agent Training ğŸš€")
    
    # 1. åˆ›å»ºç¯å¢ƒ
    env_config = {
        "workflow_sizes": [10, 50, 100],
        "cluster_sizes": [4, 8, 16],
        "repetitions": 1 # åœ¨ DRL è®­ç»ƒä¸­ï¼Œè¿™ä¸ªå‚æ•°æ„ä¹‰ä¸å¤§
    }
    # Stable Baselines3 éœ€è¦ä¸€ä¸ªå‡½æ•°æ¥åˆ›å»ºç¯å¢ƒ
    env_fn = lambda: WassEnv(env_config)
    env = env_fn()
    
    # æ£€æŸ¥è‡ªå®šä¹‰ç¯å¢ƒæ˜¯å¦ç¬¦åˆ Gym API
    print("\nğŸ•µï¸  Checking custom environment...")
    check_env(env)
    print("âœ… Environment check passed!")
    
    # 2. å®šä¹‰ DRL ä»£ç†
    # æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªç°æœ‰çš„ PolicyNetwork ç»“æ„ï¼Œä½†è®© SB3 æ¥è®­ç»ƒå®ƒ
    policy_kwargs = {
        "net_arch": {
            "pi": [128, 128], # Actor network
            "vf": [128, 128]  # Critic network
        }
    }
    
    # PPO æ˜¯ä¸€ä¸ªéå¸¸å¼ºå¤§ä¸”ç¨³å®šçš„ç®—æ³•
    agent = PPO(
        "MlpPolicy",
        env,
        policy_kwargs=policy_kwargs,
        verbose=1, # æ‰“å°è®­ç»ƒè¿‡ç¨‹
        tensorboard_log="./drl_tensorboard_logs/"
    )

    # 3. å¼€å§‹è®­ç»ƒ
    # total_timesteps å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ï¼Œ100000 æ˜¯ä¸€ä¸ªä¸é”™çš„èµ·ç‚¹
    total_timesteps = 100000
    print(f"\nğŸ§  Starting PPO training for {total_timesteps} timesteps...")
    agent.learn(total_timesteps=total_timesteps, progress_bar=True)
    
    # 4. ä¿å­˜è®­ç»ƒå¥½çš„ç­–ç•¥ç½‘ç»œ
    print("\nâœ… DRL training complete. Saving the policy network...")
    model_path = Path("models/wass_models.pth")
    
    try:
        checkpoint = torch.load(model_path, map_location="cpu", weights_only=False)
        print("   Found existing model file. Updating Policy Network weights.")
    except (FileNotFoundError, EOFError):
        checkpoint = {}
        print("   No existing model file found. Creating a new checkpoint.")
    
    # ä» SB3 ä»£ç†ä¸­æå–ç­–ç•¥ç½‘ç»œçš„çŠ¶æ€å­—å…¸
    trained_policy_state_dict = agent.policy.state_dict()
    
    # æ³¨æ„ï¼šè¿™é‡Œçš„ç»“æ„éœ€è¦ä¸ ai_schedulers.py ä¸­çš„ PolicyNetwork åŒ¹é…
    # SB3 çš„ MlpPolicy ç»“æ„æ›´å¤æ‚ï¼Œç›´æ¥ä¿å­˜å¯èƒ½ä¸å…¼å®¹
    # ç®€åŒ–ï¼šæˆ‘ä»¬åªä¿å­˜ actor ç½‘ç»œçš„éƒ¨åˆ†æƒé‡
    # åœ¨å®é™…é¡¹ç›®ä¸­ï¼Œéœ€è¦ç¡®ä¿ç½‘ç»œç»“æ„å®Œå…¨ä¸€è‡´
    # è¿™é‡Œæˆ‘ä»¬åšä¸€ä¸ªæ˜ å°„
    policy_net = PolicyNetwork(state_dim=64, hidden_dim=128)
    new_state_dict = policy_net.state_dict()
    
    # ç®€å•çš„æƒé‡æ˜ å°„ï¼ˆå¯èƒ½éœ€è¦æ ¹æ®å®é™…å±‚åè°ƒæ•´ï¼‰
    new_state_dict['network.0.weight'] = trained_policy_state_dict['mlp_extractor.policy_net.0.weight']
    new_state_dict['network.0.bias'] = trained_policy_state_dict['mlp_extractor.policy_net.0.bias']
    new_state_dict['network.2.weight'] = trained_policy_state_dict['mlp_extractor.policy_net.2.weight']
    new_state_dict['network.2.bias'] = trained_policy_state_dict['mlp_extractor.policy_net.2.bias']
    # æœ€åä¸€å±‚
    new_state_dict['network.4.weight'] = trained_policy_state_dict['action_net.weight']
    new_state_dict['network.4.bias'] = trained_policy_state_dict['action_net.bias']
    
    checkpoint["policy_network"] = new_state_dict
    
    torch.save(checkpoint, model_path)
    print(f"âœ… Policy Network weights updated and saved to {model_path}")
    print("\nğŸ‰ All three stages are complete! Your AI schedulers are now fully trained.")


if __name__ == "__main__":
    main()