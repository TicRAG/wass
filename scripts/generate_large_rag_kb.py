#!/usr/bin/env python3
"""
WASS-RAG å¤§è§„æ¨¡çŸ¥è¯†åº“ç”Ÿæˆå™¨ (æ–¹æ¡ˆA)
ç”Ÿæˆ2500ä¸ªé«˜è´¨é‡RAGçŸ¥è¯†æ¡ˆä¾‹
"""

import json
import random
import argparse
from pathlib import Path
from datetime import datetime

def generate_large_rag_knowledge(num_cases=2500):
    """ç”Ÿæˆå¤§è§„æ¨¡RAGçŸ¥è¯†åº“"""
    
    print(f"ğŸš€ å¼€å§‹ç”Ÿæˆ {num_cases} ä¸ªRAGçŸ¥è¯†æ¡ˆä¾‹...")
    
    # èŠ‚ç‚¹é…ç½®
    node_configs = {
        "ComputeHost1": {"capacity": 2.0, "memory": 16},
        "ComputeHost2": {"capacity": 3.0, "memory": 24}, 
        "ComputeHost3": {"capacity": 2.5, "memory": 20},
        "ComputeHost4": {"capacity": 4.0, "memory": 32}
    }
    
    # è°ƒåº¦å™¨ç±»å‹åŠå…¶ç‰¹æ€§
    scheduler_types = {
        "FIFO": {"efficiency": 0.7, "variance": 0.3},
        "HEFT": {"efficiency": 0.85, "variance": 0.15},
        "Random": {"efficiency": 0.6, "variance": 0.4},
        "WASS-Heuristic": {"efficiency": 0.9, "variance": 0.1},
        "Optimal": {"efficiency": 1.0, "variance": 0.05}
    }
    
    # å·¥ä½œæµæ¨¡å¼
    workflow_patterns = ["montage", "ligo", "cybershake", "sipht", "genome"]
    
    all_cases = []
    scheduler_counts = {}
    node_counts = {}
    
    for case_id in range(num_cases):
        # éšæœºé€‰æ‹©å‚æ•°
        scheduler = random.choice(list(scheduler_types.keys()))
        node = random.choice(list(node_configs.keys()))
        pattern = random.choice(workflow_patterns)
        
        # ç”Ÿæˆä»»åŠ¡ç‰¹å¾
        task_size = random.choice(["small", "medium", "large"])
        if task_size == "small":
            task_flops = random.uniform(1e8, 1e9)
            workflow_size = random.randint(5, 20)
        elif task_size == "medium":
            task_flops = random.uniform(1e9, 10e9)
            workflow_size = random.randint(20, 100)
        else:  # large
            task_flops = random.uniform(10e9, 100e9)
            workflow_size = random.randint(100, 500)
        
        # è®¡ç®—æ‰§è¡Œæ—¶é—´ï¼ˆè€ƒè™‘è°ƒåº¦å™¨æ•ˆç‡ï¼‰
        node_capacity = node_configs[node]["capacity"]
        scheduler_efficiency = scheduler_types[scheduler]["efficiency"]
        base_exec_time = task_flops / (node_capacity * 1e9)
        actual_exec_time = base_exec_time / scheduler_efficiency
        
        # æ·»åŠ éšæœºæ€§
        variance = scheduler_types[scheduler]["variance"]
        noise = random.uniform(1 - variance, 1 + variance)
        actual_exec_time *= noise
        
        # è®¡ç®—å·¥ä½œæµmakespanï¼ˆç®€åŒ–ä¼°ç®—ï¼‰
        critical_path_length = workflow_size * 0.3  # å‡è®¾å…³é”®è·¯å¾„å 30%
        workflow_makespan = critical_path_length * actual_exec_time
        
        # è®¡ç®—æ€§èƒ½æ¯”ç‡
        optimal_time = base_exec_time / scheduler_types["Optimal"]["efficiency"]
        performance_ratio = optimal_time / actual_exec_time
        
        # æ•°æ®å±€éƒ¨æ€§è¯„åˆ†
        data_locality_score = random.uniform(0.1, 1.0)
        if scheduler == "WASS-Heuristic":
            data_locality_score *= 1.2  # WASS-Heuristicè€ƒè™‘æ•°æ®å±€éƒ¨æ€§
        
        # åˆ›å»ºæ¡ˆä¾‹
        case = {
            "case_id": f"case_{case_id:06d}",
            "workflow_id": f"{pattern}_workflow_{case_id // 10}",
            "task_id": f"task_{case_id % workflow_size}",
            "scheduler_type": scheduler,
            "chosen_node": node,
            "workflow_pattern": pattern,
            "workflow_size": workflow_size,
            "task_flops": task_flops,
            "task_execution_time": actual_exec_time,
            "workflow_makespan": workflow_makespan,
            "node_capacity": node_capacity,
            "node_memory": node_configs[node]["memory"],
            "performance_ratio": performance_ratio,
            "data_locality_score": data_locality_score,
            "scheduler_efficiency": scheduler_efficiency,
            "task_size_category": task_size,
            "timestamp": datetime.now().isoformat(),
            "features": {
                "cpu_intensive": task_flops > 5e9,
                "memory_intensive": node_configs[node]["memory"] > 20,
                "io_intensive": pattern in ["genome", "montage"],
                "critical_path_task": random.random() > 0.7,
                "data_dependent": data_locality_score > 0.7
            }
        }
        
        all_cases.append(case)
        
        # ç»Ÿè®¡
        scheduler_counts[scheduler] = scheduler_counts.get(scheduler, 0) + 1
        node_counts[node] = node_counts.get(node, 0) + 1
        
        # è¿›åº¦æ˜¾ç¤º
        if (case_id + 1) % 500 == 0:
            print(f"å·²ç”Ÿæˆ {case_id + 1}/{num_cases} ä¸ªæ¡ˆä¾‹...")
    
    return all_cases, scheduler_counts, node_counts

def save_knowledge_base(cases, output_path="data/extended_rag_knowledge_v2.json"):
    """ä¿å­˜çŸ¥è¯†åº“åˆ°æ–‡ä»¶"""
    
    knowledge_base = {
        "metadata": {
            "version": "2.0",
            "description": "æ‰©å±•çš„WASS-RAGçŸ¥è¯†åº“ (æ–¹æ¡ˆA)",
            "total_cases": len(cases),
            "generated_at": datetime.now().isoformat(),
            "features": [
                "å¤šç§å·¥ä½œæµæ¨¡å¼",
                "5ç§è°ƒåº¦å™¨ç±»å‹", 
                "è¯¦ç»†æ€§èƒ½æŒ‡æ ‡",
                "æ•°æ®å±€éƒ¨æ€§å»ºæ¨¡",
                "ä»»åŠ¡ç‰¹å¾åˆ†ç±»"
            ]
        },
        "cases": cases
    }
    
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(knowledge_base, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… æ‰©å±•RAGçŸ¥è¯†åº“å·²ä¿å­˜: {output_path}")
    return output_path

def analyze_distribution(scheduler_counts, node_counts):
    """åˆ†ææ¡ˆä¾‹åˆ†å¸ƒ"""
    print("\nğŸ“Š æ¡ˆä¾‹åˆ†å¸ƒåˆ†æ:")
    
    print("\nè°ƒåº¦å™¨åˆ†å¸ƒ:")
    for scheduler, count in sorted(scheduler_counts.items()):
        percentage = count / sum(scheduler_counts.values()) * 100
        print(f"  {scheduler}: {count} ä¸ªæ¡ˆä¾‹ ({percentage:.1f}%)")
    
    print("\nèŠ‚ç‚¹åˆ†å¸ƒ:")
    for node, count in sorted(node_counts.items()):
        percentage = count / sum(node_counts.values()) * 100
        print(f"  {node}: {count} ä¸ªæ¡ˆä¾‹ ({percentage:.1f}%)")

def main():
    parser = argparse.ArgumentParser(description='ç”Ÿæˆå¤§è§„æ¨¡RAGçŸ¥è¯†åº“')
    parser.add_argument('--num_cases', type=int, default=2500, 
                       help='ç”Ÿæˆçš„æ¡ˆä¾‹æ•°é‡')
    parser.add_argument('--output', default='data/extended_rag_knowledge_v2.json',
                       help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # ç”ŸæˆçŸ¥è¯†åº“
    cases, scheduler_counts, node_counts = generate_large_rag_knowledge(args.num_cases)
    
    # ä¿å­˜æ–‡ä»¶
    output_path = save_knowledge_base(cases, args.output)
    
    # åˆ†æåˆ†å¸ƒ
    analyze_distribution(scheduler_counts, node_counts)
    
    print(f"\nğŸ‰ å¤§è§„æ¨¡RAGçŸ¥è¯†åº“ç”Ÿæˆå®Œæˆ!")
    print(f"ğŸ“ æ–‡ä»¶ä½ç½®: {output_path}")
    print(f"ğŸ“Š æ¡ˆä¾‹æ€»æ•°: {len(cases)}")
    
    # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
    file_size = Path(output_path).stat().st_size / 1024 / 1024  # MB
    print(f"ğŸ’¾ æ–‡ä»¶å¤§å°: {file_size:.2f} MB")

if __name__ == "__main__":
    main()
