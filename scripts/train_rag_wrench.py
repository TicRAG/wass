#!/usr/bin/env python3
"""
åŸºäºWRENCHçš„RAGçŸ¥è¯†åº“è®­ç»ƒè„šæœ¬
ä½¿ç”¨çœŸå®çš„WRENCHä»¿çœŸæ•°æ®æ„å»ºRAGçŸ¥è¯†åº“
"""

import sys
import os
import json
import time
import random
import numpy as np
import yaml
import pickle
from pathlib import Path
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass, asdict
from collections import defaultdict

# ç¡®ä¿èƒ½å¯¼å…¥WRENCH
try:
    import wrench
except ImportError:
    print("Error: WRENCH not available. Please install wrench-python-api.")
    sys.exit(1)

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, str(parent_dir))

def load_config(cfg_path: str) -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(cfg_path, 'r', encoding='utf-8') as f:
        cfg = yaml.safe_load(f) or {}
    
    # Process includes
    if 'include' in cfg:
        base_dir = os.path.dirname(cfg_path)
        for include_file in cfg['include']:
            include_path = os.path.join(base_dir, include_file)
            if os.path.exists(include_path):
                with open(include_path, 'r', encoding='utf-8') as f:
                    include_cfg = yaml.safe_load(f) or {}
                    for key, value in include_cfg.items():
                        if key not in cfg:
                            cfg[key] = value
    return cfg

@dataclass
class WRENCHKnowledgeCase:
    """åŸºäºWRENCHçš„çŸ¥è¯†æ¡ˆä¾‹"""
    # å·¥ä½œæµç‰¹å¾
    workflow_id: str
    task_count: int
    dependency_ratio: float
    critical_path_length: int
    workflow_embedding: np.ndarray
    
    # ä»»åŠ¡ç‰¹å¾
    task_id: str
    task_flops: float
    task_input_files: int
    task_output_files: int
    task_dependencies: int
    task_children: int
    task_features: np.ndarray
    
    # èŠ‚ç‚¹ç‰¹å¾
    available_nodes: List[str]
    node_capacities: Dict[str, float]
    node_loads: Dict[str, float]
    node_features: np.ndarray
    
    # è°ƒåº¦å†³ç­–å’Œç»“æœ
    scheduler_type: str
    chosen_node: str
    action_taken: int
    
    # æ€§èƒ½ç»“æœ
    task_execution_time: float
    task_wait_time: float
    workflow_makespan: float
    node_utilization: Dict[str, float]
    
    # å…ƒæ•°æ®
    simulation_time: float
    platform_config: str
    metadata: Dict[str, Any]

class WRENCHWorkflowEmbedder:
    """åŸºäºWRENCHçš„å·¥ä½œæµåµŒå…¥å™¨"""
    
    def __init__(self, embedding_dim: int = 64):
        self.embedding_dim = embedding_dim
    
    def encode_workflow(self, workflow, tasks: List) -> np.ndarray:
        """å°†WRENCHå·¥ä½œæµç¼–ç ä¸ºå‘é‡"""
        if not tasks:
            return np.zeros(self.embedding_dim)
        
        # å·¥ä½œæµçº§åˆ«ç‰¹å¾
        num_tasks = len(tasks)
        
        # è®¡ç®—ä¾èµ–å…³ç³»
        total_dependencies = 0
        total_children = 0
        task_flops = []
        
        for task in tasks:
            deps = len(task.get_input_files())
            children = task.get_number_of_children()
            flops = task.get_flops()
            
            total_dependencies += deps
            total_children += children
            task_flops.append(flops)
        
        # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
        features = [
            num_tasks,
            total_dependencies / max(num_tasks, 1),  # å¹³å‡ä¾èµ–æ•°
            total_children / max(num_tasks, 1),      # å¹³å‡å­ä»»åŠ¡æ•°
            np.mean(task_flops) if task_flops else 0,
            np.std(task_flops) if len(task_flops) > 1 else 0,
            max(task_flops) if task_flops else 0,
            min(task_flops) if task_flops else 0
        ]
        
        # å¡«å……åˆ°æŒ‡å®šç»´åº¦
        while len(features) < self.embedding_dim:
            features.append(0.0)
        
        return np.array(features[:self.embedding_dim], dtype=np.float32)
    
    def encode_task(self, task) -> np.ndarray:
        """ç¼–ç å•ä¸ªä»»åŠ¡ç‰¹å¾"""
        features = [
            task.get_flops() / 1e9,  # æ ‡å‡†åŒ–åˆ°GFlops
            len(task.get_input_files()),
            len(task.get_output_files()),
            task.get_number_of_children(),
            1.0  # ä»»åŠ¡æ´»è·ƒæ ‡è®°
        ]
        return np.array(features, dtype=np.float32)
    
    def encode_nodes(self, node_capacities: Dict[str, float], 
                    node_loads: Dict[str, float]) -> np.ndarray:
        """ç¼–ç èŠ‚ç‚¹ç‰¹å¾"""
        if not node_capacities:
            return np.zeros(12)  # 4èŠ‚ç‚¹ * 3ç‰¹å¾
        
        features = []
        for node in sorted(node_capacities.keys()):
            capacity = node_capacities.get(node, 0.0)
            load = node_loads.get(node, 0.0)
            availability = max(0.0, capacity - load)
            
            features.extend([
                capacity / 4.0,      # æ ‡å‡†åŒ–å®¹é‡
                load / 4.0,          # æ ‡å‡†åŒ–è´Ÿè½½
                availability / 4.0   # æ ‡å‡†åŒ–å¯ç”¨æ€§
            ])
        
        # ç¡®ä¿å›ºå®šé•¿åº¦
        while len(features) < 12:
            features.append(0.0)
        
        return np.array(features[:12], dtype=np.float32)

class WRENCHRAGKnowledgeBase:
    """åŸºäºWRENCHçš„RAGçŸ¥è¯†åº“"""
    
    def __init__(self, embedding_dim: int = 64):
        self.cases: List[WRENCHKnowledgeCase] = []
        self.embedder = WRENCHWorkflowEmbedder(embedding_dim)
        self.case_index = {}
    
    def add_case(self, case: WRENCHKnowledgeCase):
        """æ·»åŠ çŸ¥è¯†æ¡ˆä¾‹"""
        self.cases.append(case)
    
    def build_index(self):
        """æ„å»ºæ£€ç´¢ç´¢å¼•"""
        if not self.cases:
            return
        
        print(f"æ„å»ºæ£€ç´¢ç´¢å¼•ï¼Œå…± {len(self.cases)} ä¸ªæ¡ˆä¾‹...")
        
        # æå–æ‰€æœ‰å·¥ä½œæµåµŒå…¥
        embeddings = np.array([case.workflow_embedding for case in self.cases])
        
        # ä½¿ç”¨ç®€å•çš„k-meansèšç±»
        n_clusters = min(20, len(self.cases))
        cluster_centers = self._simple_kmeans(embeddings, n_clusters)
        
        # ä¸ºæ¯ä¸ªæ¡ˆä¾‹åˆ†é…åˆ°æœ€è¿‘çš„èšç±»
        self.case_index = {i: [] for i in range(n_clusters)}
        
        for i, case in enumerate(self.cases):
            distances = [np.linalg.norm(case.workflow_embedding - center) 
                        for center in cluster_centers]
            cluster_id = np.argmin(distances)
            self.case_index[cluster_id].append(i)
        
        print(f"ç´¢å¼•æ„å»ºå®Œæˆï¼š{n_clusters} ä¸ªèšç±»")
        for i, cases in self.case_index.items():
            print(f"  èšç±» {i}: {len(cases)} ä¸ªæ¡ˆä¾‹")
    
    def _simple_kmeans(self, data: np.ndarray, k: int, max_iters: int = 50) -> np.ndarray:
        """ç®€å•çš„K-meanså®ç°"""
        n, d = data.shape
        if n < k:
            return data
        
        centroids = data[np.random.choice(n, k, replace=False)]
        
        for _ in range(max_iters):
            # åˆ†é…ç‚¹åˆ°æœ€è¿‘çš„èšç±»ä¸­å¿ƒ
            distances = np.sqrt(((data - centroids[:, np.newaxis])**2).sum(axis=2))
            assignments = np.argmin(distances, axis=0)
            
            # æ›´æ–°èšç±»ä¸­å¿ƒ
            new_centroids = np.array([data[assignments == i].mean(axis=0) 
                                    if np.any(assignments == i) else centroids[i]
                                    for i in range(k)])
            
            # æ£€æŸ¥æ”¶æ•›
            if np.allclose(centroids, new_centroids):
                break
            
            centroids = new_centroids
        
        return centroids
    
    def retrieve_similar_cases(self, query_embedding: np.ndarray, 
                             query_task_features: np.ndarray,
                             k: int = 5) -> List[Tuple[WRENCHKnowledgeCase, float]]:
        """æ£€ç´¢ç›¸ä¼¼æ¡ˆä¾‹"""
        if not self.cases:
            return []
        
        similarities = []
        for case in self.cases:
            # å·¥ä½œæµç›¸ä¼¼åº¦
            workflow_sim = self._cosine_similarity(query_embedding, case.workflow_embedding)
            
            # ä»»åŠ¡ç›¸ä¼¼åº¦
            task_sim = self._cosine_similarity(query_task_features, case.task_features)
            
            # ç»¼åˆç›¸ä¼¼åº¦
            total_sim = 0.7 * workflow_sim + 0.3 * task_sim
            similarities.append((case, total_sim))
        
        # æ’åºå¹¶è¿”å›top-k
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:k]
    
    def _cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        norm_a = np.linalg.norm(a)
        norm_b = np.linalg.norm(b)
        if norm_a == 0 or norm_b == 0:
            return 0.0
        return np.dot(a, b) / (norm_a * norm_b)
    
    def save(self, path: str):
        """ä¿å­˜çŸ¥è¯†åº“"""
        Path(path).parent.mkdir(parents=True, exist_ok=True)
        
        with open(path, 'wb') as f:
            pickle.dump({
                'cases': self.cases,
                'case_index': self.case_index,
                'embedding_dim': self.embedder.embedding_dim
            }, f)
        
        print(f"çŸ¥è¯†åº“å·²ä¿å­˜åˆ° {path}")
    
    @classmethod
    def load(cls, path: str) -> 'WRENCHRAGKnowledgeBase':
        """åŠ è½½çŸ¥è¯†åº“"""
        with open(path, 'rb') as f:
            data = pickle.load(f)
        
        kb = cls(data['embedding_dim'])
        kb.cases = data['cases']
        kb.case_index = data['case_index']
        
        return kb

class WRENCHRAGTrainer:
    """åŸºäºWRENCHçš„RAGè®­ç»ƒå™¨"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.knowledge_base = WRENCHRAGKnowledgeBase()
        
        # WRENCHå¹³å°é…ç½®
        self.platform_file = config['platform']['platform_file']
        self.controller_host = "ControllerHost"
        
        # èŠ‚ç‚¹é…ç½®
        self.compute_nodes = ["ComputeHost1", "ComputeHost2", "ComputeHost3", "ComputeHost4"]
        self.node_capacities = {
            "ComputeHost1": 2.0,
            "ComputeHost2": 3.0,
            "ComputeHost3": 2.5,
            "ComputeHost4": 4.0
        }
        
        # è°ƒåº¦å™¨é…ç½®
        self.schedulers = ["HEFT", "WASS-Heuristic"]
        
        print(f"WRENCH RAGè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def generate_wrench_cases(self, num_cases: int = 1000):
        """ä½¿ç”¨WRENCHç”ŸæˆçŸ¥è¯†æ¡ˆä¾‹"""
        print(f"ğŸš€ å¼€å§‹ç”Ÿæˆ {num_cases} ä¸ªWRENCHçŸ¥è¯†æ¡ˆä¾‹...")
        
        with open(self.platform_file, 'r', encoding='utf-8') as f:
            platform_xml = f.read()
        
        cases_generated = 0
        
        for scheduler_type in self.schedulers:
            cases_per_scheduler = num_cases // len(self.schedulers)
            
            for case_idx in range(cases_per_scheduler):
                try:
                    case = self._generate_single_case(platform_xml, scheduler_type, case_idx)
                    if case:
                        self.knowledge_base.add_case(case)
                        cases_generated += 1
                        
                        if cases_generated % 50 == 0:
                            print(f"  å·²ç”Ÿæˆ {cases_generated}/{num_cases} ä¸ªæ¡ˆä¾‹...")
                
                except Exception as e:
                    print(f"ç”Ÿæˆæ¡ˆä¾‹å¤±è´¥ (è°ƒåº¦å™¨: {scheduler_type}, ç´¢å¼•: {case_idx}): {e}")
                    continue
        
        print(f"âœ… æ€»å…±ç”Ÿæˆäº† {cases_generated} ä¸ªWRENCHçŸ¥è¯†æ¡ˆä¾‹")
    
    def _generate_single_case(self, platform_xml: str, scheduler_type: str, case_idx: int) -> WRENCHKnowledgeCase:
        """ç”Ÿæˆå•ä¸ªçŸ¥è¯†æ¡ˆä¾‹"""
        # åˆ›å»ºä»¿çœŸ
        sim = wrench.Simulation()
        sim.start(platform_xml, self.controller_host)
        
        try:
            # åˆ›å»ºæœåŠ¡
            storage_service = sim.create_simple_storage_service("StorageHost", ["/storage"])
            
            compute_resources = {}
            for node in self.compute_nodes:
                compute_resources[node] = (4, 8_589_934_592)
            
            compute_service = sim.create_bare_metal_compute_service(
                "ComputeHost1", compute_resources, "/scratch", {}, {}
            )
            
            # åˆ›å»ºéšæœºå·¥ä½œæµ
            workflow = sim.create_workflow()
            num_tasks = random.randint(5, 20)
            tasks = []
            files = []
            
            # åˆ›å»ºä»»åŠ¡
            for i in range(num_tasks):
                flops = random.uniform(1e9, 10e9)
                task = workflow.add_task(f"task_{case_idx}_{i}", flops, 1, 1, 0)
                tasks.append(task)
                
                # åˆ›å»ºè¾“å‡ºæ–‡ä»¶
                if i < num_tasks - 1:
                    output_file = sim.add_file(f"output_{case_idx}_{i}", random.randint(1024, 10240))
                    task.add_output_file(output_file)
                    files.append(output_file)
            
            # åˆ›å»ºä¾èµ–å…³ç³»
            for i in range(1, min(num_tasks, len(files) + 1)):
                if i > 1 and random.random() < 0.3:
                    dep_idx = random.randint(0, i-2)
                    if dep_idx < len(files):
                        tasks[i].add_input_file(files[dep_idx])
            
            # ä¸ºæ–‡ä»¶åˆ›å»ºå‰¯æœ¬
            for file in files:
                storage_service.create_file_copy(file)
            
            # å·¥ä½œæµåµŒå…¥
            workflow_embedding = self.knowledge_base.embedder.encode_workflow(workflow, tasks)
            
            # æ¨¡æ‹Ÿè°ƒåº¦è¿‡ç¨‹
            node_loads = {node: 0.0 for node in self.compute_nodes}
            task_results = []
            
            ready_tasks = workflow.get_ready_tasks()
            while ready_tasks:
                current_task = ready_tasks[0]
                
                # ä»»åŠ¡ç‰¹å¾
                task_features = self.knowledge_base.embedder.encode_task(current_task)
                
                # èŠ‚ç‚¹ç‰¹å¾
                node_features = self.knowledge_base.embedder.encode_nodes(
                    self.node_capacities, node_loads)
                
                # æ ¹æ®è°ƒåº¦å™¨é€‰æ‹©èŠ‚ç‚¹
                if scheduler_type == "HEFT":
                    # æœ€å¿«å¤„ç†å™¨
                    chosen_node = max(self.compute_nodes, key=lambda x: self.node_capacities[x])
                elif scheduler_type == "FIFO":
                    # æœ€å°‘è´Ÿè½½
                    chosen_node = min(self.compute_nodes, key=lambda x: node_loads[x])
                else:  # Random
                    chosen_node = random.choice(self.compute_nodes)
                
                action_taken = self.compute_nodes.index(chosen_node)
                
                # æäº¤ä½œä¸š
                file_locations = {}
                for f in current_task.get_input_files():
                    file_locations[f] = storage_service
                for f in current_task.get_output_files():
                    file_locations[f] = storage_service
                
                job = sim.create_standard_job([current_task], file_locations)
                compute_service.submit_standard_job(job)
                
                # ç­‰å¾…å®Œæˆ
                start_time = sim.get_simulated_time()
                while True:
                    event = sim.wait_for_next_event()
                    if event["event_type"] == "standard_job_completion":
                        completed_job = event["standard_job"]
                        if completed_job == job:
                            break
                    elif event["event_type"] == "simulation_termination":
                        break
                
                end_time = sim.get_simulated_time()
                execution_time = end_time - start_time
                
                # æ›´æ–°èŠ‚ç‚¹è´Ÿè½½
                node_loads[chosen_node] += execution_time
                
                # åˆ›å»ºçŸ¥è¯†æ¡ˆä¾‹
                case = WRENCHKnowledgeCase(
                    workflow_id=f"workflow_{case_idx}",
                    task_count=num_tasks,
                    dependency_ratio=sum(len(t.get_input_files()) for t in tasks) / num_tasks,
                    critical_path_length=num_tasks,  # ç®€åŒ–
                    workflow_embedding=workflow_embedding,
                    
                    task_id=current_task.get_name(),
                    task_flops=current_task.get_flops(),
                    task_input_files=len(current_task.get_input_files()),
                    task_output_files=len(current_task.get_output_files()),
                    task_dependencies=len(current_task.get_input_files()),
                    task_children=current_task.get_number_of_children(),
                    task_features=task_features,
                    
                    available_nodes=self.compute_nodes.copy(),
                    node_capacities=self.node_capacities.copy(),
                    node_loads=node_loads.copy(),
                    node_features=node_features,
                    
                    scheduler_type=scheduler_type,
                    chosen_node=chosen_node,
                    action_taken=action_taken,
                    
                    task_execution_time=execution_time,
                    task_wait_time=0.0,  # ç®€åŒ–
                    workflow_makespan=end_time,
                    node_utilization=node_loads.copy(),
                    
                    simulation_time=end_time,
                    platform_config=self.platform_file,
                    metadata={
                        "scheduler": scheduler_type,
                        "case_index": case_idx,
                        "generated_at": time.strftime("%Y-%m-%d %H:%M:%S")
                    }
                )
                
                task_results.append(case)
                ready_tasks = workflow.get_ready_tasks()
                
                # åªè®°å½•ç¬¬ä¸€ä¸ªä»»åŠ¡çš„æ¡ˆä¾‹ï¼ˆç®€åŒ–ï¼‰
                break
        
        finally:
            sim.terminate()
        
        return task_results[0] if task_results else None
    
    def train_retriever(self):
        """è®­ç»ƒRAGæ£€ç´¢å™¨"""
        print("ğŸ”§ è®­ç»ƒRAGæ£€ç´¢å™¨...")
        
        # æ„å»ºç´¢å¼•
        self.knowledge_base.build_index()
        
        # è¯„ä¼°æ£€ç´¢è´¨é‡
        self._evaluate_retrieval()
        
        print("âœ… RAGæ£€ç´¢å™¨è®­ç»ƒå®Œæˆ")
    
    def _evaluate_retrieval(self):
        """è¯„ä¼°æ£€ç´¢è´¨é‡"""
        if len(self.knowledge_base.cases) < 10:
            print("æ¡ˆä¾‹æ•°é‡ä¸è¶³ï¼Œè·³è¿‡è¯„ä¼°")
            return
        
        # éšæœºé€‰æ‹©æµ‹è¯•æ¡ˆä¾‹
        test_cases = np.random.choice(len(self.knowledge_base.cases), 
                                    min(20, len(self.knowledge_base.cases)), 
                                    replace=False)
        
        retrieval_scores = []
        
        for case_idx in test_cases:
            test_case = self.knowledge_base.cases[case_idx]
            
            # æ£€ç´¢ç›¸ä¼¼æ¡ˆä¾‹
            retrieved = self.knowledge_base.retrieve_similar_cases(
                test_case.workflow_embedding, 
                test_case.task_features, 
                k=5
            )
            
            # è®¡ç®—è°ƒåº¦å™¨ä¸€è‡´æ€§
            retrieved_schedulers = [case.scheduler_type for case, _ in retrieved]
            consistency = retrieved_schedulers.count(test_case.scheduler_type) / len(retrieved_schedulers)
            retrieval_scores.append(consistency)
        
        avg_consistency = np.mean(retrieval_scores) if retrieval_scores else 0.0
        print(f"ğŸ“Š æ£€ç´¢è´¨é‡è¯„ä¼° - è°ƒåº¦å™¨ä¸€è‡´æ€§: {avg_consistency:.3f}")
    
    def save_knowledge_base(self, path: str = "data/wrench_rag_knowledge_base.pkl"):
        """ä¿å­˜çŸ¥è¯†åº“"""
        self.knowledge_base.save(path)
        
        # ä¹Ÿä¿å­˜ä¸ºJSONæ ¼å¼ä¾¿äºæŸ¥çœ‹
        json_path = path.replace('.pkl', '.json')
        json_data = {
            'metadata': {
                'total_cases': len(self.knowledge_base.cases),
                'schedulers': list(set(case.scheduler_type for case in self.knowledge_base.cases)),
                'generated_at': time.strftime("%Y-%m-%d %H:%M:%S")
            },
            'sample_cases': []
        }
        
        # æ·»åŠ ä¸€äº›æ ·æœ¬æ¡ˆä¾‹
        for i, case in enumerate(self.knowledge_base.cases[:10]):
            json_data['sample_cases'].append({
                'workflow_id': case.workflow_id,
                'task_id': case.task_id,
                'scheduler_type': case.scheduler_type,
                'chosen_node': case.chosen_node,
                'task_execution_time': case.task_execution_time,
                'workflow_makespan': case.workflow_makespan
            })
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ çŸ¥è¯†åº“æ¦‚è¦å·²ä¿å­˜åˆ° {json_path}")

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) != 2:
        print("Usage: python scripts/train_rag_wrench.py <config.yaml>")
        sys.exit(1)
    
    config = load_config(sys.argv[1])
    trainer = WRENCHRAGTrainer(config)
    
    # ç”ŸæˆWRENCHæ¡ˆä¾‹
    num_cases = config.get('rag', {}).get('num_cases', 500)
    trainer.generate_wrench_cases(num_cases)
    
    # è®­ç»ƒæ£€ç´¢å™¨
    trainer.train_retriever()
    
    # ä¿å­˜çŸ¥è¯†åº“
    trainer.save_knowledge_base()
    
    print(f"\nğŸ‰ åŸºäºWRENCHçš„RAGè®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“ˆ çŸ¥è¯†åº“åŒ…å« {len(trainer.knowledge_base.cases)} ä¸ªçœŸå®ä»¿çœŸæ¡ˆä¾‹")

if __name__ == "__main__":
    main()
