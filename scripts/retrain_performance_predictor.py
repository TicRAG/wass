#!/usr/bin/env python3
"""
é‡æ–°è®­ç»ƒæ€§èƒ½é¢„æµ‹å™¨æ¨¡å‹ï¼Œä¿®å¤è´Ÿå€¼é¢„æµ‹é—®é¢˜
"""

import os
import sys
import torch
import torch.nn as nn
import numpy as np
import pickle
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)
sys.path.insert(0, os.path.join(parent_dir, 'src'))

try:
    from torch.utils.data import TensorDataset, DataLoader
    from src.ai_schedulers import PerformancePredictor, RAGKnowledgeBase
    HAS_AI_MODULES = True
except ImportError as e:
    print(f"Error: Required AI modules not available: {e}")
    sys.exit(1)

def create_improved_training_data(num_samples: int = 5000) -> List[Dict[str, Any]]:
    """åˆ›å»ºæ”¹è¿›çš„è®­ç»ƒæ•°æ®ï¼Œç¡®ä¿makespanåˆ†å¸ƒåˆç†"""
    
    print(f"ğŸ”§ Generating {num_samples} improved training samples...")
    training_data = []
    
    # ç»Ÿè®¡ç”Ÿæˆçš„makespanåˆ†å¸ƒ
    makespan_values = []
    
    for i in range(num_samples):
        # ç”Ÿæˆéšæœºå·¥ä½œæµï¼ˆæ›´å¤šæ ·åŒ–ï¼‰
        task_count = np.random.randint(3, 101)  # 3-100ä¸ªä»»åŠ¡
        cluster_size = np.random.randint(2, 21)  # 2-20ä¸ªèŠ‚ç‚¹
        
        # ä¸ºæ¯ä¸ªèŠ‚ç‚¹ç”Ÿæˆä¸åŒçš„å®¹é‡
        for node_idx in range(min(cluster_size, 10)):  # é™åˆ¶èŠ‚ç‚¹æ•°ä»¥é¿å…è¿‡å¤šæ•°æ®
            # èŠ‚ç‚¹ç‰¹æ€§
            cpu_capacity = np.random.uniform(8.0, 32.0)  # 8-32 GFlops
            memory_capacity = np.random.uniform(8.0, 64.0)  # 8-64 GB
            current_load = np.random.uniform(0.1, 0.9)
            
            # å·¥ä½œæµç‰¹å¾ï¼ˆæ›´çœŸå®çš„èŒƒå›´ï¼‰
            workflow_features = {
                "task_count": task_count,
                "avg_task_flops": np.random.uniform(0.5e9, 5e9),  # æ¯ä¸ªä»»åŠ¡0.5-5 GFlops
                "avg_memory": np.random.uniform(0.5, 8.0),  # 0.5-8 GB
                "dependency_ratio": np.random.uniform(0.1, 0.7),
                "data_intensity": np.random.uniform(0.05, 0.4)
            }
            
            # ç”ŸæˆçŠ¶æ€åµŒå…¥ï¼ˆ32ç»´ï¼‰
            state_embedding = np.array([
                task_count / 100.0,  # å½’ä¸€åŒ–ä»»åŠ¡æ•°
                workflow_features["avg_task_flops"] / 5e9,  # å½’ä¸€åŒ–è®¡ç®—é‡
                workflow_features["avg_memory"] / 8.0,  # å½’ä¸€åŒ–å†…å­˜
                workflow_features["dependency_ratio"],
                workflow_features["data_intensity"],
                cluster_size / 20.0,  # å½’ä¸€åŒ–é›†ç¾¤å¤§å°
                current_load,  # å½“å‰è´Ÿè½½
            ] + [np.random.randn() * 0.05 for _ in range(25)])  # å¡«å……åˆ°32ç»´
            
            # ç”ŸæˆåŠ¨ä½œåµŒå…¥ï¼ˆ32ç»´ï¼‰- èŠ‚ç‚¹é€‰æ‹©
            action_embedding = np.array([
                node_idx / 10.0,  # èŠ‚ç‚¹ç´¢å¼•å½’ä¸€åŒ–
                cpu_capacity / 32.0,  # CPUå®¹é‡å½’ä¸€åŒ–
                memory_capacity / 64.0,  # å†…å­˜å®¹é‡å½’ä¸€åŒ–
                current_load,  # å½“å‰è´Ÿè½½
                1.0 - current_load,  # ç©ºé—²åº¦
                (cpu_capacity / 32.0) * (1.0 - current_load),  # æœ‰æ•ˆè®¡ç®—èƒ½åŠ›
                (memory_capacity / 64.0) * (1.0 - current_load),  # æœ‰æ•ˆå†…å­˜
            ] + [np.random.randn() * 0.05 for _ in range(25)])  # å¡«å……åˆ°32ç»´
            
            # ç”Ÿæˆä¸Šä¸‹æ–‡åµŒå…¥ï¼ˆ32ç»´ï¼‰- å†å²ä¿¡æ¯
            historical_makespan = np.random.uniform(10.0, 200.0)
            similarity_score = np.random.uniform(0.4, 0.95)
            case_count = np.random.randint(3, 10)
            
            context_embedding = np.array([
                historical_makespan / 200.0,  # å†å²makespanå½’ä¸€åŒ–
                similarity_score,  # ç›¸ä¼¼åº¦å¾—åˆ†
                case_count / 10.0,  # æ¡ˆä¾‹æ•°é‡å½’ä¸€åŒ–
                np.random.uniform(0.6, 1.0),  # ç½®ä¿¡åº¦
            ] + [np.random.randn() * 0.05 for _ in range(28)])  # å¡«å……åˆ°32ç»´
            
            # æ”¹è¿›çš„makespanè®¡ç®—ï¼ˆç¡®ä¿ç‰©ç†åˆç†æ€§ï¼‰
            # å•ä»»åŠ¡å¹³å‡æ‰§è¡Œæ—¶é—´
            avg_task_time = workflow_features["avg_task_flops"] / (cpu_capacity * 1e9)
            
            # è€ƒè™‘å¹¶è¡Œæ€§çš„æ€»æ‰§è¡Œæ—¶é—´
            total_compute_time = task_count * avg_task_time
            parallel_efficiency = 0.6 + np.random.uniform(0.0, 0.3)  # 60-90%å¹¶è¡Œæ•ˆç‡
            ideal_parallel_time = total_compute_time / (cluster_size * parallel_efficiency)
            
            # å„ç§å¼€é”€å› å­
            load_overhead = 1.0 + current_load * 0.6  # è´Ÿè½½å¼€é”€
            dependency_overhead = 1.0 + workflow_features["dependency_ratio"] * 0.4  # ä¾èµ–å¼€é”€
            communication_overhead = 1.0 + workflow_features["data_intensity"] * 0.3  # é€šä¿¡å¼€é”€
            
            # éšæœºå˜åŒ–ï¼ˆæ¨¡æ‹Ÿç³»ç»Ÿå™ªå£°ï¼‰
            noise_factor = np.random.uniform(0.85, 1.15)
            
            # æœ€ç»ˆmakespan
            makespan = ideal_parallel_time * load_overhead * dependency_overhead * communication_overhead * noise_factor
            
            # ç¡®ä¿makespanåœ¨åˆç†èŒƒå›´å†…
            makespan = max(0.5, min(500.0, makespan))
            makespan_values.append(makespan)
            
            # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾ï¼ˆ96ç»´ï¼š32+32+32ï¼‰
            combined_features = np.concatenate([state_embedding, action_embedding, context_embedding])
            
            training_data.append({
                "id": f"improved_{i}_{node_idx}",
                "state_embedding": state_embedding.tolist(),
                "action_embedding": action_embedding.tolist(),
                "context_embedding": context_embedding.tolist(),
                "features": combined_features.tolist(),
                "makespan": makespan,
                "workflow_features": workflow_features,
                "node_features": {
                    "cpu_capacity": cpu_capacity,
                    "memory_capacity": memory_capacity,
                    "current_load": current_load
                }
            })
    
    # æ‰“å°makespanåˆ†å¸ƒç»Ÿè®¡
    makespan_array = np.array(makespan_values)
    print(f"ğŸ“Š Makespan distribution:")
    print(f"   Mean: {np.mean(makespan_array):.2f}s")
    print(f"   Std:  {np.std(makespan_array):.2f}s")
    print(f"   Min:  {np.min(makespan_array):.2f}s")
    print(f"   Max:  {np.max(makespan_array):.2f}s")
    print(f"   Median: {np.median(makespan_array):.2f}s")
    
    return training_data

def train_improved_performance_predictor(training_data: List[Dict[str, Any]], epochs: int = 200, batch_size: int = 64):
    """è®­ç»ƒæ”¹è¿›çš„æ€§èƒ½é¢„æµ‹å™¨"""
    
    print(f"ğŸš€ Training improved performance predictor...")
    print(f"   Training samples: {len(training_data)}")
    print(f"   Epochs: {epochs}")
    print(f"   Batch size: {batch_size}")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"   Device: {device}")
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    X = np.array([sample["features"] for sample in training_data])
    y = np.array([sample["makespan"] for sample in training_data])
    
    # æ•°æ®å½’ä¸€åŒ–ï¼ˆé‡è¦ï¼ï¼‰
    y_mean, y_std = np.mean(y), np.std(y)
    y_normalized = (y - y_mean) / (y_std + 1e-8)
    
    print(f"ğŸ“ˆ Training data statistics:")
    print(f"   Original y: mean={y_mean:.2f}, std={y_std:.2f}")
    print(f"   Normalized y: mean={np.mean(y_normalized):.6f}, std={np.std(y_normalized):.6f}")
    print(f"   Normalized range: [{np.min(y_normalized):.3f}, {np.max(y_normalized):.3f}]")
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    X_tensor = torch.FloatTensor(X).to(device)
    y_tensor = torch.FloatTensor(y_normalized).to(device)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataset = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    # åˆ›å»ºæ¨¡å‹
    model = PerformancePredictor(input_dim=96, hidden_dim=128).to(device)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=20, factor=0.5)
    
    # è®­ç»ƒå¾ªç¯
    best_loss = float('inf')
    patience_counter = 0
    
    for epoch in range(epochs):
        model.train()
        total_loss = 0.0
        
        for batch_X, batch_y in dataloader:
            optimizer.zero_grad()
            predictions = model(batch_X).squeeze()
            loss = criterion(predictions, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        avg_loss = total_loss / len(dataloader)
        scheduler.step(avg_loss)
        
        # æ—©åœæ£€æŸ¥
        if avg_loss < best_loss:
            best_loss = avg_loss
            patience_counter = 0
        else:
            patience_counter += 1
        
        if epoch % 20 == 0:
            print(f"   Epoch {epoch:3d}: Loss = {avg_loss:.6f}")
        
        if patience_counter >= 50:
            print(f"   Early stopping at epoch {epoch}")
            break
    
    # è¯„ä¼°æ¨¡å‹
    model.eval()
    with torch.no_grad():
        all_predictions = model(X_tensor).squeeze().cpu().numpy()
        all_predictions_denorm = all_predictions * y_std + y_mean
        
        mse = np.mean((all_predictions_denorm - y) ** 2)
        mae = np.mean(np.abs(all_predictions_denorm - y))
        r2 = 1 - np.sum((y - all_predictions_denorm) ** 2) / np.sum((y - np.mean(y)) ** 2)
        
        pred_std = np.std(all_predictions_denorm)
        pred_range = np.max(all_predictions_denorm) - np.min(all_predictions_denorm)
        
        print(f"\nâœ… Training completed!")
        print(f"   Final Loss: {best_loss:.6f}")
        print(f"   MSE: {mse:.2f}")
        print(f"   MAE: {mae:.2f}")
        print(f"   RÂ²: {r2:.4f}")
        print(f"   Prediction std: {pred_std:.2f}")
        print(f"   Prediction range: {pred_range:.2f}")
    
    return model, y_mean, y_std, {
        "mse": mse, "mae": mae, "r2": r2,
        "pred_std": pred_std, "pred_range": pred_range
    }

def regenerate_knowledge_base(training_data: List[Dict[str, Any]]) -> RAGKnowledgeBase:
    """æ ¹æ®æ–°çš„è®­ç»ƒæ•°æ®é‡æ–°ç”ŸæˆçŸ¥è¯†åº“"""
    
    print(f"\nğŸ”„ Regenerating knowledge base with {len(training_data)} cases...")
    
    # åˆ›å»ºæ–°çš„çŸ¥è¯†åº“
    kb = RAGKnowledgeBase(embedding_dim=32)
    
    for data in training_data:
        # ä½¿ç”¨çŠ¶æ€åµŒå…¥ä½œä¸ºä¸»è¦ç‰¹å¾
        embedding = np.array(data["state_embedding"], dtype=np.float32)
        
        # æ„å»ºå·¥ä½œæµä¿¡æ¯
        workflow_info = {
            "task_count": data["workflow_features"]["task_count"],
            "avg_task_flops": data["workflow_features"]["avg_task_flops"],
            "avg_memory": data["workflow_features"]["avg_memory"],
            "dependency_ratio": data["workflow_features"]["dependency_ratio"],
            "data_intensity": data["workflow_features"]["data_intensity"],
            "complexity": "medium",
            "type": "retrained_synthetic"
        }
        
        # ç”Ÿæˆè™šæ‹ŸåŠ¨ä½œåºåˆ—ï¼ˆèŠ‚ç‚¹åˆ†é…ï¼‰
        cluster_size = int(data["workflow_features"]["task_count"] * 0.1) + 2  # ä¼°ç®—é›†ç¾¤å¤§å°
        actions = [f"node_{i % cluster_size}" for i in range(data["workflow_features"]["task_count"])]
        
        # ä½¿ç”¨å®é™…çš„makespan
        makespan = data["makespan"]
        
        # æ·»åŠ åˆ°çŸ¥è¯†åº“
        kb.add_case(embedding, workflow_info, actions, makespan)
    
    print(f"âœ… Knowledge base regenerated with {len(kb.cases)} cases")
    return kb

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ Retraining Performance Predictor with Improved Data")
    print("=" * 60)
    
    # ç”Ÿæˆæ”¹è¿›çš„è®­ç»ƒæ•°æ®
    training_data = create_improved_training_data(num_samples=5000)
    
    # è®­ç»ƒæ¨¡å‹
    model, y_mean, y_std, metrics = train_improved_performance_predictor(training_data)
    
    # é‡æ–°ç”ŸæˆçŸ¥è¯†åº“ï¼ˆä½¿ç”¨ç›¸åŒçš„è®­ç»ƒæ•°æ®ï¼‰
    kb = regenerate_knowledge_base(training_data)
    
    # ä¿å­˜æ¨¡å‹
    model_path = "models/wass_models.pth"
    print(f"\nğŸ’¾ Saving retrained model to {model_path}...")
    
    # åŠ è½½ç°æœ‰checkpointï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    try:
        checkpoint = torch.load(model_path, map_location="cpu")
        print("   Loaded existing checkpoint")
    except:
        checkpoint = {}
        print("   Creating new checkpoint")
    
    # æ›´æ–°æ€§èƒ½é¢„æµ‹å™¨
    checkpoint["performance_predictor"] = model.state_dict()
    
    # æ›´æ–°å…ƒæ•°æ®
    if "metadata" not in checkpoint:
        checkpoint["metadata"] = {}
    
    checkpoint["metadata"]["performance_predictor"] = {
        "y_mean": float(y_mean),
        "y_std": float(y_std),
        "training_samples": len(training_data),
        "retrained_at": "2025-09-08",
        "validation_results": metrics
    }
    
    # ä¿å­˜æ¨¡å‹
    os.makedirs("models", exist_ok=True)
    torch.save(checkpoint, model_path)
    
    # ä¿å­˜çŸ¥è¯†åº“
    kb_path = "data/knowledge_base.pkl"
    print(f"\nğŸ’¾ Saving regenerated knowledge base to {kb_path}...")
    os.makedirs("data", exist_ok=True)
    kb.save_knowledge_base(kb_path)
    
    print(f"âœ… Model and knowledge base retrained and saved successfully!")
    print(f"   New normalization: mean={y_mean:.2f}, std={y_std:.2f}")
    print(f"   Performance metrics: RÂ²={metrics['r2']:.4f}, MSE={metrics['mse']:.2f}")
    print(f"   Knowledge base cases: {len(kb.cases)}")
    print(f"\nğŸ‰ Ready for testing! Run: python experiments/real_experiment_framework.py")

if __name__ == "__main__":
    main()
