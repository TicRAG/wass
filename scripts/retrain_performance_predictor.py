#!/usr/bin/env python3
"""
é‡æ–°è®­ç»ƒæ€§èƒ½é¢„æµ‹å™¨æ¨¡å‹ï¼Œä¿®å¤è´Ÿå€¼é¢„æµ‹é—®é¢˜
"""

import os
import sys
import torch
import torch.nn as nn
import numpy as np
import pickle
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)
sys.path.insert(0, os.path.join(parent_dir, 'src'))

try:
    from torch.utils.data import TensorDataset, DataLoader
    from src.ai_schedulers import PerformancePredictor, RAGKnowledgeBase
    HAS_AI_MODULES = True
except ImportError as e:
    print(f"Error: Required AI modules not available: {e}")
    sys.exit(1)

# ä½äº scripts/retrain_performance_predictor.py æ–‡ä»¶ä¸­
# ç”¨ä¸‹é¢çš„å…¨éƒ¨ä»£ç æ›¿æ¢æ‰ç°æœ‰çš„ create_improved_training_data å‡½æ•°

def create_improved_training_data(num_scenarios: int = 5000) -> List[Dict[str, Any]]:
    """
    ç”Ÿæˆé«˜è´¨é‡çš„åˆæˆè®­ç»ƒæ•°æ®ï¼ˆV4 - æœ€ç»ˆä¿®å¤ç‰ˆï¼‰
    ç¡®ä¿ç‰¹å¾ç”Ÿæˆé€»è¾‘ä¸ ai_schedulers.py ä¸­çš„é€»è¾‘å®Œå…¨ä¸€è‡´ã€‚
    """
    print(f"ğŸ”§ Generating {num_scenarios} scenarios for training data...")

    # å¯¼å…¥è°ƒåº¦å™¨ä»¥å¤ç”¨å…¶å†…éƒ¨é€»è¾‘
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬æ˜¯åœ¨è®­ç»ƒè„šæœ¬ä¸­å¯¼å…¥è°ƒåº¦å™¨æ¨¡å—
    from src.ai_schedulers import WASSRAGScheduler, SchedulingState

    # åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„è°ƒåº¦å™¨å®ä¾‹æ¥è°ƒç”¨å…¶ç¼–ç å‡½æ•°
    # æˆ‘ä»¬ä¸éœ€è¦åŠ è½½å®ƒçš„æ¨¡å‹ï¼Œåªéœ€è¦å®ƒçš„ç‰¹å¾ç¼–ç æ–¹æ³•
    temp_scheduler = WASSRAGScheduler()
    training_data = []
    makespan_values = []

    for i in range(num_scenarios):
        # 1. åˆ›å»ºä¸€ä¸ªéšæœºçš„ã€å¤šæ ·åŒ–çš„è°ƒåº¦åœºæ™¯ (State)
        num_nodes = np.random.randint(2, 21)
        
        nodes = {f"node_{j}": {
            "cpu_capacity": round(np.random.uniform(2.0, 8.0), 2),
            "memory_capacity": round(np.random.uniform(8.0, 64.0), 2),
            "current_load": round(np.random.uniform(0.1, 0.9), 2),
        } for j in range(num_nodes)}

        # ç¡®ä¿ä»»åŠ¡çš„flopså€¼ä¸ai_schedulers.pyä¸­çš„å•ä½ä¸€è‡´ (GFlops)
        task_info = {
            "id": f"task_{i}",
            "flops": float(np.random.uniform(0.5e9, 15e9)),
            "memory": round(np.random.uniform(1.0, 16.0), 2),
            "dependencies": [f"task_{k}" for k in range(np.random.randint(0, 4))]
        }

        state = SchedulingState(
            workflow_graph={"tasks": [task_info], "task_requirements": {f"task_{i}": task_info}},
            cluster_state={"nodes": nodes},
            pending_tasks=[f"task_{i}"],
            current_task=f"task_{i}",
            available_nodes=list(nodes.keys()),
            timestamp=0.0
        )

        # 2. ç¼–ç é€šç”¨çš„ State å’Œ Context éƒ¨åˆ†
        # è°ƒç”¨è°ƒåº¦å™¨è‡ªå·±çš„æ–¹æ³•æ¥ç¡®ä¿é€»è¾‘ä¸€è‡´
        state_embedding = temp_scheduler._extract_simple_features_fallback(state)
        context_embedding = torch.randn(32, device=temp_scheduler.device) # æ¨¡æ‹Ÿéšæœºä¸Šä¸‹æ–‡

        # 3. ä¸ºè¯¥åœºæ™¯ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹ç”Ÿæˆä¸€ä¸ªç‹¬ç«‹çš„è®­ç»ƒæ ·æœ¬
        for node_name, node_details in nodes.items():
            # å…³é”®ä¿®å¤ï¼šè°ƒç”¨ä¸é¢„æµ‹æ—¶å®Œå…¨ç›¸åŒçš„ _encode_action å‡½æ•°
            action_embedding = temp_scheduler._encode_action(node_name, state)

            # æ‹¼æ¥æˆ96ç»´ç‰¹å¾å‘é‡ï¼Œç¡®ä¿100%ä¸€è‡´æ€§
            combined_features = torch.cat([
                state_embedding,
                action_embedding,
                context_embedding
            ]).cpu().numpy()
            
            # 4. æ ¹æ®ç‰¹å¾ä¼°ç®—ä¸€ä¸ªçœŸå®çš„æ‰§è¡Œæ—¶é—´ (y)ï¼Œè¿™ä¸ªé€»è¾‘éœ€è¦å°½å¯èƒ½æ¨¡æ‹ŸçœŸå®ä¸–ç•Œ
            task_cpu_gflops = task_info["flops"] / 1e9
            node_cpu_cap = node_details["cpu_capacity"]
            node_load = node_details["current_load"]
            
            available_cpu = node_cpu_cap * (1.0 - node_load)
            
            # åŸºç¡€æ—¶é—´ = ä»»åŠ¡è®¡ç®—é‡ / èŠ‚ç‚¹å¯ç”¨ç®—åŠ›
            base_time = task_cpu_gflops / max(available_cpu, 0.1)
            
            # å¢åŠ ä¸€äº›å™ªå£°å’Œæƒ©ç½šé¡¹
            mem_penalty = max(0, task_info["memory"] - node_details["memory_capacity"]) * 0.5
            load_penalty = node_load * 2.0
            random_noise = np.random.uniform(-0.5, 0.5)
            
            # æœ€ç»ˆæ‰§è¡Œæ—¶é—´
            execution_time = base_time + mem_penalty + load_penalty + random_noise
            execution_time = max(1.0, min(180.0, execution_time)) # çº¦æŸåœ¨åˆç†èŒƒå›´

            makespan_values.append(execution_time)
            
            training_data.append({
                "features": combined_features.tolist(),
                "makespan": execution_time
                # å…¶ä»–å…ƒæ•°æ®å¯ä»¥æŒ‰éœ€ä¿ç•™
            })
            
    # æ‰“å°ä»»åŠ¡æ‰§è¡Œæ—¶é—´åˆ†å¸ƒç»Ÿè®¡
    makespan_array = np.array(makespan_values)
    print(f"ğŸ“Š Single task execution time distribution:")
    print(f"   Mean: {np.mean(makespan_array):.2f}s")
    print(f"   Std:  {np.std(makespan_array):.2f}s")
    print(f"   Min:  {np.min(makespan_array):.2f}s")
    print(f"   Max:  {np.max(makespan_array):.2f}s")
    print(f"   Median: {np.median(makespan_array):.2f}s")
    
    return training_data

def train_improved_performance_predictor(training_data: List[Dict[str, Any]], epochs: int = 200, batch_size: int = 64):
    """è®­ç»ƒæ”¹è¿›çš„æ€§èƒ½é¢„æµ‹å™¨"""
    
    print(f"ğŸš€ Training improved performance predictor...")
    print(f"   Training samples: {len(training_data)}")
    print(f"   Epochs: {epochs}")
    print(f"   Batch size: {batch_size}")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"   Device: {device}")
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    X = np.array([sample["features"] for sample in training_data])
    y = np.array([sample["makespan"] for sample in training_data])
    
    # æ•°æ®å½’ä¸€åŒ–ï¼ˆé‡è¦ï¼ï¼‰
    y_mean, y_std = np.mean(y), np.std(y)
    y_normalized = (y - y_mean) / (y_std + 1e-8)
    
    print(f"ğŸ“ˆ Training data statistics:")
    print(f"   Original y: mean={y_mean:.2f}, std={y_std:.2f}")
    print(f"   Normalized y: mean={np.mean(y_normalized):.6f}, std={np.std(y_normalized):.6f}")
    print(f"   Normalized range: [{np.min(y_normalized):.3f}, {np.max(y_normalized):.3f}]")
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    X_tensor = torch.FloatTensor(X).to(device)
    y_tensor = torch.FloatTensor(y_normalized).to(device)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataset = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    # åˆ›å»ºæ¨¡å‹
    model = PerformancePredictor(input_dim=96, hidden_dim=128).to(device)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=20, factor=0.5)
    
    # è®­ç»ƒå¾ªç¯
    best_loss = float('inf')
    patience_counter = 0
    
    for epoch in range(epochs):
        model.train()
        total_loss = 0.0
        
        for batch_X, batch_y in dataloader:
            optimizer.zero_grad()
            predictions = model(batch_X).squeeze()
            loss = criterion(predictions, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        avg_loss = total_loss / len(dataloader)
        scheduler.step(avg_loss)
        
        # æ—©åœæ£€æŸ¥
        if avg_loss < best_loss:
            best_loss = avg_loss
            patience_counter = 0
        else:
            patience_counter += 1
        
        if epoch % 20 == 0:
            print(f"   Epoch {epoch:3d}: Loss = {avg_loss:.6f}")
        
        if patience_counter >= 50:
            print(f"   Early stopping at epoch {epoch}")
            break
    
    # è¯„ä¼°æ¨¡å‹
    model.eval()
    with torch.no_grad():
        all_predictions = model(X_tensor).squeeze().cpu().numpy()
        all_predictions_denorm = all_predictions * y_std + y_mean
        
        mse = np.mean((all_predictions_denorm - y) ** 2)
        mae = np.mean(np.abs(all_predictions_denorm - y))
        r2 = 1 - np.sum((y - all_predictions_denorm) ** 2) / np.sum((y - np.mean(y)) ** 2)
        
        pred_std = np.std(all_predictions_denorm)
        pred_range = np.max(all_predictions_denorm) - np.min(all_predictions_denorm)
        
        print(f"\nâœ… Training completed!")
        print(f"   Final Loss: {best_loss:.6f}")
        print(f"   MSE: {mse:.2f}")
        print(f"   MAE: {mae:.2f}")
        print(f"   RÂ²: {r2:.4f}")
        print(f"   Prediction std: {pred_std:.2f}")
        print(f"   Prediction range: {pred_range:.2f}")
    
    return model, y_mean, y_std, {
        "mse": mse, "mae": mae, "r2": r2,
        "pred_std": pred_std, "pred_range": pred_range
    }

def regenerate_knowledge_base(training_data: List[Dict[str, Any]]) -> RAGKnowledgeBase:
    """æ ¹æ®æ–°çš„è®­ç»ƒæ•°æ®é‡æ–°ç”ŸæˆçŸ¥è¯†åº“"""
    
    print(f"\nğŸ”„ Regenerating knowledge base with {len(training_data)} cases...")
    
    # åˆ›å»ºæ–°çš„çŸ¥è¯†åº“
    kb = RAGKnowledgeBase(embedding_dim=32)
    
    for data in training_data:
        # ä½¿ç”¨çŠ¶æ€åµŒå…¥ä½œä¸ºä¸»è¦ç‰¹å¾
        embedding = np.array(data["state_embedding"], dtype=np.float32)
        
        # æ„å»ºå·¥ä½œæµä¿¡æ¯
        workflow_info = {
            "task_count": data["workflow_features"]["task_count"],
            "avg_task_flops": data["workflow_features"]["avg_task_flops"],
            "avg_memory": data["workflow_features"]["avg_memory"],
            "dependency_ratio": data["workflow_features"]["dependency_ratio"],
            "data_intensity": data["workflow_features"]["data_intensity"],
            "complexity": "medium",
            "type": "retrained_synthetic"
        }
        
        # ç”Ÿæˆè™šæ‹ŸåŠ¨ä½œåºåˆ—ï¼ˆèŠ‚ç‚¹åˆ†é…ï¼‰
        cluster_size = int(data["workflow_features"]["task_count"] * 0.1) + 2  # ä¼°ç®—é›†ç¾¤å¤§å°
        actions = [f"node_{i % cluster_size}" for i in range(data["workflow_features"]["task_count"])]
        
        # ä½¿ç”¨å®é™…çš„makespan
        makespan = data["makespan"]
        
        # æ·»åŠ åˆ°çŸ¥è¯†åº“
        kb.add_case(embedding, workflow_info, actions, makespan)
    
    print(f"âœ… Knowledge base regenerated with {len(kb.cases)} cases")
    return kb

def main():
    print("ğŸ”§ Retraining Performance Predictor with Improved Data")
    print("=" * 60)
    
    # ç”Ÿæˆæ”¹è¿›çš„è®­ç»ƒæ•°æ®
    # --- è¯·ä¿®æ”¹ä¸‹é¢è¿™ä¸€è¡Œ ---
    # å°† num_samples ä¿®æ”¹ä¸º num_scenarios
    training_data = create_improved_training_data(num_scenarios=5000)
    
    # è®­ç»ƒæ¨¡å‹
    model, y_mean, y_std, metrics = train_improved_performance_predictor(training_data)
    
    # é‡æ–°ç”ŸæˆçŸ¥è¯†åº“ï¼ˆä½¿ç”¨ç›¸åŒçš„è®­ç»ƒæ•°æ®ï¼‰
    kb = regenerate_knowledge_base(training_data)
    
    # ä¿å­˜æ¨¡å‹
    model_path = "models/wass_models.pth"
    print(f"\nğŸ’¾ Saving retrained model to {model_path}...")
    
    # åŠ è½½ç°æœ‰checkpointï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    try:
        checkpoint = torch.load(model_path, map_location="cpu")
        print("   Loaded existing checkpoint")
    except:
        checkpoint = {}
        print("   Creating new checkpoint")
    
    # æ›´æ–°æ€§èƒ½é¢„æµ‹å™¨ï¼ˆä¿ç•™å…¶ä»–ç»„ä»¶ï¼‰
    checkpoint["performance_predictor"] = model.state_dict()
    
    # ç¡®ä¿å…¶ä»–å¿…è¦ç»„ä»¶å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºé»˜è®¤å€¼
    if "policy_network" not in checkpoint:
        from src.ai_schedulers import PolicyNetwork
        checkpoint["policy_network"] = PolicyNetwork(
            state_dim=32, action_dim=1, hidden_dim=128
        ).state_dict()
        print("   Added default PolicyNetwork")
    
    if "gnn_encoder" not in checkpoint:
        try:
            from src.ai_schedulers import GraphEncoder
            checkpoint["gnn_encoder"] = GraphEncoder(
                node_feature_dim=8, edge_feature_dim=4, 
                hidden_dim=64, output_dim=32
            ).state_dict()
            print("   Added default GraphEncoder")
        except Exception as e:
            print(f"   Skipping GraphEncoder: {e}")
    
    # æ›´æ–°å…ƒæ•°æ®
    if "metadata" not in checkpoint:
        checkpoint["metadata"] = {}
    
    checkpoint["metadata"]["performance_predictor"] = {
        "y_mean": float(y_mean),
        "y_std": float(y_std),
        "training_samples": len(training_data),
        "retrained_at": "2025-09-08",
        "validation_results": metrics
    }
    
    # ä¿å­˜æ¨¡å‹
    os.makedirs("models", exist_ok=True)
    torch.save(checkpoint, model_path)
    
    # ä¿å­˜çŸ¥è¯†åº“
    kb_path = "data/knowledge_base.pkl"
    print(f"\nğŸ’¾ Saving regenerated knowledge base to {kb_path}...")
    os.makedirs("data", exist_ok=True)
    kb.save_knowledge_base(kb_path)
    
    print(f"âœ… Model and knowledge base retrained and saved successfully!")
    print(f"   New normalization: mean={y_mean:.2f}, std={y_std:.2f}")
    print(f"   Performance metrics: RÂ²={metrics['r2']:.4f}, MSE={metrics['mse']:.2f}")
    print(f"   Knowledge base cases: {len(kb.cases)}")
    print(f"\nğŸ‰ Ready for testing! Run: python experiments/real_experiment_framework.py")

if __name__ == "__main__":
    main()
