#!/usr/bin/env python3
"""
é‡æ–°è®­ç»ƒæ€§èƒ½é¢„æµ‹å™¨æ¨¡å‹ï¼Œä¿®å¤è´Ÿå€¼é¢„æµ‹é—®é¢˜
"""

import os
import sys
import torch
import torch.nn as nn
import numpy as np
import pickle
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)
sys.path.insert(0, os.path.join(parent_dir, 'src'))

try:
    from torch.utils.data import TensorDataset, DataLoader
    from src.ai_schedulers import PerformancePredictor, RAGKnowledgeBase
    HAS_AI_MODULES = True
except ImportError as e:
    print(f"Error: Required AI modules not available: {e}")
    sys.exit(1)

def create_improved_training_data(num_scenarios: int = 5000) -> List[Dict[str, Any]]:
    """
    ç”Ÿæˆé«˜è´¨é‡çš„åˆæˆè®­ç»ƒæ•°æ®ï¼ˆV5 - æœ€ç»ˆä¿®å¤ç‰ˆï¼‰
    ç¡®ä¿ç‰¹å¾ç”Ÿæˆé€»è¾‘ä¸€è‡´ï¼Œå¹¶ä¸ºçŸ¥è¯†åº“ä¿ç•™å¿…è¦å­—æ®µã€‚
    """
    print(f"ğŸ”§ Generating {num_scenarios} scenarios for training data...")
    from src.ai_schedulers import WASSRAGScheduler, SchedulingState
    temp_scheduler = WASSRAGScheduler()
    training_data = []
    makespan_values = []

    for i in range(num_scenarios):
        num_nodes = np.random.randint(2, 21)
        nodes = {f"node_{j}": {
            "cpu_capacity": round(np.random.uniform(2.0, 8.0), 2),
            "memory_capacity": round(np.random.uniform(8.0, 64.0), 2),
            "current_load": round(np.random.uniform(0.1, 0.9), 2),
        } for j in range(num_nodes)}

        task_info = {
            "id": f"task_{i}", "flops": float(np.random.uniform(0.5e9, 15e9)),
            "memory": round(np.random.uniform(1.0, 16.0), 2),
            "dependencies": [f"task_{k}" for k in range(np.random.randint(0, 4))]
        }

        state = SchedulingState(
            workflow_graph={"tasks": [task_info], "task_requirements": {f"task_{i}": task_info}},
            cluster_state={"nodes": nodes}, pending_tasks=[f"task_{i}"], current_task=f"task_{i}",
            available_nodes=list(nodes.keys()), timestamp=0.0
        )

        state_embedding = temp_scheduler._extract_simple_features_fallback(state)
        context_embedding = torch.randn(32, device=temp_scheduler.device)

        for node_name, node_details in nodes.items():
            action_embedding = temp_scheduler._encode_action(node_name, state)
            combined_features = torch.cat([
                state_embedding, action_embedding, context_embedding
            ]).cpu().numpy()
            
            task_cpu_gflops = task_info["flops"] / 1e9
            available_cpu = node_details["cpu_capacity"] * (1.0 - node_details["current_load"])
            base_time = task_cpu_gflops / max(available_cpu, 0.1)
            mem_penalty = max(0, task_info["memory"] - node_details["memory_capacity"]) * 0.5
            load_penalty = node_details["current_load"] * 2.0
            random_noise = np.random.uniform(-0.5, 0.5)
            execution_time = max(1.0, min(180.0, base_time + mem_penalty + load_penalty + random_noise))
            makespan_values.append(execution_time)
            
            # --- å…³é”®ä¿®æ”¹ï¼šå°† state_embedding å’Œå…¶ä»–å…ƒæ•°æ®ä¹Ÿå­˜èµ·æ¥ ---
            training_data.append({
                "features": combined_features.tolist(),
                "makespan": execution_time,
                "state_embedding": state_embedding.cpu().numpy().tolist(), # æ·»åŠ  state_embedding
                "workflow_features": { # æ·»åŠ çŸ¥è¯†åº“éœ€è¦çš„å…ƒæ•°æ®
                    "task_count": 1,
                    "avg_task_flops": task_info["flops"],
                    "avg_memory": task_info["memory"]
                }
            })
            
    makespan_array = np.array(makespan_values)
    print(f"ğŸ“Š Single task execution time distribution:")
    print(f"   Mean: {np.mean(makespan_array):.2f}s, Std: {np.std(makespan_array):.2f}s, "
          f"Min: {np.min(makespan_array):.2f}s, Max: {np.max(makespan_array):.2f}s")
    
    return training_data

def train_improved_performance_predictor(training_data: List[Dict[str, Any]], epochs: int = 200, batch_size: int = 64):
    """è®­ç»ƒæ”¹è¿›çš„æ€§èƒ½é¢„æµ‹å™¨"""
    
    print(f"ğŸš€ Training improved performance predictor...")
    print(f"   Training samples: {len(training_data)}")
    print(f"   Epochs: {epochs}")
    print(f"   Batch size: {batch_size}")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"   Device: {device}")
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    X = np.array([sample["features"] for sample in training_data])
    y = np.array([sample["makespan"] for sample in training_data])
    
    # æ•°æ®å½’ä¸€åŒ–ï¼ˆé‡è¦ï¼ï¼‰
    y_mean, y_std = np.mean(y), np.std(y)
    y_normalized = (y - y_mean) / (y_std + 1e-8)
    
    print(f"ğŸ“ˆ Training data statistics:")
    print(f"   Original y: mean={y_mean:.2f}, std={y_std:.2f}")
    print(f"   Normalized y: mean={np.mean(y_normalized):.6f}, std={np.std(y_normalized):.6f}")
    print(f"   Normalized range: [{np.min(y_normalized):.3f}, {np.max(y_normalized):.3f}]")
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    X_tensor = torch.FloatTensor(X).to(device)
    y_tensor = torch.FloatTensor(y_normalized).to(device)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataset = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    # åˆ›å»ºæ¨¡å‹
    model = PerformancePredictor(input_dim=96, hidden_dim=128).to(device)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=20, factor=0.5)
    
    # è®­ç»ƒå¾ªç¯
    best_loss = float('inf')
    patience_counter = 0
    
    for epoch in range(epochs):
        model.train()
        total_loss = 0.0
        
        for batch_X, batch_y in dataloader:
            optimizer.zero_grad()
            predictions = model(batch_X).squeeze()
            loss = criterion(predictions, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        avg_loss = total_loss / len(dataloader)
        scheduler.step(avg_loss)
        
        # æ—©åœæ£€æŸ¥
        if avg_loss < best_loss:
            best_loss = avg_loss
            patience_counter = 0
        else:
            patience_counter += 1
        
        if epoch % 20 == 0:
            print(f"   Epoch {epoch:3d}: Loss = {avg_loss:.6f}")
        
        if patience_counter >= 50:
            print(f"   Early stopping at epoch {epoch}")
            break
    
    # è¯„ä¼°æ¨¡å‹
    model.eval()
    with torch.no_grad():
        all_predictions = model(X_tensor).squeeze().cpu().numpy()
        all_predictions_denorm = all_predictions * y_std + y_mean
        
        mse = np.mean((all_predictions_denorm - y) ** 2)
        mae = np.mean(np.abs(all_predictions_denorm - y))
        r2 = 1 - np.sum((y - all_predictions_denorm) ** 2) / np.sum((y - np.mean(y)) ** 2)
        
        pred_std = np.std(all_predictions_denorm)
        pred_range = np.max(all_predictions_denorm) - np.min(all_predictions_denorm)
        
        print(f"\nâœ… Training completed!")
        print(f"   Final Loss: {best_loss:.6f}")
        print(f"   MSE: {mse:.2f}")
        print(f"   MAE: {mae:.2f}")
        print(f"   RÂ²: {r2:.4f}")
        print(f"   Prediction std: {pred_std:.2f}")
        print(f"   Prediction range: {pred_range:.2f}")
    
    return model, y_mean, y_std, {
        "mse": mse, "mae": mae, "r2": r2,
        "pred_std": pred_std, "pred_range": pred_range
    }

def regenerate_knowledge_base(training_data: List[Dict[str, Any]]) -> RAGKnowledgeBase:
    """æ ¹æ®æ–°çš„è®­ç»ƒæ•°æ®é‡æ–°ç”ŸæˆçŸ¥è¯†åº“ï¼ˆV2 - ä¿®å¤ç‰ˆï¼‰"""
    print(f"\nğŸ”„ Regenerating knowledge base with {len(training_data)} cases...")
    
    kb = RAGKnowledgeBase(embedding_dim=32)
    
    for data in training_data:
        # --- å…³é”®ä¿®æ”¹ï¼šç°åœ¨å¯ä»¥å®‰å…¨åœ°è¯»å– "state_embedding" ---
        embedding = np.array(data["state_embedding"], dtype=np.float32)
        
        # æ„å»ºä¸€ä¸ªç®€åŒ–çš„ workflow_info
        workflow_info = data.get("workflow_features", {"type": "retrained_synthetic"})
        
        # ä½¿ç”¨å®é™…çš„makespan
        makespan = data["makespan"]
        
        # æ·»åŠ åˆ°çŸ¥è¯†åº“
        kb.add_case(embedding, workflow_info, actions=[], makespan=makespan)
    
    print(f"âœ… Knowledge base regenerated with {len(kb.cases)} cases")
    return kb

def main():
    print("ğŸ”§ Retraining Performance Predictor with Improved Data")
    print("=" * 60)
    
    # ç”Ÿæˆæ”¹è¿›çš„è®­ç»ƒæ•°æ®
    # --- è¯·ä¿®æ”¹ä¸‹é¢è¿™ä¸€è¡Œ ---
    # å°† num_samples ä¿®æ”¹ä¸º num_scenarios
    training_data = create_improved_training_data(num_scenarios=5000)
    
    # è®­ç»ƒæ¨¡å‹
    model, y_mean, y_std, metrics = train_improved_performance_predictor(training_data)
    
    # é‡æ–°ç”ŸæˆçŸ¥è¯†åº“ï¼ˆä½¿ç”¨ç›¸åŒçš„è®­ç»ƒæ•°æ®ï¼‰
    kb = regenerate_knowledge_base(training_data)
    
    # ä¿å­˜æ¨¡å‹
    model_path = "models/wass_models.pth"
    print(f"\nğŸ’¾ Saving retrained model to {model_path}...")
    
    # åŠ è½½ç°æœ‰checkpointï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    try:
        checkpoint = torch.load(model_path, map_location="cpu")
        print("   Loaded existing checkpoint")
    except:
        checkpoint = {}
        print("   Creating new checkpoint")
    
    # æ›´æ–°æ€§èƒ½é¢„æµ‹å™¨ï¼ˆä¿ç•™å…¶ä»–ç»„ä»¶ï¼‰
    checkpoint["performance_predictor"] = model.state_dict()
    
    # ç¡®ä¿å…¶ä»–å¿…è¦ç»„ä»¶å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºé»˜è®¤å€¼
    if "policy_network" not in checkpoint:
        from src.ai_schedulers import PolicyNetwork
        checkpoint["policy_network"] = PolicyNetwork(
            state_dim=32, action_dim=1, hidden_dim=128
        ).state_dict()
        print("   Added default PolicyNetwork")
    
    if "gnn_encoder" not in checkpoint:
        try:
            from src.ai_schedulers import GraphEncoder
            checkpoint["gnn_encoder"] = GraphEncoder(
                node_feature_dim=8, edge_feature_dim=4, 
                hidden_dim=64, output_dim=32
            ).state_dict()
            print("   Added default GraphEncoder")
        except Exception as e:
            print(f"   Skipping GraphEncoder: {e}")
    
    # æ›´æ–°å…ƒæ•°æ®
    if "metadata" not in checkpoint:
        checkpoint["metadata"] = {}
    
    checkpoint["metadata"]["performance_predictor"] = {
        "y_mean": float(y_mean),
        "y_std": float(y_std),
        "training_samples": len(training_data),
        "retrained_at": "2025-09-08",
        "validation_results": metrics
    }
    
    # ä¿å­˜æ¨¡å‹
    os.makedirs("models", exist_ok=True)
    torch.save(checkpoint, model_path)
    
    # ä¿å­˜çŸ¥è¯†åº“
    kb_path = "data/knowledge_base.pkl"
    print(f"\nğŸ’¾ Saving regenerated knowledge base to {kb_path}...")
    os.makedirs("data", exist_ok=True)
    kb.save_knowledge_base(kb_path)
    
    print(f"âœ… Model and knowledge base retrained and saved successfully!")
    print(f"   New normalization: mean={y_mean:.2f}, std={y_std:.2f}")
    print(f"   Performance metrics: RÂ²={metrics['r2']:.4f}, MSE={metrics['mse']:.2f}")
    print(f"   Knowledge base cases: {len(kb.cases)}")
    print(f"\nğŸ‰ Ready for testing! Run: python experiments/real_experiment_framework.py")

if __name__ == "__main__":
    main()
