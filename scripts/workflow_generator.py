#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
WASS-RAG å·¥ä½œæµç”Ÿæˆå™¨
æ”¯æŒç”Ÿæˆä¸åŒè§„æ¨¡å’Œå¤æ‚åº¦çš„ç§‘å­¦å·¥ä½œæµï¼Œå‚è€ƒçœŸå®HPCåº”ç”¨åœºæ™¯
"""

import json
import os
import random
import argparse
from pathlib import Path
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime

@dataclass
class Task:
    """å·¥ä½œæµä»»åŠ¡å®šä¹‰"""
    id: str
    name: str
    memory: int     # MB
    flops: float    # æµ®ç‚¹è¿ç®—æ¬¡æ•°
    input_files: List[str]
    output_files: List[str]
    dependencies: List[str]  # ä¾èµ–çš„ä»»åŠ¡ID

@dataclass
class File:
    """æ–‡ä»¶å®šä¹‰"""
    id: str
    name: str
    size: int  # bytes

@dataclass
class Workflow:
    """å®Œæ•´å·¥ä½œæµå®šä¹‰"""
    name: str
    description: str
    tasks: List[Task]
    files: List[File]
    entry_task: str
    exit_task: str

class WorkflowPattern:
    """å·¥ä½œæµæ¨¡å¼å®šä¹‰"""
    
    # å¹³å°å‚æ•°ï¼ˆç”¨äºCCRè®¡ç®—ï¼‰
    AVG_PROCESSOR_SPEED = 2.625e9  # å¹³å‡å¤„ç†å™¨é€Ÿåº¦ (2.625 Gflops)
    AVG_BANDWIDTH = 1e9  # å¹³å‡å¸¦å®½ (1 GBps)
    
    @staticmethod
    def calculate_data_size(compute_flops: float, ccr: float) -> int:
        """
        æ ¹æ®è®¡ç®—é‡å’ŒCCRè®¡ç®—æ•°æ®å¤§å°
        :param compute_flops: è®¡ç®—é‡ (flops)
        :param ccr: é€šä¿¡è®¡ç®—æ¯”
        :return: æ•°æ®å¤§å° (bytes)
        """
        # è®¡ç®—æ—¶é—´ = è®¡ç®—é‡ / å¤„ç†å™¨é€Ÿåº¦
        compute_time = compute_flops / WorkflowPattern.AVG_PROCESSOR_SPEED
        # é€šä¿¡æ—¶é—´ = è®¡ç®—æ—¶é—´ * CCR
        communication_time = compute_time * ccr
        # æ•°æ®å¤§å° = é€šä¿¡æ—¶é—´ * å¸¦å®½
        data_size = int(communication_time * WorkflowPattern.AVG_BANDWIDTH)
        # ç¡®ä¿æœ€å°æ•°æ®å¤§å°
        return max(data_size, 1024)
    
    @staticmethod
    def generate_montage_like(num_tasks: int, ccr: float = 1.0) -> Workflow:
        """ç”Ÿæˆç±»Montageï¼ˆå¤©æ–‡å­¦å›¾åƒæ‹¼æ¥ï¼‰å·¥ä½œæµ"""
        tasks = []
        files = []
        
        # ç¬¬ä¸€é˜¶æ®µï¼šé¢„å¤„ç†ä»»åŠ¡ï¼ˆå¹¶è¡Œï¼‰
        preprocess_tasks = min(num_tasks // 3, 20)
        for i in range(preprocess_tasks):
            task_id = f"preprocess_{i}"
            input_file = f"raw_image_{i}.fits"
            output_file = f"processed_image_{i}.fits"
            
            flops = random.uniform(1e10, 5e10)
            input_size = WorkflowPattern.calculate_data_size(flops, ccr)
            output_size = WorkflowPattern.calculate_data_size(flops, ccr)
            
            files.append(File(input_file, input_file, input_size))
            files.append(File(output_file, output_file, output_size))
            
            tasks.append(Task(
                id=task_id,
                name=f"Preprocess Image {i}",
                memory=random.randint(2000, 4000),  # 2-4GB
                flops=flops,
                input_files=[input_file],
                output_files=[output_file],
                dependencies=[]
            ))
        
        # ç¬¬äºŒé˜¶æ®µï¼šå·®å¼‚æ£€æµ‹ï¼ˆéœ€è¦å‰é˜¶æ®µè¾“å‡ºï¼‰
        diff_tasks = min((num_tasks - preprocess_tasks) // 2, 15)
        for i in range(diff_tasks):
            task_id = f"diff_{i}"
            # éšæœºé€‰æ‹©ä¸¤ä¸ªé¢„å¤„ç†çš„è¾“å‡ºä½œä¸ºè¾“å…¥
            # ç¡®ä¿æœ‰è¶³å¤Ÿçš„ä»»åŠ¡å¯ä¾›é€‰æ‹©
            num_deps = min(2, preprocess_tasks)
            if num_deps > 0:
                deps = random.sample(tasks[:preprocess_tasks], num_deps)
            else:
                deps = []
            
            input_files = [dep.output_files[0] for dep in deps]
            output_file = f"diff_{i}.fits"
            
            flops = random.uniform(5e9, 2e10)
            output_size = WorkflowPattern.calculate_data_size(flops, ccr)
            
            files.append(File(output_file, output_file, output_size))
            
            tasks.append(Task(
                id=task_id,
                name=f"Difference Detection {i}",
                memory=random.randint(1000, 2000),
                flops=flops,
                input_files=input_files,
                output_files=[output_file],
                dependencies=[dep.id for dep in deps]
            ))
        
        # ç¬¬ä¸‰é˜¶æ®µï¼šæœ€ç»ˆæ‹¼æ¥ï¼ˆéœ€è¦æ‰€æœ‰å‰é˜¶æ®µè¾“å‡ºï¼‰
        remaining_tasks = num_tasks - preprocess_tasks - diff_tasks
        for i in range(remaining_tasks):
            task_id = f"mosaic_{i}"
            # éœ€è¦æ‰€æœ‰å·®å¼‚æ£€æµ‹çš„è¾“å‡º
            input_files = [task.output_files[0] for task in tasks if task.id.startswith('diff_')]
            output_file = f"final_mosaic_{i}.fits"
            
            flops = random.uniform(2e10, 1e11)
            output_size = WorkflowPattern.calculate_data_size(flops, ccr)
            
            files.append(File(output_file, output_file, output_size))
            
            tasks.append(Task(
                id=task_id,
                name=f"Final Mosaic {i}",
                memory=random.randint(4000, 8000),  # 4-8GB
                flops=flops,
                input_files=input_files,
                output_files=[output_file],
                dependencies=[task.id for task in tasks if task.id.startswith('diff_')]
            ))
        
        return Workflow(
            name=f"Montage-like-{num_tasks}",
            description=f"å¤©æ–‡å­¦å›¾åƒæ‹¼æ¥å·¥ä½œæµï¼Œ{num_tasks}ä¸ªä»»åŠ¡",
            tasks=tasks,
            files=files,
            entry_task=tasks[0].id,
            exit_task=tasks[-1].id
        )
    
    @staticmethod
    def generate_ligo_like(num_tasks: int, ccr: float = 1.0) -> Workflow:
        """ç”Ÿæˆç±»LIGOï¼ˆå¼•åŠ›æ³¢æ£€æµ‹ï¼‰å·¥ä½œæµ"""
        tasks = []
        files = []
        
        # æ•°æ®åˆ‡åˆ†é˜¶æ®µ
        split_tasks = min(num_tasks // 4, 10)
        for i in range(split_tasks):
            task_id = f"split_{i}"
            input_file = f"raw_data_{i}.dat"
            output_files = [f"segment_{i}_{j}.dat" for j in range(4)]
            
            flops = random.uniform(1e9, 5e9)
            input_size = WorkflowPattern.calculate_data_size(flops, ccr)
            
            files.append(File(input_file, input_file, input_size))  # 1-2GB
            for out_file in output_files:
                # æ¯ä¸ªè¾“å‡ºæ–‡ä»¶çš„å¤§å°æ ¹æ®æ€»è®¡ç®—é‡åˆ†é…
                output_size = WorkflowPattern.calculate_data_size(flops / len(output_files), ccr)
                files.append(File(out_file, out_file, output_size))
            
            tasks.append(Task(
                id=task_id,
                name=f"Data Split {i}",
                memory=random.randint(1000, 2000),
                flops=flops,
                input_files=[input_file],
                output_files=output_files,
                dependencies=[]
            ))
        
        # åˆ†æé˜¶æ®µï¼ˆé«˜å¹¶è¡Œåº¦ï¼‰
        analyze_tasks = num_tasks - split_tasks - split_tasks  # å‰©ä½™å¤§éƒ¨åˆ†ç”¨äºåˆ†æ
        for i in range(analyze_tasks):
            task_id = f"analyze_{i}"
            # éšæœºé€‰æ‹©ä¸€ä¸ªåˆ‡åˆ†ä»»åŠ¡çš„è¾“å‡º
            split_task = random.choice(tasks[:split_tasks])
            input_file = random.choice(split_task.output_files)
            output_file = f"analysis_result_{i}.json"
            
            flops = random.uniform(5e10, 2e11)  # é«˜è®¡ç®—é‡
            output_size = WorkflowPattern.calculate_data_size(flops, ccr)
            
            files.append(File(output_file, output_file, output_size))
            
            tasks.append(Task(
                id=task_id,
                name=f"Signal Analysis {i}",
                memory=random.randint(3000, 6000),
                flops=flops,  # é«˜è®¡ç®—é‡
                input_files=[input_file],
                output_files=[output_file],
                dependencies=[split_task.id]
            ))
        
        # æ±‡æ€»é˜¶æ®µ
        for i in range(split_tasks):
            task_id = f"merge_{i}"
            # æ”¶é›†æ‰€æœ‰åˆ†æç»“æœ
            input_files = [task.output_files[0] for task in tasks if task.id.startswith('analyze_')]
            output_file = f"detection_report_{i}.pdf"
            
            flops = random.uniform(1e10, 5e10)
            output_size = WorkflowPattern.calculate_data_size(flops, ccr)
            
            files.append(File(output_file, output_file, output_size))
            
            tasks.append(Task(
                id=task_id,
                name=f"Result Merge {i}",
                memory=random.randint(2000, 4000),
                flops=flops,
                input_files=input_files,
                output_files=[output_file],
                dependencies=[task.id for task in tasks if task.id.startswith('analyze_')]
            ))
        
        return Workflow(
            name=f"LIGO-like-{num_tasks}",
            description=f"å¼•åŠ›æ³¢æ£€æµ‹å·¥ä½œæµï¼Œ{num_tasks}ä¸ªä»»åŠ¡",
            tasks=tasks,
            files=files,
            entry_task=tasks[0].id,
            exit_task=tasks[-1].id
        )
    
    @staticmethod
    def generate_communication_intensive(num_tasks: int, ccr: float = 10.0) -> Workflow:
        """ç”Ÿæˆé€šä¿¡å¯†é›†å‹å·¥ä½œæµï¼ˆé«˜CCRï¼‰"""
        tasks = []
        files = []
        
        # åˆ›å»ºå¤šä¸ªå°å‹è®¡ç®—ä»»åŠ¡ï¼Œä½†äº§ç”Ÿå¤§é‡æ•°æ®ä¼ è¾“
        for i in range(num_tasks):
            task_id = f"comm_task_{i}"
            input_file = f"input_data_{i}.dat"
            output_file = f"output_data_{i}.dat"
            
            # é™åˆ¶è®¡ç®—é‡ï¼Œä½†äº§ç”Ÿå¤§é‡æ•°æ®ä¼ è¾“
            flops = random.uniform(1e8, 1e9)  # å°è®¡ç®—é‡
            input_size = WorkflowPattern.calculate_data_size(flops, ccr)
            output_size = WorkflowPattern.calculate_data_size(flops, ccr)
            
            files.append(File(input_file, input_file, input_size))
            files.append(File(output_file, output_file, output_size))
            
            # åˆ›å»ºä¾èµ–é“¾ï¼Œå¼ºåˆ¶æ•°æ®æµåŠ¨
            dependencies = [f"comm_task_{i-1}"] if i > 0 else []
            
            tasks.append(Task(
                id=task_id,
                name=f"Communication Task {i}",
                memory=random.randint(500, 2000),
                flops=flops,
                input_files=[input_file],
                output_files=[output_file],
                dependencies=dependencies
            ))
        
        return Workflow(
            name=f"Communication-Intensive-{num_tasks}",
            description=f"é€šä¿¡å¯†é›†å‹å·¥ä½œæµï¼Œ{num_tasks}ä¸ªä»»åŠ¡ï¼ŒCCR={ccr}",
            tasks=tasks,
            files=files,
            entry_task=tasks[0].id,
            exit_task=tasks[-1].id
        )

    @staticmethod
    def generate_cybershake_like(num_tasks: int, ccr: float = 1.0) -> Workflow:
        """ç”Ÿæˆç±»CyberShakeï¼ˆåœ°éœ‡æ¨¡æ‹Ÿï¼‰å·¥ä½œæµ"""
        tasks = []
        files = []
        
        # é¢„å¤„ç†é˜¶æ®µ
        prep_tasks = min(num_tasks // 5, 8)
        for i in range(prep_tasks):
            task_id = f"prep_{i}"
            input_file = f"seismic_model_{i}.dat"
            output_file = f"preprocessed_model_{i}.dat"
            
            flops = random.uniform(2e10, 8e10)
            input_size = WorkflowPattern.calculate_data_size(flops, ccr)
            output_size = WorkflowPattern.calculate_data_size(flops, ccr)
            
            files.append(File(input_file, input_file, input_size))
            files.append(File(output_file, output_file, output_size))
            
            tasks.append(Task(
                id=task_id,
                name=f"Model Preprocessing {i}",
                memory=random.randint(2000, 4000),
                flops=flops,
                input_files=[input_file],
                output_files=[output_file],
                dependencies=[]
            ))
        
        # ä»¿çœŸé˜¶æ®µï¼ˆè®¡ç®—å¯†é›†ï¼‰
        sim_tasks = num_tasks - prep_tasks - prep_tasks
        for i in range(sim_tasks):
            task_id = f"simulate_{i}"
            # ä¾èµ–ä¸€ä¸ªé¢„å¤„ç†ä»»åŠ¡
            prep_task = random.choice(tasks[:prep_tasks])
            input_file = prep_task.output_files[0]
            output_file = f"simulation_result_{i}.dat"
            
            flops = random.uniform(1e11, 5e11)  # æé«˜è®¡ç®—é‡
            output_size = WorkflowPattern.calculate_data_size(flops, ccr)
            
            files.append(File(output_file, output_file, output_size))
            
            tasks.append(Task(
                id=task_id,
                name=f"Earthquake Simulation {i}",
                memory=random.randint(6000, 12000),  # 6-12GB
                flops=flops,  # æé«˜è®¡ç®—é‡
                input_files=[input_file],
                output_files=[output_file],
                dependencies=[prep_task.id]
            ))
        
        # åå¤„ç†é˜¶æ®µ
        for i in range(prep_tasks):
            task_id = f"postprocess_{i}"
            # æ”¶é›†éƒ¨åˆ†ä»¿çœŸç»“æœ
            sim_subset = random.sample([task for task in tasks if task.id.startswith('simulate_')], 
                                     min(3, len([task for task in tasks if task.id.startswith('simulate_')])))
            input_files = [task.output_files[0] for task in sim_subset]
            output_file = f"hazard_map_{i}.png"
            
            flops = random.uniform(5e9, 2e10)
            output_size = WorkflowPattern.calculate_data_size(flops, ccr)
            
            files.append(File(output_file, output_file, output_size))
            
            tasks.append(Task(
                id=task_id,
                name=f"Hazard Analysis {i}",
                memory=random.randint(3000, 6000),
                flops=flops,
                input_files=input_files,
                output_files=[output_file],
                dependencies=[task.id for task in sim_subset]
            ))
        
        return Workflow(
            name=f"CyberShake-like-{num_tasks}",
            description=f"åœ°éœ‡æ¨¡æ‹Ÿå·¥ä½œæµï¼Œ{num_tasks}ä¸ªä»»åŠ¡",
            tasks=tasks,
            files=files,
            entry_task=tasks[0].id,
            exit_task=tasks[-1].id
        )

class WorkflowGenerator:
    """å·¥ä½œæµç”Ÿæˆå™¨ä¸»ç±»"""
    
    def __init__(self, output_dir: str = "data/workflows", ccr: float = 1.0):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.ccr = ccr  # Communication to Computation Ratio
        
        self.patterns = {
            'montage': lambda n: WorkflowPattern.generate_montage_like(n, self.ccr),
            'ligo': lambda n: WorkflowPattern.generate_ligo_like(n, self.ccr),
            'cybershake': lambda n: WorkflowPattern.generate_cybershake_like(n, self.ccr),
            'comm_intensive': lambda n: WorkflowPattern.generate_communication_intensive(n, self.ccr)
        }
    
    def generate_workflow_set(self, pattern: str, task_counts: List[int]) -> List[str]:
        """ç”Ÿæˆä¸€ç»„ä¸åŒè§„æ¨¡çš„å·¥ä½œæµ"""
        if pattern not in self.patterns:
            raise ValueError(f"æœªçŸ¥çš„å·¥ä½œæµæ¨¡å¼: {pattern}. æ”¯æŒçš„æ¨¡å¼: {list(self.patterns.keys())}")
        
        generated_files = []
        pattern_func = self.patterns[pattern]
        
        for count in task_counts:
            workflow = pattern_func(count)
            filename = f"{pattern}_{count}_tasks.json"
            filepath = self.output_dir / filename
            
            # ä¿å­˜ä¸ºJSONæ ¼å¼
            workflow_dict = {
                'metadata': {
                    'name': workflow.name,
                    'description': workflow.description,
                    'generated_at': datetime.now().isoformat(),
                    'task_count': len(workflow.tasks),
                    'file_count': len(workflow.files)
                },
                'workflow': {
                    'tasks': [asdict(task) for task in workflow.tasks],
                    'files': [asdict(file) for file in workflow.files],
                    'entry_task': workflow.entry_task,
                    'exit_task': workflow.exit_task
                }
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(workflow_dict, f, indent=2, ensure_ascii=False)
            
            generated_files.append(str(filepath))
            print(f"âœ… ç”Ÿæˆå·¥ä½œæµ: {filename} ({count} ä»»åŠ¡)")
        
        return generated_files
    
    def generate_single_workflow(self, pattern: str, task_count: int, random_seed: int, filename: str = None) -> str:
        """ç”Ÿæˆå•ä¸ªå›ºå®šå·¥ä½œæµï¼ˆç”¨äºå…¬å¹³å®éªŒï¼‰"""
        if pattern not in self.patterns:
            raise ValueError(f"æœªçŸ¥çš„å·¥ä½œæµæ¨¡å¼: {pattern}. æ”¯æŒçš„æ¨¡å¼: {list(self.patterns.keys())}")
        
        # è®¾ç½®å›ºå®šç§å­ç¡®ä¿å¯é‡ç°
        original_state = random.getstate()
        random.seed(random_seed)
        
        try:
            pattern_func = self.patterns[pattern]
            workflow = pattern_func(task_count)
            
            if filename is None:
                filename = f"{pattern}_{task_count}_seed{random_seed}.json"
            
            filepath = self.output_dir / filename
            
            # ä¿å­˜ä¸ºJSONæ ¼å¼
            workflow_dict = {
                'metadata': {
                    'name': workflow.name,
                    'description': workflow.description,
                    'generated_at': datetime.now().isoformat(),
                    'task_count': len(workflow.tasks),
                    'file_count': len(workflow.files),
                    'random_seed': random_seed,
                    'pattern': pattern,
                    'ccr': self.ccr
                },
                'workflow': {
                    'tasks': [asdict(task) for task in workflow.tasks],
                    'files': [asdict(file) for file in workflow.files],
                    'entry_task': workflow.entry_task,
                    'exit_task': workflow.exit_task
                }
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(workflow_dict, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… ç”Ÿæˆå›ºå®šå·¥ä½œæµ: {filename} ({task_count} ä»»åŠ¡, ç§å­: {random_seed})")
            return str(filepath)
            
        finally:
            # æ¢å¤éšæœºçŠ¶æ€
            random.setstate(original_state)
    
    def generate_all_scales(self) -> Dict[str, List[str]]:
        """ç”Ÿæˆæ‰€æœ‰è§„æ¨¡çš„æ ‡å‡†å·¥ä½œæµé›†åˆ"""
        # å®šä¹‰æ ‡å‡†è§„æ¨¡é›†åˆ
        small_scale = [10, 20, 30, 50]        # å°è§„æ¨¡ï¼šå¿«é€Ÿæµ‹è¯•
        medium_scale = [100, 200, 300, 500]   # ä¸­ç­‰è§„æ¨¡ï¼šå¸¸è§„å®éªŒ
        large_scale = [1000, 1500, 2000]      # å¤§è§„æ¨¡ï¼šå¯æ‰©å±•æ€§æµ‹è¯•
        
        all_files = {}
        
        # ä¸ºæ¯ç§æ¨¡å¼ç”Ÿæˆä¸åŒè§„æ¨¡
        for pattern in self.patterns:
            print(f"\nğŸš€ ç”Ÿæˆ {pattern.upper()} æ¨¡å¼å·¥ä½œæµ...")
            
            pattern_files = []
            pattern_files.extend(self.generate_workflow_set(pattern, small_scale))
            pattern_files.extend(self.generate_workflow_set(pattern, medium_scale))
            pattern_files.extend(self.generate_workflow_set(pattern, large_scale))
            
            all_files[pattern] = pattern_files
            print(f"ğŸ“Š {pattern} æ¨¡å¼å®Œæˆ: {len(pattern_files)} ä¸ªå·¥ä½œæµ")
        
        return all_files
    
    def generate_summary(self, generated_files: Dict[str, List[str]]) -> str:
        """ç”Ÿæˆå·¥ä½œæµé›†åˆæ‘˜è¦"""
        summary_path = self.output_dir / "workflow_summary.json"
        
        summary = {
            'generation_info': {
                'generated_at': datetime.now().isoformat(),
                'total_patterns': len(generated_files),
                'total_workflows': sum(len(files) for files in generated_files.values())
            },
            'patterns': {}
        }
        
        for pattern, files in generated_files.items():
            workflow_info = []
            for file_path in files:
                with open(file_path, 'r') as f:
                    data = json.load(f)
                    workflow_info.append({
                        'filename': Path(file_path).name,
                        'task_count': data['metadata']['task_count'],
                        'file_count': data['metadata']['file_count']
                    })
            
            summary['patterns'][pattern] = {
                'count': len(files),
                'workflows': workflow_info
            }
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        return str(summary_path)

def main():
    parser = argparse.ArgumentParser(description='WASS-RAG å·¥ä½œæµç”Ÿæˆå™¨')
    parser.add_argument('--pattern', choices=['montage', 'ligo', 'cybershake', 'comm_intensive', 'all'], 
                       default='all', help='å·¥ä½œæµæ¨¡å¼')
    parser.add_argument('--tasks', nargs='+', type=int, 
                       help='ä»»åŠ¡æ•°é‡åˆ—è¡¨ï¼Œä¾‹å¦‚ï¼š--tasks 50 100 200')
    parser.add_argument('--output', default='data/workflows', 
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--ccr', type=float, default=1.0,
                       help='é€šä¿¡è®¡ç®—æ¯” (Communication to Computation Ratio)ï¼Œé»˜è®¤ä¸º1.0')
    
    args = parser.parse_args()
    
    generator = WorkflowGenerator(args.output, args.ccr)
    
    if args.pattern == 'all':
        print("ğŸŒŸ ç”Ÿæˆå®Œæ•´å·¥ä½œæµé›†åˆ...")
        generated_files = generator.generate_all_scales()
        summary_path = generator.generate_summary(generated_files)
        
        print(f"\nğŸ“‹ å·¥ä½œæµæ‘˜è¦å·²ä¿å­˜: {summary_path}")
        print(f"ğŸ‰ æ€»è®¡ç”Ÿæˆ {sum(len(files) for files in generated_files.values())} ä¸ªå·¥ä½œæµæ–‡ä»¶")
        
    else:
        if not args.tasks:
            args.tasks = [50, 100, 200]  # é»˜è®¤è§„æ¨¡
            
        print(f"ğŸš€ ç”Ÿæˆ {args.pattern} æ¨¡å¼å·¥ä½œæµ...")
        files = generator.generate_workflow_set(args.pattern, args.tasks)
        
        print(f"âœ… å®Œæˆ! ç”Ÿæˆäº† {len(files)} ä¸ªå·¥ä½œæµæ–‡ä»¶")
        for file_path in files:
            print(f"  - {file_path}")

if __name__ == "__main__":
    main()
