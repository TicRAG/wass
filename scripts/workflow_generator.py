#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
WASS-RAG å·¥ä½œæµç”Ÿæˆå™¨
æ”¯æŒç”Ÿæˆä¸åŒè§„æ¨¡å’Œå¤æ‚åº¦çš„ç§‘å­¦å·¥ä½œæµï¼Œå‚è€ƒçœŸå®HPCåº”ç”¨åœºæ™¯
"""

import json
import os
import random
import argparse
from pathlib import Path
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime

@dataclass
class Task:
    """å·¥ä½œæµä»»åŠ¡å®šä¹‰"""
    id: str
    name: str
    runtime: float  # ç§’
    memory: int     # MB
    flops: float    # æµ®ç‚¹è¿ç®—æ¬¡æ•°
    input_files: List[str]
    output_files: List[str]
    dependencies: List[str]  # ä¾èµ–çš„ä»»åŠ¡ID

@dataclass
class File:
    """æ–‡ä»¶å®šä¹‰"""
    id: str
    name: str
    size: int  # bytes

@dataclass
class Workflow:
    """å®Œæ•´å·¥ä½œæµå®šä¹‰"""
    name: str
    description: str
    tasks: List[Task]
    files: List[File]
    entry_task: str
    exit_task: str

class WorkflowPattern:
    """å·¥ä½œæµæ¨¡å¼å®šä¹‰"""
    
    @staticmethod
    def generate_montage_like(num_tasks: int) -> Workflow:
        """ç”Ÿæˆç±»Montageï¼ˆå¤©æ–‡å­¦å›¾åƒæ‹¼æ¥ï¼‰å·¥ä½œæµ"""
        tasks = []
        files = []
        
        # ç¬¬ä¸€é˜¶æ®µï¼šé¢„å¤„ç†ä»»åŠ¡ï¼ˆå¹¶è¡Œï¼‰
        preprocess_tasks = min(num_tasks // 3, 20)
        for i in range(preprocess_tasks):
            task_id = f"preprocess_{i}"
            input_file = f"raw_image_{i}.fits"
            output_file = f"processed_image_{i}.fits"
            
            files.append(File(input_file, input_file, random.randint(100, 500) * 1024 * 1024))  # 100-500MB
            files.append(File(output_file, output_file, random.randint(80, 400) * 1024 * 1024))
            
            tasks.append(Task(
                id=task_id,
                name=f"Preprocess Image {i}",
                runtime=random.uniform(300, 900),  # 5-15åˆ†é’Ÿ
                memory=random.randint(2000, 4000),  # 2-4GB
                flops=random.uniform(1e10, 5e10),
                input_files=[input_file],
                output_files=[output_file],
                dependencies=[]
            ))
        
        # ç¬¬äºŒé˜¶æ®µï¼šå·®å¼‚æ£€æµ‹ï¼ˆéœ€è¦å‰é˜¶æ®µè¾“å‡ºï¼‰
        diff_tasks = min((num_tasks - preprocess_tasks) // 2, 15)
        for i in range(diff_tasks):
            task_id = f"diff_{i}"
            # éšæœºé€‰æ‹©ä¸¤ä¸ªé¢„å¤„ç†çš„è¾“å‡ºä½œä¸ºè¾“å…¥
            deps = random.sample(tasks[:preprocess_tasks], 2)
            input_files = [dep.output_files[0] for dep in deps]
            output_file = f"diff_{i}.fits"
            
            files.append(File(output_file, output_file, random.randint(10, 50) * 1024 * 1024))
            
            tasks.append(Task(
                id=task_id,
                name=f"Difference Detection {i}",
                runtime=random.uniform(120, 480),  # 2-8åˆ†é’Ÿ
                memory=random.randint(1000, 2000),
                flops=random.uniform(5e9, 2e10),
                input_files=input_files,
                output_files=[output_file],
                dependencies=[dep.id for dep in deps]
            ))
        
        # ç¬¬ä¸‰é˜¶æ®µï¼šæœ€ç»ˆæ‹¼æ¥ï¼ˆéœ€è¦æ‰€æœ‰å‰é˜¶æ®µè¾“å‡ºï¼‰
        remaining_tasks = num_tasks - preprocess_tasks - diff_tasks
        for i in range(remaining_tasks):
            task_id = f"mosaic_{i}"
            # éœ€è¦æ‰€æœ‰å·®å¼‚æ£€æµ‹çš„è¾“å‡º
            input_files = [task.output_files[0] for task in tasks if task.id.startswith('diff_')]
            output_file = f"final_mosaic_{i}.fits"
            
            files.append(File(output_file, output_file, random.randint(500, 1000) * 1024 * 1024))
            
            tasks.append(Task(
                id=task_id,
                name=f"Final Mosaic {i}",
                runtime=random.uniform(600, 1800),  # 10-30åˆ†é’Ÿ
                memory=random.randint(4000, 8000),  # 4-8GB
                flops=random.uniform(2e10, 1e11),
                input_files=input_files,
                output_files=[output_file],
                dependencies=[task.id for task in tasks if task.id.startswith('diff_')]
            ))
        
        return Workflow(
            name=f"Montage-like-{num_tasks}",
            description=f"å¤©æ–‡å­¦å›¾åƒæ‹¼æ¥å·¥ä½œæµï¼Œ{num_tasks}ä¸ªä»»åŠ¡",
            tasks=tasks,
            files=files,
            entry_task=tasks[0].id,
            exit_task=tasks[-1].id
        )
    
    @staticmethod
    def generate_ligo_like(num_tasks: int) -> Workflow:
        """ç”Ÿæˆç±»LIGOï¼ˆå¼•åŠ›æ³¢æ£€æµ‹ï¼‰å·¥ä½œæµ"""
        tasks = []
        files = []
        
        # æ•°æ®åˆ‡åˆ†é˜¶æ®µ
        split_tasks = min(num_tasks // 4, 10)
        for i in range(split_tasks):
            task_id = f"split_{i}"
            input_file = f"raw_data_{i}.dat"
            output_files = [f"segment_{i}_{j}.dat" for j in range(4)]
            
            files.append(File(input_file, input_file, random.randint(1000, 2000) * 1024 * 1024))  # 1-2GB
            for out_file in output_files:
                files.append(File(out_file, out_file, random.randint(200, 400) * 1024 * 1024))
            
            tasks.append(Task(
                id=task_id,
                name=f"Data Split {i}",
                runtime=random.uniform(60, 180),
                memory=random.randint(1000, 2000),
                flops=random.uniform(1e9, 5e9),
                input_files=[input_file],
                output_files=output_files,
                dependencies=[]
            ))
        
        # åˆ†æé˜¶æ®µï¼ˆé«˜å¹¶è¡Œåº¦ï¼‰
        analyze_tasks = num_tasks - split_tasks - split_tasks  # å‰©ä½™å¤§éƒ¨åˆ†ç”¨äºåˆ†æ
        for i in range(analyze_tasks):
            task_id = f"analyze_{i}"
            # éšæœºé€‰æ‹©ä¸€ä¸ªåˆ‡åˆ†ä»»åŠ¡çš„è¾“å‡º
            split_task = random.choice(tasks[:split_tasks])
            input_file = random.choice(split_task.output_files)
            output_file = f"analysis_result_{i}.json"
            
            files.append(File(output_file, output_file, random.randint(1, 10) * 1024 * 1024))
            
            tasks.append(Task(
                id=task_id,
                name=f"Signal Analysis {i}",
                runtime=random.uniform(1200, 3600),  # 20-60åˆ†é’Ÿï¼Œè®¡ç®—å¯†é›†
                memory=random.randint(3000, 6000),
                flops=random.uniform(5e10, 2e11),  # é«˜è®¡ç®—é‡
                input_files=[input_file],
                output_files=[output_file],
                dependencies=[split_task.id]
            ))
        
        # æ±‡æ€»é˜¶æ®µ
        for i in range(split_tasks):
            task_id = f"merge_{i}"
            # æ”¶é›†æ‰€æœ‰åˆ†æç»“æœ
            input_files = [task.output_files[0] for task in tasks if task.id.startswith('analyze_')]
            output_file = f"detection_report_{i}.pdf"
            
            files.append(File(output_file, output_file, random.randint(5, 20) * 1024 * 1024))
            
            tasks.append(Task(
                id=task_id,
                name=f"Result Merge {i}",
                runtime=random.uniform(300, 900),
                memory=random.randint(2000, 4000),
                flops=random.uniform(1e10, 5e10),
                input_files=input_files,
                output_files=[output_file],
                dependencies=[task.id for task in tasks if task.id.startswith('analyze_')]
            ))
        
        return Workflow(
            name=f"LIGO-like-{num_tasks}",
            description=f"å¼•åŠ›æ³¢æ£€æµ‹å·¥ä½œæµï¼Œ{num_tasks}ä¸ªä»»åŠ¡",
            tasks=tasks,
            files=files,
            entry_task=tasks[0].id,
            exit_task=tasks[-1].id
        )
    
    @staticmethod
    def generate_cybershake_like(num_tasks: int) -> Workflow:
        """ç”Ÿæˆç±»CyberShakeï¼ˆåœ°éœ‡æ¨¡æ‹Ÿï¼‰å·¥ä½œæµ"""
        tasks = []
        files = []
        
        # é¢„å¤„ç†é˜¶æ®µ
        prep_tasks = min(num_tasks // 5, 8)
        for i in range(prep_tasks):
            task_id = f"prep_{i}"
            input_file = f"seismic_model_{i}.dat"
            output_file = f"preprocessed_model_{i}.dat"
            
            files.append(File(input_file, input_file, random.randint(200, 800) * 1024 * 1024))
            files.append(File(output_file, output_file, random.randint(150, 600) * 1024 * 1024))
            
            tasks.append(Task(
                id=task_id,
                name=f"Model Preprocessing {i}",
                runtime=random.uniform(600, 1200),
                memory=random.randint(2000, 4000),
                flops=random.uniform(2e10, 8e10),
                input_files=[input_file],
                output_files=[output_file],
                dependencies=[]
            ))
        
        # ä»¿çœŸé˜¶æ®µï¼ˆè®¡ç®—å¯†é›†ï¼‰
        sim_tasks = num_tasks - prep_tasks - prep_tasks
        for i in range(sim_tasks):
            task_id = f"simulate_{i}"
            # ä¾èµ–ä¸€ä¸ªé¢„å¤„ç†ä»»åŠ¡
            prep_task = random.choice(tasks[:prep_tasks])
            input_file = prep_task.output_files[0]
            output_file = f"simulation_result_{i}.dat"
            
            files.append(File(output_file, output_file, random.randint(300, 1200) * 1024 * 1024))
            
            tasks.append(Task(
                id=task_id,
                name=f"Earthquake Simulation {i}",
                runtime=random.uniform(3600, 7200),  # 1-2å°æ—¶ï¼Œé«˜è®¡ç®—é‡
                memory=random.randint(6000, 12000),  # 6-12GB
                flops=random.uniform(1e11, 5e11),  # æé«˜è®¡ç®—é‡
                input_files=[input_file],
                output_files=[output_file],
                dependencies=[prep_task.id]
            ))
        
        # åå¤„ç†é˜¶æ®µ
        for i in range(prep_tasks):
            task_id = f"postprocess_{i}"
            # æ”¶é›†éƒ¨åˆ†ä»¿çœŸç»“æœ
            sim_subset = random.sample([task for task in tasks if task.id.startswith('simulate_')], 
                                     min(3, len([task for task in tasks if task.id.startswith('simulate_')])))
            input_files = [task.output_files[0] for task in sim_subset]
            output_file = f"hazard_map_{i}.png"
            
            files.append(File(output_file, output_file, random.randint(50, 200) * 1024 * 1024))
            
            tasks.append(Task(
                id=task_id,
                name=f"Hazard Analysis {i}",
                runtime=random.uniform(900, 1800),
                memory=random.randint(3000, 6000),
                flops=random.uniform(5e9, 2e10),
                input_files=input_files,
                output_files=[output_file],
                dependencies=[task.id for task in sim_subset]
            ))
        
        return Workflow(
            name=f"CyberShake-like-{num_tasks}",
            description=f"åœ°éœ‡æ¨¡æ‹Ÿå·¥ä½œæµï¼Œ{num_tasks}ä¸ªä»»åŠ¡",
            tasks=tasks,
            files=files,
            entry_task=tasks[0].id,
            exit_task=tasks[-1].id
        )

class WorkflowGenerator:
    """å·¥ä½œæµç”Ÿæˆå™¨ä¸»ç±»"""
    
    def __init__(self, output_dir: str = "data/workflows"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.patterns = {
            'montage': WorkflowPattern.generate_montage_like,
            'ligo': WorkflowPattern.generate_ligo_like,
            'cybershake': WorkflowPattern.generate_cybershake_like
        }
    
    def generate_workflow_set(self, pattern: str, task_counts: List[int]) -> List[str]:
        """ç”Ÿæˆä¸€ç»„ä¸åŒè§„æ¨¡çš„å·¥ä½œæµ"""
        if pattern not in self.patterns:
            raise ValueError(f"æœªçŸ¥çš„å·¥ä½œæµæ¨¡å¼: {pattern}. æ”¯æŒçš„æ¨¡å¼: {list(self.patterns.keys())}")
        
        generated_files = []
        pattern_func = self.patterns[pattern]
        
        for count in task_counts:
            workflow = pattern_func(count)
            filename = f"{pattern}_{count}_tasks.json"
            filepath = self.output_dir / filename
            
            # ä¿å­˜ä¸ºJSONæ ¼å¼
            workflow_dict = {
                'metadata': {
                    'name': workflow.name,
                    'description': workflow.description,
                    'generated_at': datetime.now().isoformat(),
                    'task_count': len(workflow.tasks),
                    'file_count': len(workflow.files)
                },
                'workflow': {
                    'tasks': [asdict(task) for task in workflow.tasks],
                    'files': [asdict(file) for file in workflow.files],
                    'entry_task': workflow.entry_task,
                    'exit_task': workflow.exit_task
                }
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(workflow_dict, f, indent=2, ensure_ascii=False)
            
            generated_files.append(str(filepath))
            print(f"âœ… ç”Ÿæˆå·¥ä½œæµ: {filename} ({count} ä»»åŠ¡)")
        
        return generated_files
    
    def generate_all_scales(self) -> Dict[str, List[str]]:
        """ç”Ÿæˆæ‰€æœ‰è§„æ¨¡çš„æ ‡å‡†å·¥ä½œæµé›†åˆ"""
        # å®šä¹‰æ ‡å‡†è§„æ¨¡é›†åˆ
        small_scale = [10, 20, 30, 50]        # å°è§„æ¨¡ï¼šå¿«é€Ÿæµ‹è¯•
        medium_scale = [100, 200, 300, 500]   # ä¸­ç­‰è§„æ¨¡ï¼šå¸¸è§„å®éªŒ
        large_scale = [1000, 1500, 2000]      # å¤§è§„æ¨¡ï¼šå¯æ‰©å±•æ€§æµ‹è¯•
        
        all_files = {}
        
        # ä¸ºæ¯ç§æ¨¡å¼ç”Ÿæˆä¸åŒè§„æ¨¡
        for pattern in self.patterns:
            print(f"\nğŸš€ ç”Ÿæˆ {pattern.upper()} æ¨¡å¼å·¥ä½œæµ...")
            
            pattern_files = []
            pattern_files.extend(self.generate_workflow_set(pattern, small_scale))
            pattern_files.extend(self.generate_workflow_set(pattern, medium_scale))
            pattern_files.extend(self.generate_workflow_set(pattern, large_scale))
            
            all_files[pattern] = pattern_files
            print(f"ğŸ“Š {pattern} æ¨¡å¼å®Œæˆ: {len(pattern_files)} ä¸ªå·¥ä½œæµ")
        
        return all_files
    
    def generate_summary(self, generated_files: Dict[str, List[str]]) -> str:
        """ç”Ÿæˆå·¥ä½œæµé›†åˆæ‘˜è¦"""
        summary_path = self.output_dir / "workflow_summary.json"
        
        summary = {
            'generation_info': {
                'generated_at': datetime.now().isoformat(),
                'total_patterns': len(generated_files),
                'total_workflows': sum(len(files) for files in generated_files.values())
            },
            'patterns': {}
        }
        
        for pattern, files in generated_files.items():
            workflow_info = []
            for file_path in files:
                with open(file_path, 'r') as f:
                    data = json.load(f)
                    workflow_info.append({
                        'filename': Path(file_path).name,
                        'task_count': data['metadata']['task_count'],
                        'file_count': data['metadata']['file_count']
                    })
            
            summary['patterns'][pattern] = {
                'count': len(files),
                'workflows': workflow_info
            }
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        return str(summary_path)

def main():
    parser = argparse.ArgumentParser(description='WASS-RAG å·¥ä½œæµç”Ÿæˆå™¨')
    parser.add_argument('--pattern', choices=['montage', 'ligo', 'cybershake', 'all'], 
                       default='all', help='å·¥ä½œæµæ¨¡å¼')
    parser.add_argument('--tasks', nargs='+', type=int, 
                       help='ä»»åŠ¡æ•°é‡åˆ—è¡¨ï¼Œä¾‹å¦‚ï¼š--tasks 50 100 200')
    parser.add_argument('--output', default='data/workflows', 
                       help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    generator = WorkflowGenerator(args.output)
    
    if args.pattern == 'all':
        print("ğŸŒŸ ç”Ÿæˆå®Œæ•´å·¥ä½œæµé›†åˆ...")
        generated_files = generator.generate_all_scales()
        summary_path = generator.generate_summary(generated_files)
        
        print(f"\nğŸ“‹ å·¥ä½œæµæ‘˜è¦å·²ä¿å­˜: {summary_path}")
        print(f"ğŸ‰ æ€»è®¡ç”Ÿæˆ {sum(len(files) for files in generated_files.values())} ä¸ªå·¥ä½œæµæ–‡ä»¶")
        
    else:
        if not args.tasks:
            args.tasks = [50, 100, 200]  # é»˜è®¤è§„æ¨¡
            
        print(f"ğŸš€ ç”Ÿæˆ {args.pattern} æ¨¡å¼å·¥ä½œæµ...")
        files = generator.generate_workflow_set(args.pattern, args.tasks)
        
        print(f"âœ… å®Œæˆ! ç”Ÿæˆäº† {len(files)} ä¸ªå·¥ä½œæµæ–‡ä»¶")
        for file_path in files:
            print(f"  - {file_path}")

if __name__ == "__main__":
    main()
