#!/usr/bin/env python3
"""
WASS-RAG é˜¶æ®µä¸€ï¼šçŸ¥è¯†åº“æ’­ç§è„šæœ¬

è¯¥è„šæœ¬åˆ©ç”¨ä¿®å¤åçš„ç¦»æ•£äº‹ä»¶ä»¿çœŸå™¨ï¼Œè¿è¡Œå¤§é‡é«˜è´¨é‡çš„åŸºå‡†è°ƒåº¦ç®—æ³•ï¼ˆå¦‚ HEFTï¼‰ï¼Œ
å¹¶å°†è¯¦ç»†çš„ä»¿çœŸè¿‡ç¨‹æ•°æ®è®°å½•ä¸‹æ¥ï¼Œä¸ºåç»­çš„ Performance Predictor å’Œ DRL Agent è®­ç»ƒ
æä¾›é«˜è´¨é‡çš„ã€æºäºâ€œçœŸå®â€ç¯å¢ƒçš„æ•°æ®é›†ã€‚
"""

import sys
import os
import json
import time
from pathlib import Path
from dataclasses import dataclass, asdict
from typing import List, Dict, Any
import numpy as np
import copy

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, str(parent_dir))
sys.path.insert(0, str(parent_dir / 'src'))

# å¯¼å…¥ä»¿çœŸå™¨å’Œè°ƒåº¦å™¨
from experiments.real_experiment_framework import WassExperimentRunner, ExperimentConfig
from src.ai_schedulers import create_scheduler, SchedulingState

@dataclass
class TrainingSample:
    """å®šä¹‰ä¸€æ¡ç”¨äºè®­ç»ƒçš„æ ·æœ¬"""
    state_features: List[float]
    action_features: List[float]
    context_features: List[float]
    # ç›®æ ‡å€¼
    final_makespan: float # æ•´ä¸ªå·¥ä½œæµçš„æœ€ç»ˆå®Œå·¥æ—¶é—´
    achieved_finish_time: float # è¿™ä¸ªç‰¹å®šä»»åŠ¡çš„å®Œæˆæ—¶é—´

class KnowledgeSeedingFramework(WassExperimentRunner):
    """
    ä¸€ä¸ªä¸“é—¨ç”¨äºç”Ÿæˆè®­ç»ƒæ•°æ®é›†çš„ä»¿çœŸæ¡†æ¶å­ç±»ã€‚
    å®ƒé‡å†™äº†ä»¿çœŸå¾ªç¯ï¼Œä»¥æ•è·æ¯ä¸€æ­¥çš„è¯¦ç»†çŠ¶æ€å’Œå†³ç­–ä¿¡æ¯ã€‚
    """
    def __init__(self, config: ExperimentConfig):
        super().__init__(config)
        self.training_dataset: List[TrainingSample] = []
        # æˆ‘ä»¬éœ€è¦ä¸€ä¸ªä¸´æ—¶çš„WASS-RAGè°ƒåº¦å™¨å®ä¾‹ï¼Œæ¥å€Ÿç”¨å®ƒçš„ç‰¹å¾ç¼–ç æ–¹æ³•
        self.feature_encoder = create_scheduler("WASS-RAG")

    def run_and_collect_data(self):
        """è¿è¡Œä»¿çœŸå¹¶æ”¶é›†æ‰€æœ‰å†³ç­–ç‚¹çš„æ•°æ®"""
        total_simulations = len(self.config.workflow_sizes) * len(self.config.cluster_sizes) * self.config.repetitions
        print(f"ğŸš€ Starting Knowledge Seeding process for {total_simulations} simulations...")
        
        base_seed = int(time.time())
        sim_count = 0

        for task_count in self.config.workflow_sizes:
            for cluster_size in self.config.cluster_sizes:
                for rep in range(self.config.repetitions):
                    sim_count += 1
                    print(f"\n--- Running simulation {sim_count}/{total_simulations} (Tasks: {task_count}, Nodes: {cluster_size}, Rep: {rep}) ---")
                    
                    scenario_seed = base_seed + sim_count
                    workflow, cluster = self._generate_scenario(task_count, cluster_size, scenario_seed)
                    
                    # æˆ‘ä»¬åªä½¿ç”¨ HEFT æ¥ç”Ÿæˆé«˜è´¨é‡çš„åˆå§‹å†³ç­–æ•°æ®
                    self._run_simulation_and_capture(workflow, cluster, "HEFT")
        
        self._save_dataset()

    def _run_simulation_and_capture(self, workflow: Dict, cluster: Dict, method: str):
        """
        é‡å†™çš„ä»¿çœŸå¾ªç¯ï¼Œæ ¸å¿ƒç›®æ ‡æ˜¯æ•è·æ¯ä¸€ä¸ªå†³ç­–ç‚¹çš„ (state, action, outcome) æ•°æ®ã€‚
        """
        # ä»¿çœŸçŠ¶æ€å˜é‡
        node_available_time = {node: 0.0 for node in cluster}
        task_finish_time = {}
        task_placements = {}
        
        # æ•°æ®æ•è·å˜é‡
        decision_records = []

        # åˆå§‹åŒ–è°ƒåº¦å™¨
        scheduler = self._get_heft_scheduler(workflow, cluster)
        pending_tasks_set = {task['id'] for task in workflow['tasks']}
        
        while pending_tasks_set:
            ready_tasks = [
                task for task_id in sorted(list(pending_tasks_set))
                if all(dep in task_finish_time for dep in (task := next(t for t in workflow['tasks'] if t['id'] == task_id))['dependencies'])
            ]

            if not ready_tasks:
                if not pending_tasks_set: break
                raise RuntimeError("Simulation stuck: No ready tasks.")
            
            task_to_schedule = scheduler.get_next_task(ready_tasks)
            if not task_to_schedule: continue

            current_task_id = task_to_schedule['id']
            current_sim_time = min(node_available_time.values())

            earliest_start_times = {}
            for node in cluster:
                data_ready_time = max([task_finish_time.get(dep, 0) + (0.1 if task_placements.get(dep) != node else 0) for dep in task_to_schedule['dependencies']], default=0)
                earliest_start_times[node] = max(node_available_time[node], data_ready_time)

            state = SchedulingState(
                workflow_graph=workflow,
                cluster_state={"nodes": cluster, "earliest_start_times": earliest_start_times},
                pending_tasks=list(pending_tasks_set),
                current_task=current_task_id,
                available_nodes=list(cluster.keys()),
                timestamp=current_sim_time
            )
            
            # HEFT åšå‡ºå†³ç­–
            decision = scheduler.make_decision(state)
            chosen_node = decision.target_node
            
            # --- å…³é”®ï¼šæ•è·å†³ç­–ç¬é—´çš„çŠ¶æ€å’ŒåŠ¨ä½œç‰¹å¾ ---
            state_features = self.feature_encoder._extract_simple_features_fallback(state).cpu().numpy().tolist()
            action_features = self.feature_encoder._encode_action(chosen_node, state).cpu().numpy().tolist()
            # åœ¨è¿™ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬è¿˜æ²¡æœ‰RAGä¸Šä¸‹æ–‡
            context_features = np.zeros(32).tolist()

            # æ›´æ–°ä»¿çœŸçŠ¶æ€
            task_flops = task_to_schedule['flops']
            node_cpu_gflops = cluster[chosen_node]['cpu_capacity']
            exec_time = task_flops / (node_cpu_gflops * 1e9)
            start_time = earliest_start_times[chosen_node]
            finish_time = start_time + exec_time
            
            task_finish_time[current_task_id] = finish_time
            task_placements[current_task_id] = chosen_node
            node_available_time[chosen_node] = finish_time
            pending_tasks_set.remove(current_task_id)

            # è®°å½•è¿™æ¬¡å†³ç­–
            decision_records.append({
                "state_features": state_features,
                "action_features": action_features,
                "context_features": context_features,
                "achieved_finish_time": finish_time
            })

        # æ•´ä¸ªå·¥ä½œæµå®Œæˆåï¼Œè®¡ç®—æœ€ç»ˆ makespan
        final_makespan = max(task_finish_time.values()) if task_finish_time else 0
        print(f"  Simulation complete. Final Makespan: {final_makespan:.2f}s")

        # å°†æœ€ç»ˆ makespan å›å¡«åˆ°æ¯ä¸€æ¡è®°å½•ä¸­ï¼Œå¹¶å­˜å…¥ä¸»æ•°æ®é›†
        for record in decision_records:
            self.training_dataset.append(TrainingSample(
                state_features=record['state_features'],
                action_features=record['action_features'],
                context_features=record['context_features'],
                final_makespan=final_makespan,
                achieved_finish_time=record['achieved_finish_time']
            ))

    def _save_dataset(self):
        """å°†æ”¶é›†åˆ°çš„æ•°æ®é›†ä¿å­˜åˆ°æ–‡ä»¶"""
        output_file = Path("data/kb_training_dataset.json")
        output_file.parent.mkdir(exist_ok=True)
        
        dataset_as_dicts = [asdict(sample) for sample in self.training_dataset]
        
        with open(output_file, 'w') as f:
            json.dump(dataset_as_dicts, f, indent=2)
            
        print(f"\nâœ… Successfully generated and saved {len(self.training_dataset)} training samples to {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®ç”Ÿæˆçš„æ•°æ®é‡
    config = ExperimentConfig(
        name="Knowledge Base Seeding",
        workflow_sizes=[50, 100, 150], # ä½¿ç”¨æ›´å¤§ã€æ›´å¤æ‚çš„å·¥ä½œæµ
        scheduling_methods=["HEFT"],  # åªä½¿ç”¨HEFT
        cluster_sizes=[8, 16, 32],
        repetitions=10, # æ›´å¤šçš„é‡å¤æ¬¡æ•°ä»¥äº§ç”Ÿä¸°å¯Œçš„æ•°æ®
        output_dir="temp_kb_seeding_results" # ä¸´æ—¶ç›®å½•ï¼Œæˆ‘ä»¬ä¸å…³å¿ƒè¿™é‡Œçš„æœ€ç»ˆæ€§èƒ½
    )
    
    seeder = KnowledgeSeedingFramework(config)
    seeder.run_and_collect_data()

if __name__ == "__main__":
    main()