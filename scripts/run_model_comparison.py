#!/usr/bin/env python3
"""æ¯”è¾ƒä¸åŒæ ‡ç­¾æ¨¡å‹çš„æ€§èƒ½."""

import os
import sys
import yaml
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.pipeline_enhanced import run_enhanced_pipeline

def create_label_model_configs():
    """åˆ›å»ºä¸åŒæ ‡ç­¾æ¨¡å‹çš„é…ç½®."""
    
    configs_dir = Path('configs/label_model_experiments')
    configs_dir.mkdir(parents=True, exist_ok=True)
    
    # åŸºç¡€é…ç½®æ¨¡æ¿
    base_labeling = {
        'abstain': -1,
        'lfs': [
            {
                'name': 'keyword_positive',
                'type': 'keyword',
                'keywords': ['good', 'excellent', 'amazing', 'great', 'wonderful'],
                'label': 1
            },
            {
                'name': 'keyword_negative',
                'type': 'keyword', 
                'keywords': ['bad', 'terrible', 'awful', 'poor', 'horrible'],
                'label': 0
            },
            {
                'name': 'regex_positive',
                'type': 'regex',
                'pattern': 'love|perfect|brilliant',
                'label': 1
            },
            {
                'name': 'length_filter',
                'type': 'length',
                'min_length': 5,
                'max_length': 50, 
                'label': 1
            }
        ]
    }
    
    # ä¸åŒæ ‡ç­¾æ¨¡å‹é…ç½®
    model_configs = {
        'majority_vote.yaml': {
            'label_model': {
                'type': 'majority_vote',
                'params': {}
            }
        },
        'wrench_majority.yaml': {
            'label_model': {
                'type': 'wrench',
                'model_name': 'MajorityVoting',
                'params': {}
            }
        },
        'wrench_snorkel.yaml': {
            'label_model': {
                'type': 'wrench',
                'model_name': 'Snorkel',
                'params': {
                    'lr': 0.01,
                    'l2': 0.01,
                    'n_epochs': 100
                }
            }
        }
    }
    
    # ä¿å­˜é…ç½®æ–‡ä»¶
    for filename, config in model_configs.items():
        config['labeling'] = base_labeling
        config_path = configs_dir / filename
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, allow_unicode=True, default_flow_style=False)
        print(f"âœ“ åˆ›å»ºé…ç½®: {config_path}")
    
    return list(model_configs.keys())

def run_label_model_comparison():
    """è¿è¡Œæ ‡ç­¾æ¨¡å‹å¯¹æ¯”å®éªŒ."""
    
    # åŠ è½½åŸºç¡€é…ç½®
    base_config_path = Path('configs_example.yaml')
    if not base_config_path.exists():
        print("âŒ åŸºç¡€é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: configs_example.yaml")
        return
    
    with open(base_config_path, encoding='utf-8') as f:
        base_config = yaml.safe_load(f)
    
    # åˆ›å»ºæ¨¡å‹é…ç½®
    print("ğŸ”§ åˆ›å»ºæ ‡ç­¾æ¨¡å‹é…ç½®...")
    model_configs = create_label_model_configs()
    
    # è¿è¡Œå®éªŒ
    results = {}
    configs_dir = Path('configs/label_model_experiments')
    
    for model_config_file in model_configs:
        model_config_path = configs_dir / model_config_file
        exp_name = model_config_path.stem
        
        print(f"\nğŸš€ è¿è¡Œå®éªŒ: {exp_name}")
        
        try:
            # åŠ è½½æ¨¡å‹é…ç½®
            with open(model_config_path, encoding='utf-8') as f:
                model_data = yaml.safe_load(f)
            
            # åˆå¹¶é…ç½®
            config = base_config.copy()
            config.update(model_data)
            config['experiment_name'] = f"label_model_{exp_name}"
            config['paths']['results_dir'] = f"results/label_model_experiments/{exp_name}/"
            
            # ä¿å­˜ä¸´æ—¶é…ç½®
            temp_config_path = f"temp_model_{exp_name}.yaml"
            with open(temp_config_path, 'w', encoding='utf-8') as f:
                yaml.dump(config, f, allow_unicode=True, default_flow_style=False)
            
            # è¿è¡Œå®éªŒ
            result = run_enhanced_pipeline(temp_config_path)
            results[exp_name] = result
            
            print(f"âœ“ {exp_name} å®Œæˆ")
            print(f"  - æ¨¡å‹ç±»å‹: {config['label_model']['type']}")
            if 'model_name' in config['label_model']:
                print(f"  - æ¨¡å‹åç§°: {config['label_model']['model_name']}")
            print(f"  - å‡†ç¡®ç‡: {result.get('eval_stats', {}).get('accuracy', 0):.4f}")
            print(f"  - F1: {result.get('eval_stats', {}).get('f1', 0):.4f}")
            
        except Exception as e:
            print(f"âœ— {exp_name} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            temp_config_path = f"temp_model_{exp_name}.yaml"
            if Path(temp_config_path).exists():
                Path(temp_config_path).unlink()
    
    # ä¿å­˜æ±‡æ€»ç»“æœ
    summary_path = Path('results/label_model_experiments/summary_all.json')
    summary_path.parent.mkdir(parents=True, exist_ok=True)
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"\nğŸ“Š å®éªŒæ±‡æ€»ä¿å­˜åˆ°: {summary_path}")
    return results

def compare_model_results(results):
    """æ¯”è¾ƒæ ‡ç­¾æ¨¡å‹å®éªŒç»“æœ."""
    if not results:
        print("âŒ æ²¡æœ‰å®éªŒç»“æœå¯æ¯”è¾ƒ")
        return
    
    print("\nğŸ“ˆ æ ‡ç­¾æ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
    print("-" * 90)
    print(f"{'æ¨¡å‹':<20} {'å‡†ç¡®ç‡':<10} {'F1':<10} {'è¦†ç›–ç‡':<10} {'å†²çªç‡':<10} {'è®­ç»ƒæ—¶é—´':<10}")
    print("-" * 90)
    
    for exp_name, result in results.items():
        accuracy = result.get('eval_stats', {}).get('accuracy', 0)
        f1 = result.get('eval_stats', {}).get('f1', 0)
        coverage = result.get('labeling_stats', {}).get('coverage', 0)
        conflict_rate = result.get('labeling_stats', {}).get('conflict_rate', 0)
        
        # è·å–è®­ç»ƒæ—¶é—´
        label_model_stage = result.get('stages', {}).get('label_model', {})
        train_time = label_model_stage.get('elapsed_seconds', 0)
        
        print(f"{exp_name:<20} {accuracy:<10.4f} {f1:<10.4f} {coverage:<10.4f} {conflict_rate:<10.4f} {train_time:<10.3f}s")
    
    print("-" * 90)
    
    # åˆ†æç»“æœ
    print(f"\nğŸ“Š æ€§èƒ½åˆ†æ:")
    
    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    best_accuracy = max(results.items(), key=lambda x: x[1].get('eval_stats', {}).get('accuracy', 0))
    best_f1 = max(results.items(), key=lambda x: x[1].get('eval_stats', {}).get('f1', 0))
    fastest = min(results.items(), key=lambda x: x[1].get('stages', {}).get('label_model', {}).get('elapsed_seconds', float('inf')))
    
    print(f"  ğŸ† æœ€é«˜å‡†ç¡®ç‡: {best_accuracy[0]} ({best_accuracy[1]['eval_stats']['accuracy']:.4f})")
    print(f"  ğŸ† æœ€é«˜F1: {best_f1[0]} ({best_f1[1]['eval_stats']['f1']:.4f})")
    print(f"  âš¡ æœ€å¿«è®­ç»ƒ: {fastest[0]} ({fastest[1]['stages']['label_model']['elapsed_seconds']:.3f}s)")
    
    # è®¡ç®—å¹³å‡æ€§èƒ½
    avg_accuracy = sum(r.get('eval_stats', {}).get('accuracy', 0) for r in results.values()) / len(results)
    avg_f1 = sum(r.get('eval_stats', {}).get('f1', 0) for r in results.values()) / len(results)
    
    print(f"\nğŸ“ å¹³å‡æ€§èƒ½:")
    print(f"  å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.4f}")
    print(f"  å¹³å‡F1: {avg_f1:.4f}")
    
    # Wrench vs å†…ç½®æ¨¡å‹å¯¹æ¯”
    wrench_results = {k: v for k, v in results.items() if 'wrench' in k}
    builtin_results = {k: v for k, v in results.items() if 'majority' in k and 'wrench' not in k}
    
    if wrench_results and builtin_results:
        wrench_avg_acc = sum(r.get('eval_stats', {}).get('accuracy', 0) for r in wrench_results.values()) / len(wrench_results)
        builtin_avg_acc = sum(r.get('eval_stats', {}).get('accuracy', 0) for r in builtin_results.values()) / len(builtin_results)
        
        print(f"\nğŸ”„ Wrench vs å†…ç½®æ¨¡å‹:")
        print(f"  Wrenchå¹³å‡å‡†ç¡®ç‡: {wrench_avg_acc:.4f}")
        print(f"  å†…ç½®æ¨¡å‹å¹³å‡å‡†ç¡®ç‡: {builtin_avg_acc:.4f}")
        print(f"  æ€§èƒ½æå‡: {((wrench_avg_acc - builtin_avg_acc) / builtin_avg_acc * 100):+.2f}%")

def main():
    """ä¸»å‡½æ•°."""
    print("ğŸ·ï¸ WASS æ ‡ç­¾æ¨¡å‹å¯¹æ¯”å®éªŒ")
    print("=" * 50)
    
    # æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
    data_dir = Path('data')
    if not (data_dir / 'train.jsonl').exists():
        print("âš ï¸ è®­ç»ƒæ•°æ®ä¸å­˜åœ¨ï¼Œæ­£åœ¨ç”Ÿæˆ...")
        os.system('python scripts/gen_fake_data.py --out_dir data --train 500 --valid 100 --test 100')
    
    # è¿è¡Œå®éªŒ
    results = run_label_model_comparison()
    
    # æ¯”è¾ƒç»“æœ
    compare_model_results(results)
    
    print(f"\nâœ¨ æ ‡ç­¾æ¨¡å‹å¯¹æ¯”å®éªŒå®Œæˆï¼")
    print(f"ğŸ“ æŸ¥çœ‹ results/label_model_experiments/ ç›®å½•è·å–è¯¦ç»†ç»“æœ")
    print(f"ğŸ’¡ è¿è¡Œä»¥ä¸‹å‘½ä»¤è¿›è¡Œæ·±å…¥åˆ†æ:")
    print(f"   python scripts/analyze_results.py results/label_model_experiments/ --plot --report")

if __name__ == '__main__':
    main()
