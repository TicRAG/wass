"""
WASSå®Œæ•´å­¦æœ¯ç ”ç©¶å¹³å°

é›†æˆçœŸå®WRENCHä»¿çœŸçš„å®Œæ•´workflowç®¡ç†ç³»ç»Ÿ
"""

import json
import yaml
from pathlib import Path
from typing import Dict, Any, List
import logging
import time
from datetime import datetime

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class WASSAcademicPlatform:
    """
    WASSå­¦æœ¯ç ”ç©¶å¹³å°
    
    é›†æˆçœŸå®WRENCHä»¿çœŸå’Œå®Œæ•´çš„workflowç®¡ç†
    """
    
    def __init__(self, config_path: str = "configs/experiment.yaml"):
        self.config_path = config_path
        self.config = None
        self.wrench_simulator = None
        self.results = {}
        
    def load_config(self):
        """åŠ è½½é…ç½®"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
            logger.info(f"é…ç½®åŠ è½½æˆåŠŸ: {self.config_path}")
        except Exception as e:
            logger.error(f"é…ç½®åŠ è½½å¤±è´¥: {e}")
            # ä½¿ç”¨é»˜è®¤é…ç½®
            self.config = self._get_default_config()
    
    def _get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'data': {
                'adapter': 'jsonl',
                'train_file': 'train.jsonl',
                'valid_file': 'valid.jsonl',
                'test_file': 'test.jsonl'
            },
            'simulation': {
                'enabled': True,
                'platform': 'wrench',
                'compute_hosts': ['compute_host_1'],
                'storage_hosts': ['storage_host']
            },
            'workflow': {
                'stages': ['data_prep', 'labeling', 'training', 'evaluation'],
                'parallel': True
            },
            'paths': {
                'data_dir': './data',
                'results_dir': './results'
            }
        }
    
    def initialize_wrench_simulator(self):
        """åˆå§‹åŒ–WRENCHä»¿çœŸå™¨"""
        try:
            from wass_wrench_simulator import create_wass_wrench_simulator
            self.wrench_simulator = create_wass_wrench_simulator()
            
            if self.wrench_simulator.initialize():
                logger.info("ğŸ‰ WRENCHä»¿çœŸå™¨åˆå§‹åŒ–æˆåŠŸ")
                info = self.wrench_simulator.get_simulation_info()
                logger.info(f"WRENCHç‰ˆæœ¬: {info.get('wrench_version', 'unknown')}")
                logger.info(f"ä¸»æœºæ•°é‡: {info.get('host_count', 0)}")
                logger.info(f"Mockæ•°æ®: {info.get('mock_data', True)}")
                return True
            else:
                logger.warning("WRENCHä»¿çœŸå™¨åˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨fallbackæ¨¡å¼")
                return False
                
        except Exception as e:
            logger.error(f"WRENCHä»¿çœŸå™¨åŠ è½½å¤±è´¥: {e}")
            return False
    
    def create_academic_workflow(self) -> Dict:
        """åˆ›å»ºå­¦æœ¯ç ”ç©¶å·¥ä½œæµ"""
        workflow = {
            'name': 'wass_academic_research_workflow',
            'description': 'WASSå­¦æœ¯ç ”ç©¶å®Œæ•´å·¥ä½œæµ',
            'created_at': datetime.now().isoformat(),
            'tasks': [
                {
                    'id': 'data_preprocessing',
                    'name': 'æ•°æ®é¢„å¤„ç†',
                    'flops': 2e9,  # 2 GFlops
                    'memory': 1e9,  # 1 GB
                    'dependencies': [],
                    'stage': 'data_prep'
                },
                {
                    'id': 'feature_extraction',
                    'name': 'ç‰¹å¾æå–',
                    'flops': 5e9,  # 5 GFlops
                    'memory': 2e9,  # 2 GB  
                    'dependencies': ['data_preprocessing'],
                    'stage': 'labeling'
                },
                {
                    'id': 'label_function_execution',
                    'name': 'æ ‡æ³¨å‡½æ•°æ‰§è¡Œ',
                    'flops': 3e9,  # 3 GFlops
                    'memory': 1.5e9,  # 1.5 GB
                    'dependencies': ['feature_extraction'],
                    'stage': 'labeling'
                },
                {
                    'id': 'graph_construction',
                    'name': 'å›¾æ„å»º',
                    'flops': 4e9,  # 4 GFlops
                    'memory': 3e9,  # 3 GB
                    'dependencies': ['label_function_execution'],
                    'stage': 'training'
                },
                {
                    'id': 'gnn_training',
                    'name': 'GNNè®­ç»ƒ',
                    'flops': 15e9,  # 15 GFlops
                    'memory': 6e9,  # 6 GB
                    'dependencies': ['graph_construction'],
                    'stage': 'training'
                },
                {
                    'id': 'drl_policy_training',
                    'name': 'DRLç­–ç•¥è®­ç»ƒ',
                    'flops': 10e9,  # 10 GFlops
                    'memory': 4e9,  # 4 GB
                    'dependencies': ['gnn_training'],
                    'stage': 'training'
                },
                {
                    'id': 'model_evaluation',
                    'name': 'æ¨¡å‹è¯„ä¼°',
                    'flops': 2e9,  # 2 GFlops
                    'memory': 2e9,  # 2 GB
                    'dependencies': ['drl_policy_training'],
                    'stage': 'evaluation'
                },
                {
                    'id': 'result_analysis',
                    'name': 'ç»“æœåˆ†æ',
                    'flops': 1e9,  # 1 GFlops
                    'memory': 1e9,  # 1 GB
                    'dependencies': ['model_evaluation'],
                    'stage': 'evaluation'
                }
            ]
        }
        
        return workflow
    
    def run_academic_simulation(self) -> Dict:
        """è¿è¡Œå­¦æœ¯ä»¿çœŸ"""
        logger.info("ğŸš€ å¼€å§‹å­¦æœ¯ç ”ç©¶ä»¿çœŸ")
        
        # åˆ›å»ºå·¥ä½œæµ
        workflow = self.create_academic_workflow()
        
        # è¿è¡ŒWRENCHä»¿çœŸ
        if self.wrench_simulator:
            simulation_result = self.wrench_simulator.run_simulation(workflow)
        else:
            simulation_result = self._fallback_simulation(workflow)
        
        # åˆ†æä»¿çœŸç»“æœ
        analysis = self._analyze_simulation_results(simulation_result, workflow)
        
        # ç”Ÿæˆå­¦æœ¯æŠ¥å‘Š
        report = self._generate_academic_report(simulation_result, analysis)
        
        return {
            'workflow': workflow,
            'simulation_result': simulation_result,
            'analysis': analysis,
            'academic_report': report
        }
    
    def _analyze_simulation_results(self, sim_result: Dict, workflow: Dict) -> Dict:
        """åˆ†æä»¿çœŸç»“æœ"""
        tasks = workflow.get('tasks', [])
        
        # æŒ‰é˜¶æ®µåˆ†ç»„åˆ†æ
        stages = {}
        for task in tasks:
            stage = task.get('stage', 'unknown')
            if stage not in stages:
                stages[stage] = {
                    'tasks': [],
                    'total_flops': 0,
                    'total_memory': 0
                }
            stages[stage]['tasks'].append(task)
            stages[stage]['total_flops'] += task.get('flops', 0)
            stages[stage]['total_memory'] += task.get('memory', 0)
        
        # è®¡ç®—é˜¶æ®µæ‰§è¡Œæ—¶é—´ï¼ˆå‡è®¾é¡ºåºæ‰§è¡Œé˜¶æ®µï¼‰
        stage_times = {}
        for stage_name, stage_data in stages.items():
            # å‡è®¾é˜¶æ®µå†…ä»»åŠ¡å¯ä»¥å¹¶è¡Œ
            max_stage_flops = max(task.get('flops', 0) for task in stage_data['tasks'])
            stage_times[stage_name] = max_stage_flops / 2e9  # å‡è®¾2GFlopså¤„ç†é€Ÿåº¦
        
        # æ€§èƒ½åˆ†æ
        total_execution_time = sim_result.get('execution_time', 0)
        total_flops = sim_result.get('total_flops', 0)
        throughput = sim_result.get('throughput', 0)
        
        analysis = {
            'stage_analysis': {
                stage: {
                    'task_count': len(data['tasks']),
                    'computational_load': data['total_flops'],
                    'memory_requirement': data['total_memory'],
                    'estimated_time': stage_times.get(stage, 0)
                }
                for stage, data in stages.items()
            },
            'performance_metrics': {
                'total_execution_time': total_execution_time,
                'total_computational_load': total_flops,
                'system_throughput': throughput,
                'efficiency': sim_result.get('efficiency', 0),
                'cpu_utilization': sim_result.get('cpu_utilization', 0)
            },
            'resource_utilization': {
                'host_count': sim_result.get('host_count', 0),
                'memory_usage_ratio': sim_result.get('memory_usage', 0),
                'parallel_efficiency': sim_result.get('parallel_tasks', 0) / len(tasks) if tasks else 0
            }
        }
        
        return analysis
    
    def _generate_academic_report(self, sim_result: Dict, analysis: Dict) -> Dict:
        """ç”Ÿæˆå­¦æœ¯æŠ¥å‘Š"""
        is_real_wrench = not sim_result.get('mock_data', True)
        
        report = {
            'title': 'WASSå­¦æœ¯ç ”ç©¶å¹³å°ä»¿çœŸæŠ¥å‘Š',
            'timestamp': datetime.now().isoformat(),
            'simulation_platform': {
                'type': sim_result.get('platform_type', 'unknown'),
                'method': sim_result.get('simulation_method', 'unknown'),
                'wrench_version': sim_result.get('wrench_version', 'unknown'),
                'real_wrench_integration': is_real_wrench
            },
            'workflow_summary': {
                'workflow_name': sim_result.get('workflow_id', 'unknown'),
                'total_tasks': sim_result.get('task_count', 0),
                'workflow_depth': sim_result.get('workflow_depth', 0),
                'critical_path': sim_result.get('critical_path', [])
            },
            'performance_results': {
                'execution_time_seconds': sim_result.get('execution_time', 0),
                'computational_throughput_flops': sim_result.get('throughput', 0),
                'system_efficiency_percent': sim_result.get('efficiency', 0) * 100,
                'cpu_utilization_percent': sim_result.get('cpu_utilization', 0) * 100
            },
            'infrastructure_details': {
                'compute_hosts': sim_result.get('hosts', []),
                'host_count': sim_result.get('host_count', 0),
                'memory_utilization_percent': sim_result.get('memory_usage', 0) * 100
            },
            'academic_insights': {
                'scalability_assessment': self._assess_scalability(analysis),
                'bottleneck_analysis': self._identify_bottlenecks(analysis),
                'optimization_recommendations': self._generate_recommendations(analysis)
            },
            'validation': {
                'simulation_validity': is_real_wrench,
                'data_source': 'Real WRENCH 0.3-dev' if is_real_wrench else 'Simulated',
                'reproducibility': 'High' if is_real_wrench else 'Medium'
            }
        }
        
        return report
    
    def _assess_scalability(self, analysis: Dict) -> str:
        """è¯„ä¼°å¯æ‰©å±•æ€§"""
        cpu_util = analysis['performance_metrics']['cpu_utilization']
        if cpu_util > 0.8:
            return "High CPU utilization suggests good scalability potential"
        elif cpu_util > 0.5:
            return "Moderate CPU utilization indicates room for scaling"
        else:
            return "Low CPU utilization suggests under-utilized resources"
    
    def _identify_bottlenecks(self, analysis: Dict) -> List[str]:
        """è¯†åˆ«ç“¶é¢ˆ"""
        bottlenecks = []
        
        # æ£€æŸ¥å„é˜¶æ®µçš„è®¡ç®—è´Ÿè½½
        stages = analysis['stage_analysis']
        max_load = max(stage['computational_load'] for stage in stages.values())
        
        for stage_name, stage_data in stages.items():
            if stage_data['computational_load'] > max_load * 0.8:
                bottlenecks.append(f"{stage_name}é˜¶æ®µè®¡ç®—è´Ÿè½½è¾ƒé«˜")
        
        # æ£€æŸ¥å†…å­˜ä½¿ç”¨
        memory_usage = analysis['resource_utilization']['memory_usage_ratio']
        if memory_usage > 0.8:
            bottlenecks.append("å†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜ï¼Œå¯èƒ½æˆä¸ºç“¶é¢ˆ")
        
        return bottlenecks or ["æœªå‘ç°æ˜æ˜¾ç“¶é¢ˆ"]
    
    def _generate_recommendations(self, analysis: Dict) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        parallel_eff = analysis['resource_utilization']['parallel_efficiency']
        if parallel_eff < 0.5:
            recommendations.append("å¢åŠ ä»»åŠ¡å¹¶è¡Œåº¦ä»¥æé«˜èµ„æºåˆ©ç”¨ç‡")
        
        cpu_util = analysis['performance_metrics']['cpu_utilization']
        if cpu_util < 0.6:
            recommendations.append("ä¼˜åŒ–ä»»åŠ¡è°ƒåº¦ä»¥æé«˜CPUåˆ©ç”¨ç‡")
        
        host_count = analysis['resource_utilization']['host_count']
        if host_count < 3:
            recommendations.append("è€ƒè™‘å¢åŠ è®¡ç®—èŠ‚ç‚¹ä»¥æå‡å¹¶è¡Œå¤„ç†èƒ½åŠ›")
        
        return recommendations or ["å½“å‰é…ç½®è¾ƒä¸ºåˆç†"]
    
    def _fallback_simulation(self, workflow: Dict) -> Dict:
        """åå¤‡ä»¿çœŸ"""
        tasks = workflow.get('tasks', [])
        total_flops = sum(task.get('flops', 0) for task in tasks)
        
        return {
            'success': True,
            'workflow_id': workflow.get('name', 'fallback'),
            'execution_time': total_flops / 1e9,
            'task_count': len(tasks),
            'total_flops': total_flops,
            'mock_data': True,
            'platform_type': 'Fallback Simulation'
        }
    
    def save_results(self, results: Dict):
        """ä¿å­˜ç»“æœ"""
        results_dir = Path(self.config.get('paths', {}).get('results_dir', './results'))
        results_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        results_file = results_dir / 'wass_academic_results.json'
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜å­¦æœ¯æŠ¥å‘Š
        report_file = results_dir / 'academic_report.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(results['academic_report'], f, indent=2, ensure_ascii=False)
        
        logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {results_dir}")
    
    def run_complete_academic_research(self) -> Dict:
        """è¿è¡Œå®Œæ•´çš„å­¦æœ¯ç ”ç©¶æµç¨‹"""
        logger.info("ğŸ“ å¯åŠ¨WASSå­¦æœ¯ç ”ç©¶å¹³å°")
        
        # 1. åŠ è½½é…ç½®
        self.load_config()
        
        # 2. åˆå§‹åŒ–WRENCHä»¿çœŸå™¨
        wrench_success = self.initialize_wrench_simulator()
        
        # 3. è¿è¡Œå­¦æœ¯ä»¿çœŸ
        results = self.run_academic_simulation()
        
        # 4. æ·»åŠ å¹³å°çŠ¶æ€ä¿¡æ¯
        results['platform_status'] = {
            'wrench_initialized': wrench_success,
            'config_loaded': self.config is not None,
            'simulation_method': 'real_wrench' if wrench_success else 'fallback'
        }
        
        # 5. ä¿å­˜ç»“æœ
        self.save_results(results)
        
        # 6. æ‰“å°æ‘˜è¦
        self._print_summary(results)
        
        return results
    
    def _print_summary(self, results: Dict):
        """æ‰“å°ç»“æœæ‘˜è¦"""
        report = results['academic_report']
        sim_result = results['simulation_result']
        
        print("\n" + "="*60)
        print("ğŸ“ WASSå­¦æœ¯ç ”ç©¶å¹³å° - æ‰§è¡Œæ‘˜è¦")
        print("="*60)
        
        print(f"ğŸ“Š ä»¿çœŸå¹³å°: {report['simulation_platform']['type']}")
        print(f"ğŸ”¬ WRENCHé›†æˆ: {'âœ… çœŸå®' if report['simulation_platform']['real_wrench_integration'] else 'âŒ æ¨¡æ‹Ÿ'}")
        print(f"ğŸ“ˆ å·¥ä½œæµä»»åŠ¡: {report['workflow_summary']['total_tasks']} ä¸ª")
        print(f"â±ï¸  æ‰§è¡Œæ—¶é—´: {report['performance_results']['execution_time_seconds']:.2f} ç§’")
        print(f"ğŸš€ ç³»ç»Ÿååé‡: {report['performance_results']['computational_throughput_flops']:.2e} Flops/s")
        print(f"ğŸ’» CPUåˆ©ç”¨ç‡: {report['performance_results']['cpu_utilization_percent']:.1f}%")
        print(f"ğŸ–¥ï¸  è®¡ç®—ä¸»æœº: {report['infrastructure_details']['host_count']} ä¸ª")
        
        print(f"\nğŸ“‹ å­¦æœ¯è¯„ä¼°:")
        print(f"   å¯æ‰©å±•æ€§: {report['academic_insights']['scalability_assessment']}")
        print(f"   æ•°æ®æœ‰æ•ˆæ€§: {report['validation']['simulation_validity']}")
        print(f"   å¯é‡ç°æ€§: {report['validation']['reproducibility']}")
        
        print(f"\nğŸ¯ Mockæ•°æ®çŠ¶æ€: {sim_result.get('mock_data', True)}")
        
        print("="*60)

def main():
    """ä¸»å‡½æ•°"""
    platform = WASSAcademicPlatform()
    results = platform.run_complete_academic_research()
    return results

if __name__ == "__main__":
    main()
